Date,User,Message
3/4/24 16:39,부끄러워하는 라이언,"부끄러워하는 라이언님이 들어왔습니다.
타인, 기관 등의 사칭에 유의해 주세요. 금전 또는 개인정보를 요구 받을 경우 신고해 주시기 바랍니다.
운영정책을 위반한 메시지로 신고 접수 시 카카오톡 이용에 제한이 있을 수 있습니다."
3/4/24 16:41,신난 어피치,신난 어피치님이 들어왔습니다.
3/4/24 16:41,ㅈㅅㅇ,ㅈㅅㅇ님이 들어왔습니다.
3/4/24 16:41,권대헌,권대헌님이 들어왔습니다.
3/4/24 16:44,떨고있는 어피치,떨고있는 어피치님이 들어왔습니다.
3/4/24 16:45,으쓱으쓱 어피치,으쓱으쓱 어피치님이 들어왔습니다.
3/4/24 16:46,얼굴마사지하는 제이지,얼굴마사지하는 제이지님이 들어왔습니다.
3/4/24 16:46,N이 되고픈 엡실론,N이 되고픈 엡실론님이 들어왔습니다.
3/4/24 16:47,택배 상자를 든 네오,택배 상자를 든 네오님이 들어왔습니다.
3/4/24 16:47,불나게 일하는 네오,불나게 일하는 네오님이 들어왔습니다.
3/4/24 16:49,으쓱으쓱 어피치,으쓱으쓱 어피치님이 들어왔습니다.
3/4/24 16:49,인사하는 제이지,인사하는 제이지님이 나갔습니다.
3/4/24 16:52,하트뿅뿅 라이언,하트뿅뿅 라이언님이 들어왔습니다.
3/4/24 16:53,타일러,타일러님이 들어왔습니다.
3/4/24 16:54,허주연,허주연님이 들어왔습니다.
3/4/24 16:54,눈물 흘리는 제이지,눈물 흘리는 제이지님이 들어왔습니다.
3/4/24 16:54,제이지,제이지님이 들어왔습니다.
3/4/24 16:54,건방진 제이지,건방진 제이지님이 들어왔습니다.
3/4/24 16:55,떨고있는 어피치,떨고있는 어피치님이 들어왔습니다.
3/4/24 16:55,...,...님이 들어왔습니다.
3/4/24 16:55,블럭을 무너트리는 라이언,블럭을 무너트리는 라이언님이 들어왔습니다.
3/4/24 16:55,떨고있는 어피치,떨고있는 어피치님이 들어왔습니다.
3/4/24 16:55,nnd,nnd님이 들어왔습니다.
3/4/24 16:55,화나서 방방 뛰는 튜브,화나서 방방 뛰는 튜브님이 들어왔습니다.
3/4/24 16:56,심수기모띠,심수기모띠님이 들어왔습니다.
3/4/24 16:56,Hopeful Neo,Hopeful Neo님이 들어왔습니다.
3/4/24 16:56,Luca [루카],Are the newest slides for Spring 2024 on either eTL or the math.snu.ac.kr course site?
3/4/24 16:58,츄리닝안경 네오,츄리닝안경 네오님이 들어왔습니다.
3/4/24 16:58,정우성,정우성님이 들어왔습니다.
3/4/24 16:58,피자 먹다 자는 무지,피자 먹다 자는 무지님이 들어왔습니다.
3/4/24 16:59,부끄러워하는 라이언,부끄러워하는 라이언님이 들어왔습니다.
3/4/24 16:59,TA Youngmin,"Please refer to the class website for course materials:
https://ernestryu.com/courses/deep_learning.html"
3/4/24 16:59,먹보 네오,먹보 네오님이 들어왔습니다.
3/4/24 17:00,눈물바다에 빠진 라이언,눈물바다에 빠진 라이언님이 들어왔습니다.
3/4/24 17:00,눈물바다에 빠진 라이언,눈물바다에 빠진 라이언님이 나갔습니다.
3/4/24 17:00,탈수학,탈수학님이 들어왔습니다.
3/4/24 17:01,Apeach enjoys music,Apeach enjoys music님이 들어왔습니다.
3/4/24 17:01,눈물바다에 빠진 라이언,눈물바다에 빠진 라이언님이 들어왔습니다.
3/4/24 17:02,불나게 일하는 네오,불나게 일하는 네오님이 들어왔습니다.
3/4/24 17:02,벌 서는 라이언,벌 서는 라이언님이 들어왔습니다.
3/4/24 17:03,졸린 라이언,졸린 라이언님이 들어왔습니다.
3/4/24 17:04,택배 상자를 든 네오,택배 상자를 든 네오님이 들어왔습니다.
3/4/24 17:04,기뻐하는 라이언,기뻐하는 라이언님이 들어왔습니다.
3/4/24 17:05,좌절하는 라이언,좌절하는 라이언님이 들어왔습니다.
3/4/24 17:05,엄지척 제이지,엄지척 제이지님이 들어왔습니다.
3/4/24 17:07,뿅뿅 네오,뿅뿅 네오님이 들어왔습니다.
3/4/24 17:07,말썽쟁이 네오,말썽쟁이 네오님이 들어왔습니다.
3/4/24 17:08,기뻐하는 라이언,기뻐하는 라이언님이 들어왔습니다.
3/4/24 17:09,치즈케이크,치즈케이크님이 들어왔습니다.
3/4/24 17:09,머리 빗는 네오,머리 빗는 네오님이 들어왔습니다.
3/4/24 17:10,건배하는 프로도,건배하는 프로도님이 들어왔습니다.
3/4/24 17:10,마이크를 든 라이언,마이크를 든 라이언님이 들어왔습니다.
3/4/24 17:10,TA Jisun,"톡게시판 '공지': Please refer to the class website for course materials:
https://ernestryu.com/courses/deep_learning.html"
3/4/24 17:12,건방진 제이지,건방진 제이지님이 들어왔습니다.
3/4/24 17:15,J,J님이 들어왔습니다.
3/4/24 17:18,경례하는 프로도,경례하는 프로도님이 들어왔습니다.
3/4/24 17:23,선풍기 바람 쐬는 어피치,선풍기 바람 쐬는 어피치님이 들어왔습니다.
3/4/24 17:23,gg,gg님이 들어왔습니다.
3/4/24 17:26,Ryan kneeling down,Ryan kneeling down님이 들어왔습니다.
3/4/24 17:28,12521💻💙,12521💻💙님이 들어왔습니다.
3/4/24 17:39,애교뿜뿜 무지,애교뿜뿜 무지님이 들어왔습니다.
3/4/24 17:40,애교뿜뿜 무지,애교뿜뿜 무지님이 나갔습니다.
3/4/24 17:42,Tube als Cheerleader,Tube als Cheerleader님이 들어왔습니다.
3/4/24 17:43,Tube als Cheerleader,Does Minum Square Method only work with shallow networks?
3/4/24 17:46,라면먹는 제이지,라면먹는 제이지님이 들어왔습니다.
3/4/24 17:49,.,.님이 들어왔습니다.
3/4/24 18:13,애교뿜뿜 무지,애교뿜뿜 무지님이 들어왔습니다.
3/4/24 18:18,초롱초롱 네오,초롱초롱 네오님이 들어왔습니다.
3/4/24 18:18,멋쩍은 튜브,출석체크는 없는건가여...? Are there no attendences?
3/4/24 18:19,씩씩거리는 무지,씩씩거리는 무지님이 들어왔습니다.
3/4/24 18:19,멋쩍은 튜브,앗 위에 있네요 죄송합니다
3/4/24 18:20,하트뽀뽀 어피치,위에가 안 보여서 그런데 답장이 뭐였나요?
3/4/24 18:20,부끄러워하는 라이언,출석체크 없다는 내용이었습니다
3/4/24 18:21,택배 상자를 든 네오,삭제된 메시지입니다.
3/4/24 18:22,한성재,한성재님이 들어왔습니다.
3/4/24 18:22,쑥스럽게 인사하는 프로도,"On the first week, we will have two make-up lectures on March 5th, 5:00–6:15pm at 28-301, and March 7th, 7:00–8:15pm at 28-101, to make up for absences planned later in the semester. These make-up lectures are in addition to the regular lectures on March 4th and March 6th, 5:00–6:15pm at 28-102."
3/4/24 18:22,쑥스럽게 인사하는 프로도,혹시 공지된 두번의 보충수업도 출석해야 하나요?
3/4/24 18:27,벙찐 튜브,Is there any textbook to learn?
3/4/24 18:30,.,.님이 들어왔습니다.
3/4/24 18:31,deee,deee님이 들어왔습니다.
3/4/24 18:34,열심히 일하는 네오,혹시 심수기 녹화강의는 언제쯤 올라오는지 알 수 있을까요?
3/4/24 18:42,TA Yongin,"톡게시판 '글 공유': Please refer to the class website for course materials:
https://ernestryu.com/courses/deep_learning.html"
3/4/24 18:42,마이크를 든 라이언,마이크를 든 라이언님이 들어왔습니다.
3/4/24 18:44,얼굴마사지하는 제이지,얼굴마사지하는 제이지님이 들어왔습니다.
3/4/24 19:08,경례하는 프로도,경례하는 프로도님이 들어왔습니다.
3/4/24 19:23,귀여운 라이언,귀여운 라이언님이 들어왔습니다.
3/4/24 19:26,멋쟁이 프로도,멋쟁이 프로도님이 들어왔습니다.
3/4/24 19:29,하트뿅뿅 라이언,"안녕하세요, 혹시 녹화본은 어디에 올라왔나요?"
3/4/24 19:30,먹보 네오,먹보 네오님이 들어왔습니다.
3/4/24 19:34,열심히 일하는 네오,Have a question about the homework. Is handwritten solution acceptable? Do we have to type it in LaTex? 
3/4/24 19:35,먹보 네오,"Hello,
Apologies in advance if i have missed it in class
But are the dates for the midterm and final exams available?"
3/4/24 19:36,애교뿜뿜 어피치,애교뿜뿜 어피치님이 들어왔습니다.
3/4/24 19:36,열심히 일하는 네오,삭제된 메시지입니다.
3/4/24 19:37,부끄러워하는 라이언,Look at class website
3/4/24 19:37,기지개 고양이,사진
3/4/24 19:37,기지개 고양이,"04/13 중간
06/21 기말"
3/4/24 19:51,General Trash,"학회 등의 사유로 인하여 시험에 응시하지 못할 경우, 따로 시험을 치르거나 대체과제를 부여받는 등의 조치가 가능할지 궁금합니다."
3/4/24 19:55,TA Jisun,"For the course material, homeworks, schedules for exams, and textbooks to refer to, please visit the class website for more details.

We accept handwritten homeworks, but please make sure that those pdf files are recognizable; There are some cases where the scan app does not work properly."
3/4/24 19:55,TA Jisun,"Could you send me an email?
colleenp0515@snu.ac.kr"
3/4/24 19:57,Joyful Apeach,Joyful Apeach님이 들어왔습니다.
3/4/24 20:03,엄지척 튜브,엄지척 튜브님이 들어왔습니다.
3/4/24 20:03,.,.님이 들어왔습니다.
3/4/24 20:09,피자 먹다 자는 무지,혹시 강의계획서에는 성적평가방법을 s/u로 선택가능하다고 적혀있던데 s/u로 변경 가능한가요?
3/4/24 20:10,눈물바다 라이언,강의 녹화본이 아직 올라오지 않은건가요?
3/4/24 20:35,궁시렁 프로도,궁시렁 프로도님이 들어왔습니다.
3/4/24 20:40,불나게 일하는 네오,불나게 일하는 네오님이 나갔습니다.
3/4/24 20:43,말썽쟁이 네오,말썽쟁이 네오님이 들어왔습니다.
3/4/24 20:51,Frodo leaving office,Frodo leaving office님이 들어왔습니다.
3/4/24 21:18,부끄러운 어피치,부끄러운 어피치님이 들어왔습니다.
3/4/24 21:30,TA kibeom,"Lecture recordings take a variable amount of processing time by Zoom, so the video upload schedule will not be consistent. In the past, the video has often become available before midnight, but we cannot make any guarantees. To ensure that you receive the lecture content in a timely and consistent manner, please attend the live lectures, either in person or over Zoom."
3/4/24 21:33,애교뿜뿜 무지,In what form will the lecture video be uploaded? Will it be uploaded on eTL?
3/4/24 21:39,신난 어피치,신난 어피치님이 들어왔습니다.
3/4/24 21:43,TA kibeom,"yes, the lecture videos will be uploaded on eTL."
3/4/24 22:13,기타치는 튜브,기타치는 튜브님이 들어왔습니다.
3/4/24 22:24,응원하는 튜브,응원하는 튜브님이 들어왔습니다.
3/4/24 22:26,응원하는 튜브,안녕하세요 조교님. 혹시 HW1 관련 질문을 이 톡방에서 해도 괜찮을까요?
3/4/24 22:47,TA chaeju,물론입니다
3/4/24 22:50,응원하는 튜브,"Hw1 P4,P5에서 라이브러리(numpy) import하는 것이 허용되는지 궁금합니다"
3/4/24 22:52,응원하는 튜브,"정확히 말하자면, 스켈레톤 코드에 import되지 않은 numpy 내부 다른 라이브러리를 여쭤보는 것입니다."
3/4/24 22:59,TA chaeju,"가능하지만, Hw1의 P5같은 경우 ""대부분의 entry가 0인 행렬 A와의 행렬곱을 어떻게 효율적으로 계산할 수 있는가?""에 관한 문제이므로, 문제 의도에 맞춰서 풀어주시면 되겠습니다."
3/4/24 23:01,Neo,"성적을 S/U로 받고 싶은데, 어디에 말씀 드리면 되나요?"
3/4/24 23:02,애교뿜뿜 무지,mysnu에서 전산상으로 가능하지 않나요?
3/4/24 23:04,애교뿜뿜 무지,"수업 > 성적평가방법변경신청
이용하시면 될 것 같습니다!"
3/4/24 23:04,Neo,감사합니디!
3/4/24 23:17,응원하는 튜브,넵 감사합니다
3/4/24 23:57,Is Ryu optimal?,Is Ryu optimal?님이 들어왔습니다.
3/5/24 0:01,휘파람 프로도,휘파람 프로도님이 들어왔습니다.
3/5/24 0:37,Tube als Cheerleader,Will the make-up lecture tomorrow have additional content than today? Should you also attend the make-up lecture if you don't plan on being absent throughout the semester?
3/5/24 0:40,부탁하는 네오,"What do you mean by 'additional content than today'? As I understood, the makeup lectures this week are for potential absence or holidays this semester, so that the total number of lectures this semester doesn't get affected."
3/5/24 0:41,Tube als Cheerleader,Whether the make-up lecture won't be a repetition of the lecture today or one in the semester. I.e. will it be worth going to make-up lectures if you go to all regular ones.
3/5/24 0:44,부탁하는 네오,"I understood that it's just a normal lecture(so you must attend), and the purpose of the makeup lectures is to catch up with the required number of lectures in the entire semester. I guess there is no need for repetition since all lectures are recorded and provided online."
3/5/24 0:45,부탁하는 네오,"Mon/Wed classes have three holidays this semester, if I am correct."
3/5/24 0:50,si3k9u82,si3k9u82님이 들어왔습니다.
3/5/24 0:57,Tube als Cheerleader,I see. Thank you!
3/5/24 1:03,멋쩍은 튜브,심수기 과제 .py 파일은 찾았는데 문제는 어디 있나요? I have found the .py files of this weeks hw but i cannot find the questions ㅠㅠ where are they?
3/5/24 1:04,멋쩍은 튜브,죄송합니다... homework도 눌러지네요
3/5/24 1:04,애교뿜뿜 무지,py 파일 옆에 pdf 파일 안에 1-5번 다 있을 겁니다 (inside the pdf file attached next to .py files)
3/5/24 1:19,말썽쟁이 네오,말썽쟁이 네오님이 들어왔습니다.
3/5/24 8:48,신난 어피치,신난 어피치님이 들어왔습니다.
3/5/24 10:25,Jay-G on the couch,Jay-G on the couch님이 나갔습니다.
3/5/24 10:40,음악듣는 어피치,음악듣는 어피치님이 들어왔습니다.
3/5/24 11:01,택배 상자를 든 네오,문제 3번에 rho(X^TX)>0인 조건이 있는 것인가요? 없을 경우 alpha=0이 되면 항상 수렴하는 것 같습니다.
3/5/24 11:11,빈털터리 제이지,빈털터리 제이지님이 들어왔습니다.
3/5/24 11:23,화나서 방방 뛰는 튜브,X^TX는 PSD입니다
3/5/24 11:28,인사하는 제이지,삭제된 메시지입니다.
3/5/24 11:46,부탁하는 무지,부탁하는 무지님이 들어왔습니다.
3/5/24 12:01,.,.님이 들어왔습니다.
3/5/24 12:07,즐거운 네오,즐거운 네오님이 들어왔습니다.
3/5/24 12:21,기지개 고양이,"2, 3번 문제의 경우 θ가 발산함을 보이라는 건가요, f(θ)가 발산함을 보이라는 건가요?"
3/5/24 12:23,TA Hyojun,theta_k가 발산함을 보이시면 됩니다
3/5/24 12:31,기지개 고양이,감사합니다!
3/5/24 12:32,열심히 일하는 네오,열심히 일하는 네오님이 들어왔습니다.
3/5/24 12:58,애교뿜뿜 어피치,애교뿜뿜 어피치님이 들어왔습니다.
3/5/24 13:26,응원하는 튜브,"과제 제출 양식(파일 형식 및 파일명)이 어떻게 될까요? 
답안 필기를 pdf 첨부 및 P4, P5에 대해 작성한 코드 파일을 그대로 첨부하면 될까요?"
3/5/24 13:28,응원하는 튜브,"또한 P4에서 'Experimentally demonstrate' 라고 나와있는데, 그럼 해당 learning rate에서 수렴 및 발산하는 analytic한 이유를 기술할 필요가 없다고 생각해도 괜찮을까요?"
3/5/24 14:08,베개를 부비적대는 라이언,오늘 수업 대면으로도 진행되나요?
3/5/24 14:10,부탁하는 네오,"홈페이지 가보면 On the first week, we will have two make-up lectures on March 5th, 5:00–6:15pm at 28-301, and March 7th, 7:00–8:15pm at 28-101, to make up for absences planned later in the semester. These make-up lectures are in addition to the regular lectures on March 4th and March 6th, 5:00–6:15pm at 28-102.라고 나와있네요"
3/5/24 14:14,베개를 부비적대는 라이언,아 그렇네요 감사합니다
3/5/24 14:17,Ernest Hemingway,Ernest Hemingway님이 들어왔습니다.
3/5/24 14:57,엄지척 제이지,"과거/이번학기 학점(A-F, S/U) 커트라인을 알 수 있을까요?"
3/5/24 15:14,눈물바다에 빠진 라이언,"문제 4에서 random starting points로 GD를 수행하고 결과를 experimentally demonstrate하라고 했는데, starting points를 몇 개를 잡는지는 자유인가요? 또한 experimentally demonstrate라고 하는 것이 py 파일에서 주석이나 print 등을 이용해야 하는 것인가요, 아니면 py 파일과는 별개로 pdf 파일에 설명해야 하나요?"
3/5/24 15:43,울고있는 제이지,울고있는 제이지님이 들어왔습니다.
3/5/24 15:46,라면먹는 제이지,라면먹는 제이지님이 나갔습니다.
3/5/24 16:11,TA Hyojun,넵 analytic하게 이유를 기술할 필요는 없습니다.
3/5/24 16:11,TA Hyojun,넵 random point의 개수는 따로 정해져 있지 않습니다. 적당한 개수의 random point를 잡아 문제에서 기술된 현상이 일어남을 보여주시면 됩니다.
3/5/24 16:12,TA Hyojun,과제 제출 관련 공지가 etl에 업로드되었으니 확인해 주시면 감사하겠습니다!
3/5/24 16:13,TA chaeju,Please check the annoucement regarding the homework submission on ETL.
3/5/24 16:14,애교뿜뿜 무지,사진
3/5/24 16:14,애교뿜뿜 무지,Hello. I cannot understand why f(thetha)-inf(f(theta)) part is positive. May I have some extra explanations?
3/5/24 16:15,엄지척 튜브,because inf is infimum
3/5/24 16:16,엄지척 튜브,before we assume that inf(f(theta)) > - \inf
3/5/24 16:16,애교뿜뿜 무지,Oh I found it. Thank you!
3/5/24 16:18,손을 번쩍 든 무지,손을 번쩍 든 무지님이 들어왔습니다.
3/5/24 16:31,.,"오늘 일정이 있어 makeup class에 참석하지 못하는데, 오늘 수업도 녹화되어서 올라오는지 궁금합니다"
3/5/24 16:40,TA Gyeongmin,"네, make-up class 또한 녹화본이 올라갈 예정입니다.
Both make-up classes will be recorded and provided."
3/5/24 16:49,기지개 고양이,삭제된 메시지입니다.
3/5/24 16:58,화난 라이언,Is zoom link for today's make-up class same as before?
3/5/24 16:59,옐로카드 프로도,이전링크로 회의진행이 안되는 것 같습니다
3/5/24 17:01,쑥스럽게 인사하는 프로도,@TA Jisun @TA Jaewook @TA kibeom 확인 부탁드립니다..!
3/5/24 17:02,베개를 부비적대는 라이언,베개를 부비적대는 라이언님이 들어왔습니다.
3/5/24 17:05,TA Jisun,지금도 접속이 안되시나요?
3/5/24 17:06,TA Jisun,"https://snu-ac-kr.zoom.us/j/92896163424
Meeting ID: 928 9616 3424
Passcode: 812489"
3/5/24 17:06,옐로카드 프로도,지금 됩니다!
3/5/24 17:23,권투하는 무지,"Hi, I am aware that auditors are not allowed to submit homework. Are the auditors also not allowed to take mid-term and final exams?"
3/5/24 17:36,하트뿅뿅 라이언,하트뿅뿅 라이언님이 들어왔습니다.
3/5/24 18:20,멋쩍은 튜브,"Is mod(k,N) k(mod N)?"
3/5/24 18:22,인사하는 프로도,i think so. it is also explained as k%N
3/5/24 18:23,콘이 웃긴 무지,콘이 웃긴 무지님이 들어왔습니다.
3/5/24 18:52,애교뿜뿜 무지,애교뿜뿜 무지님이 나갔습니다.
3/5/24 19:16,최은석,최은석님이 나갔습니다.
3/5/24 19:57,권투하는 무지,권투하는 무지님이 들어왔습니다.
3/5/24 20:06,TA chaeju,"We have posted an important announcement regarding homework grading scheme and solution provision on ETL, so please make sure to read it."
3/5/24 20:20,Tube als Cheerleader,"On slide 5 of the slides, it says that the optimal value is the infimum of the set of f of feasible values for theta. Why is it not the minimum? Because this would mean that there are problems where the optimal value is not actually a feasible value. Is that true?"
3/5/24 20:31,경례하는 프로도,경례하는 프로도님이 들어왔습니다.
3/5/24 20:39,TA Youngmin,"Every subset of R that are bounded below are guaranteed to have a infimum in R while it may not have a minimum.
Therefore, defining optimal value using infimum, rather than minimum, ables us to discuss general cases.

It is true that the optimal value may not have a corresponding feasible point as the following context of slide 5 suggests.
(A solution may not exist)"
3/5/24 20:45,Tube als Cheerleader,Is R the set of function values of every feasible point?
3/5/24 20:47,TA Youngmin,"R was used to denote ""set of real numbers"""
3/5/24 20:48,인사하는 프로도,"I have a question about the submission format of our homeworks. I prefer using colab, so can I work on colab and submit the ipynb and pdf files downloaded from it?"
3/5/24 20:51,초롱초롱 네오,초롱초롱 네오님이 나갔습니다.
3/5/24 20:52,TA Youngmin,No problem as far as downloaded files are readable. 
3/5/24 21:58,비옷입은 튜브,"are recorded videos of all lectures, not just make up lectures, will be upload on the internet?"
3/5/24 22:14,nnd,yes
3/5/24 22:16,Joshua,Joshua님이 들어왔습니다.
3/6/24 8:30,소심한 네오,소심한 네오님이 들어왔습니다.
3/6/24 9:34,이준희,이준희님이 나갔습니다.
3/6/24 10:18,Apeach in love,Apeach in love님이 들어왔습니다.
3/6/24 10:23,정우성,정우성님이 나갔습니다.
3/6/24 10:28,선풍기 바람 쐬는 어피치,선풍기 바람 쐬는 어피치님이 들어왔습니다.
3/6/24 11:04,건배하는 프로도,건배하는 프로도님이 들어왔습니다.
3/6/24 12:21,경제학도,삭제된 메시지입니다.
3/6/24 12:23,기지개 고양이,"혹시 성적반영비중(중간/기말/과제), 평가방법(절대/상대//드랍포함여부) 등을 알 수 있을까요?

수업 웹사이트에는 정보가 없는 것 같고, 수강신청사이트에 기재된 정보가 맞는지 궁금합니다(대부분은 수신 사이트엔 default로 돼있어서 믿기가 힘들어서 그렇습니다)"
3/6/24 12:24,애교뿜뿜 무지,성적반영비중은 기재된 바와 같을 겁니다
3/6/24 12:24,청소하는 튜브,수업 웹사이트에도 있어요
3/6/24 12:27,기지개 고양이,반영비중은 확인하였습니다. 감사합니다.
3/6/24 12:28,기지개 고양이,혹시 절대평가 기준(최종 성적 기준 컷)을 알 수 있을까요?
3/6/24 12:30,눈물바다 라이언,"Hi, I am considering changing to an auditor. Will you plan to accept new auditors?"
3/6/24 12:37,잠오는 라이언,잠오는 라이언님이 들어왔습니다.
3/6/24 13:50,Ryan working out,Ryan working out님이 나갔습니다.
3/6/24 14:10,부끄러워하는 라이언,부끄러워하는 라이언님이 들어왔습니다.
3/6/24 14:13,아이디어 프로도,아이디어 프로도님이 들어왔습니다.
3/6/24 14:55,주피터,주피터님이 들어왔습니다.
3/6/24 15:24,떨고있는 어피치,떨고있는 어피치님이 나갔습니다.
3/6/24 15:46,빈털터리 제이지,빈털터리 제이지님이 들어왔습니다.
3/6/24 15:47,리듬타는 제이지,리듬타는 제이지님이 들어왔습니다.
3/6/24 16:48,A+ is All you need,A+ is All you need님이 들어왔습니다.
3/6/24 16:59,초롱초롱 어피치,초롱초롱 어피치님이 들어왔습니다.
3/6/24 17:01,소심한 네오,혹시 줌으로 수업 들으시는 분들 소리 들리시나요?
3/6/24 17:01,청소하는 튜브,안들립니다..
3/6/24 17:02,TA Jaewook,이제 들리시나요?
3/6/24 17:02,소심한 네오,네네! 잘 들립니다
3/6/24 17:02,청소하는 튜브,넵
3/6/24 17:02,수강생,넵 들립니다. 감사합니다.
3/6/24 17:03,리듬타는 제이지,리듬타는 제이지님이 나갔습니다.
3/6/24 17:12,tyy,tyy님이 나갔습니다.
3/6/24 17:41,hi,hi님이 들어왔습니다.
3/6/24 17:50,.,.님이 들어왔습니다.
3/6/24 18:07,.,.님이 나갔습니다.
3/6/24 18:59,졸린 무지,졸린 무지님이 나갔습니다.
3/6/24 19:11,눈빛 애교 어피치,"Hi, if I'm understanding right, shouldn't the term be first order?"
3/6/24 19:11,눈빛 애교 어피치,사진
3/6/24 19:19,치즈케이크,"IIRC, the first order-term is the one with the gradient."
3/6/24 19:21,...,It's big O 
3/6/24 19:22,눈빛 애교 어피치,아 big O군요. 감사합니다 ㅎㅎ
3/6/24 20:05,정영준,정영준님이 나갔습니다.
3/6/24 20:39,기지개 고양이,"과제1번 문제5번의 경우, 지시한 대로 작성한 소스코드(ipynb/py)를 직접 과제로 제출하면 되는것이지요?"
3/6/24 20:49,TA Hyojun,넵 맞습니다. 5번 문제는 작성하신 소스코드만 .py 혹은 .ipynb 파일로 제출해주시면 됩니다.
3/6/24 20:50,기지개 고양이,감사합니다!!
3/6/24 20:58,부탁하는 네오,"과제제출 안내에
For the coding problems, you should submit the code file in either .py file or .ipynb file, along with a PDF file that displays the executed results.라고 되어있는데요,
along with a PDF file이라는 게 코딩문제의 풀이에 해당하는 파일이 이론문제랑 별개로 있어야 한다는 것일까요? 아니면 (이론문제)+(코딩문제 설명)을 하나의 pdf파일로 제출해야 하는 것일까요?
더불어 코딩문제가 하나가 아닐 때 파이썬 파일을 하나로 제출해야 하는지 궁금합니다"
3/6/24 21:11,경례하는 프로도,경례하는 프로도님이 나갔습니다.
3/6/24 21:23,떨고있는 어피치,떨고있는 어피치님이 들어왔습니다.
3/6/24 21:39,음료수 마시는 어피치,과제 5번의 경우 행렬 연산 그대로 하는 것보다 연산량 줄이는게 목표인 것 같은데
3/6/24 21:39,오일파스타,오일파스타님이 나갔습니다.
3/6/24 21:39,음료수 마시는 어피치,꼭 필요한 연산만 하면 주어진 형식대로 np.asarray([list comprehension])으로 풀지 않아도 괜찮나요?
3/6/24 21:47,돈다발 들고 좋아하는 무지,돈다발 들고 좋아하는 무지님이 들어왔습니다.
3/6/24 21:48,TA Hyojun,넵 꼭 Hint와 같은 형식으로 코드를 작성하실 필요는 없습니다.
3/6/24 21:48,TA Hyojun,삭제된 메시지입니다.
3/6/24 21:49,TA Hyojun,"코딩 과제 문제 중에는 loss를 plot하라는 등 실행 결과가 필요한 문제가 많습니다.
이와 같은 경우 소스코드만 제출하시면 채점이 어렵기 때문에 소스코드(.py 혹은 .ipynb) 뿐 아니라 실행 결과(.pdf)를 함께 제출해 주시면 됩니다.
pdf와 소스 코드를 각각 하나의 파일로 합쳐서 제출하실 필요는 없지만, 파일이 여러 개인 경우 각각이 잘 구분될 수 있도록 파일명을 바꿔주시면 감사하겠습니다."
3/6/24 21:49,TA Hyojun,"Many coding assignment problems require execution results such as plotting the loss.
In such cases, grading is difficult with only the source code submitted. Therefore, please submit both the source code (.py or .ipynb) and the execution results (.pdf).
You don't need to combine the PDF and source code into one file each, but if there are multiple files, please rename them so that each can be distinguished clearly."
3/6/24 22:16,음료수 마시는 어피치,혹시 강의자료에서 증명한 명제들은 따로 증명없이 사용해도 괜찮나요?
3/6/24 22:23,TA Hyojun,넵 따로 증명 없이 사용하셔도 무방합니다.
3/6/24 23:58,Neo points at you,Neo points at you님이 들어왔습니다.
3/7/24 0:00,열심히 일하는 네오,"문제 3에서의 힌트에서 요구하는 식을 만들어냈는데 그 이후에 어떻게 전개해야하는지 전혀 모르겠네요. 혹시 eigenvalue와 관련한 특정 정리를 써야 풀 수 있는건가요?

거기에 ""diverges for most starting points""의 most starting points를 Lebesgue measure의 관점에서 표현하라는게 잘 모르겠네요."
3/7/24 0:37,TA Youngmin,"When regarding powers of matrices, considering eigenvalues are a common appraoch. As condition related to spectral radius was given, you may want to use it.

Lebesgue measure and the term ""almost everywhere"" was introduced for those who are wondering how ""most"" could be mathematically precised.
If you are unfamiliar with measure theory, it is enough to prove that theta diverges for all starting points except for a lower dimensional set as the remark suggests."
3/7/24 0:38,초롱초롱 무지,초롱초롱 무지님이 나갔습니다.
3/7/24 0:51,열심히 일하는 네오,I don't think we have powers of matrices in problem 3. 
3/7/24 0:52,열심히 일하는 네오,Oh you mean we don't use any theorems about eigenvalues since we don't have powers of matrices.
3/7/24 1:12,TA Youngmin,I recommend considering eigenvalues as a tool for solving the problem.
3/7/24 1:43,_,_님이 들어왔습니다.
3/7/24 2:35,하트뽀뽀 어피치,하트뽀뽀 어피치님이 나갔습니다.
3/7/24 10:33,General Trash,메일 보내드렸습니다! 확인 부탁드립니다.
3/7/24 10:45,일하기 싫은 네오,삭제된 메시지입니다.
3/7/24 11:13,경례하는 프로도,경례하는 프로도님이 들어왔습니다.
3/7/24 12:31,jaehyeok,jaehyeok님이 나갔습니다.
3/7/24 13:12,하트뿅뿅 라이언,"안녕하세요, HW1의 문제들(특히 1번 문항) 모두 분모중심표현을 사용한다고 봐도 무방할까요?"
3/7/24 13:16,불나게 일하는 네오,불나게 일하는 네오님이 나갔습니다.
3/7/24 13:16,TA chaeju,Yes it is. You may found that by comparing the dimension of the both side of part (a).
3/7/24 13:24,.,.님이 들어왔습니다.
3/7/24 14:40,하트뿅뿅 라이언,감사합니다!
3/7/24 14:43,일하기 싫은 네오,삭제된 메시지입니다.
3/7/24 14:50,눈빛 애교 어피치,눈빛 애교 어피치님이 나갔습니다.
3/7/24 14:57,눈물바다에 빠진 lion,눈물바다에 빠진 lion님이 들어왔습니다.
3/7/24 19:43,붕붕,붕붕님이 들어왔습니다.
3/7/24 22:12,비옷입은 튜브,주피터 노트북 pdf로 바꾸는 것을 공지에 올라온 사이트 보고 했는데 잘 안되는데 뭐가 잘못된 건지 잘 모르겠습니다 컴퓨터 cmd에서 코드 두개 엔터 누르고 주피터 노트북  파일에서 다운로드 들어갔더니 에러가 계속 뜹니다 
3/7/24 22:39,부탁하는 무지,전 주피터 업데이트하니깐 해결되는 것 같더라고요
3/7/24 22:40,옐로카드 프로도,안되시면 html로 다운 후 pdf로 변경하는 홈페이지 이용하시면 됩니다!
3/7/24 22:40,비옷입은 튜브,html로 바꾸고 인쇄에서 pdf로 저장 한 것도 인정 되나요?
3/7/24 22:43,기지개 고양이,저는 그냥 크롬에서 페이지 인쇄했습니다
3/7/24 22:58,뿅뿅 네오,뿅뿅 네오님이 나갔습니다.
3/7/24 22:59,TA Hyojun,혹시 어떤 에러 메시지가 뜨는지 보여주실 수 있나요? 다른 분들이 말씀해 주신 대로 html로 다운로드하고 pdf로 바꾸거나 페이지 인쇄하셔도 상관없습니다.
3/7/24 23:02,말썽쟁이 네오,말썽쟁이 네오님이 나갔습니다.
3/8/24 1:35,.,For the coding problem can I use colab and submit it in .ipynb files?
3/8/24 1:38,TA chaeju,"Yes, however, you also need to attach a PDF file displaying the execution results.

Please check the ETL announcement."
3/8/24 1:59,멋쩍은 튜브,"그러면
1. 1~3 문풀 파일
2. 코드 파일
3. 코드 설명 pdf
이렇게 올리나요 아니면 1과 3을 합치나요?"
3/8/24 2:03,TA chaeju,"You don't need to unify the PDF files. but if there are multiple files, please rename them so that each can be distinguished clearly."
3/8/24 11:54,신난 어피치,신난 어피치님이 들어왔습니다.
3/8/24 12:38,부끄러운 어피치,부끄러운 어피치님이 나갔습니다.
3/8/24 13:08,으쓱으쓱 어피치,으쓱으쓱 어피치님이 들어왔습니다.
3/8/24 13:25,으쓱으쓱 어피치,"hw 1-3에서 spectral radius의 정의가 the largest eigenvalue라고 되어있는데, 정의를 따로 찾아보니, the maximum of the absolute values of eigenvalues라고 되어있더라구요. rho >= 0라고 생각해도 무방할까요?"
3/8/24 13:28,기지개 고양이,X^T X 특성상 고윳값이 음수가 아니예요
3/8/24 13:29,기지개 고양이,.
3/8/24 13:31,애교뿜뿜 어피치,애교뿜뿜 어피치님이 들어왔습니다.
3/8/24 13:33,비옷입은 튜브,혹시 올해 중간 기말 오픈북인가요 클로즈 북 시험인지 알 수 있을까요?
3/8/24 14:21,초롱초롱 네오,초롱초롱 네오님이 들어왔습니다.
3/8/24 15:24,시무룩한 튜브,"Ch2. Ppt p.3의 attempt 1) 에서, 왜 단순히 loss function를 최소화하는 게 아니라, supremum 값을 최소화 하는 건지 궁금합니다."
3/8/24 15:26,초롱초롱 튜브,초롱초롱 튜브님이 나갔습니다.
3/8/24 15:34,밥줘,worst case를 최소화 하는 느낌 아닌가요? 그래서 다소 pessimist적인 방법이라고 나와있는거고요
3/8/24 15:35,불금 네오,불금 네오님이 나갔습니다.
3/8/24 15:37,치즈케이크,적분이 없는 함수 공간에서 두 함수의 거리를 재는 가장 직관적인 방법이 supremum norm 아닌가 싶어요
3/8/24 15:44,시무룩한 튜브,감사합니다
3/8/24 15:47,튜브낀 튜브,"In the last slide of chapter 1, what exactly is f_sub_w? That is, how does the function f_sub_w relate to the random variable w?"
3/8/24 16:46,선풍기 바람 쐬는 어피치,선풍기 바람 쐬는 어피치님이 나갔습니다.
3/8/24 17:06,휘파람 프로도,휘파람 프로도님이 들어왔습니다.
3/8/24 17:22,권투하는 무지,"in problem 5, it says 'Implement the __matmul__ methods so that the above gradient descent code runs without modification'. But don't I have to substitute A@x with A.__matmul__(x) in order to use the __matmul__ method? I don't see a way I can use the method without modifying the provided GD code."
3/8/24 17:24,눈물바다에 빠진 라이언,__matmul__ is a magic method so that we can override the @(matrix multiplication) operator.
3/8/24 17:33,권투하는 무지,oh thank you now I get it!
3/8/24 17:33,화난 라이언,I think it can be helpful to check python tutorials uploaded on the professor’s website and docs that deal with objects(class) of numpy.
3/8/24 18:00,기지개 고양이,"Python의 class 객체의 연산을 할때는 magic method라 불리는 것을 사용해요

A+2 의 연산도, A의 class에서 __add__ 라는 + 연산자에 대한 함수가 정의돼있기에 가능한거죠

MyClass.__matmul__() 도 행렬곱(matrix mult)연산자 @ 작용시 어떤 값을 리턴할지 정의하는 겁니다"
3/8/24 18:12,멋쩍은 튜브,1번에서 theta 는 p×1 행렬이라고 생각하면 되나요?
3/8/24 18:16,기지개 고양이,문제에 적힌대로 X와 행렬곱이 가능하려면 그래야될듯 해요
3/8/24 18:19,장수한,장수한님이 나갔습니다.
3/8/24 18:23,신난 어피치,사진
3/8/24 18:23,신난 어피치,혹시 MLE에서 다음 minimization으로 넘어가는 논리가 어떻게 되는지 설명해줄 수 있나요? 계속 제 논리로는 maximize인 것 같아서 질문합니다.
3/8/24 18:25,청소하는 튜브,#NAME?
3/8/24 18:26,신난 어피치,아 음수 부호를 놓쳤네요..감사합니다!
3/8/24 18:51,멋쩍은 튜브,사진
3/8/24 18:51,멋쩍은 튜브,Is this what the notation means in #1-b?
3/8/24 18:58,음료수 마시는 어피치,음료수 마시는 어피치님이 들어왔습니다.
3/8/24 18:58,열심히 일하는 네오,Yes!
3/8/24 19:01,소심한 네오,Vector Norm
3/8/24 19:02,밥줘,search for Norm in google
3/8/24 19:15,멋쩍은 튜브,Thanks
3/8/24 19:52,초롱초롱 네오,초롱초롱 네오님이 나갔습니다.
3/8/24 20:54,얼굴마사지하는 제이지,얼굴마사지하는 제이지님이 들어왔습니다.
3/8/24 20:58,하트뿅뿅 라이언,하트뿅뿅 라이언님이 들어왔습니다.
3/8/24 21:42,휘파람 프로도,휘파람 프로도님이 나갔습니다.
3/8/24 22:48,말썽쟁이 네오,말썽쟁이 네오님이 들어왔습니다.
3/9/24 2:36,시무룩한 튜브,사진
3/9/24 2:37,시무룩한 튜브,혹시 저 길이가 왜 -b/ llall 인가요?
3/9/24 2:42,ㅇㅇ,삭제된 메시지입니다.
3/9/24 2:43,ㅇㅇ,"aTx = ||a|| * (x \cdot \hat{a})
x의 성분 중 a와 평행한 부분을 생각하시면 될 것 같습니다"
3/9/24 11:29,눈빛 애교 어피치,눈빛 애교 어피치님이 나갔습니다.
3/9/24 11:31,ERE23,ERE23님이 들어왔습니다.
3/9/24 12:00,deee,혹시 shuffled cyclic sgd가 unbiased estimation property를 가지지 않는 이유를 알 수 있을까요?
3/9/24 12:43,밥줘,"그냥 cyclic sgd는 모든 index를 사용은 하지만 매 사이클마다 바뀌지 않는(mod (1,N) = mod (1+N,N)) 일정한 규칙을 갖는다는 문제가 있는데
shuffled cyclic sgd는 똑같이 모든 index를 하나씩 훑지만 N사이클마다 index 순서 자체를 랜덤하게 바꿔줘서 그 문제를 해결한거 아닐까요??"
3/9/24 12:43,밥줘,저도 부족하지만 혹시 기여가 있을까 싶어 의견 남깁니다..
3/9/24 12:45,벌 서는 라이언,공지에 올려주신 메뉴얼에 따라서 주피터 노트북을 pdf로 변환하려고 하고 있는데 다음 사진과 같은 에러 메세지와 함께 실행이 되지 않습니다. 이유를 알 수 있을까요?
3/9/24 12:45,벌 서는 라이언,사진
3/9/24 12:49,General Trash,저 명령은 python code가 아니라 shell command라서 앞에 느낌표 하나 붙이시면 될 것 같습니다.
3/9/24 12:56,벌 서는 라이언,사진
3/9/24 12:57,벌 서는 라이언,감사합니다. 그런데 실행은 되나 아래와 같은 에러 문구가 다시 나오는 것 같습니다. 이 같은 에러는 어떻게 해결할 수 있는지 알 수 있을까요?
3/9/24 12:58,기지개 고양이,저는 걍 포기하고 크롬 자체의 화면 인쇄 이용해서(Ctrl+P) pdf로 저장했어요
3/9/24 13:02,인사하는 프로도,colab 쓰실 줄 아시면 코드 colab으로 옮겨서 ‘인쇄 > pdf로 저장’ 하시면 됩니다
3/9/24 13:14,쑥스럽게 인사하는 프로도,"조교님, 혹시 문제 5에서 A의 전치행렬의 곱을 구현할때, 모든(n,r) 쌍에 대해 연산이 올바르게 작동해야 하는지, 아니면 주어진 (20,3) 조합에 대해서만 작동해도 되는지 여쭙니다. n이 r에 대해 일정 크기 이상 크지 않은 경우에도 연산이 작동해야 할까요?"
3/9/24 13:25,TA chaeju,n>=r을 만족하는 일반적인 n과 r에 대해 구현해 주세요.
3/9/24 13:28,쑥스럽게 인사하는 프로도,감사합니다
3/9/24 13:36,TA chaeju,"You may understand f_w (theta) as f(theta, w). the expression f_w (theta) emphasizes that the function f relys on the random variable w

In chapter 5, we have to deal with the continuous random variables and the expectation with respect to them"
3/9/24 14:49,부끄러워하는 라이언,부끄러워하는 라이언님이 나갔습니다.
3/9/24 16:55,TA Jisun,"In case of cyclic sgd, either shuffled or not, there is a dependency to the prior iterates. Specifically, if we denote by x_k the k-th iterate and g_k the k-th gradient given N (>=2) samples, g_2 has dependency on x_1, and E[g_2 | x_1] is not the true (full) gradient at x_2 ,therefore biased. (Not full gradient because the index at the first iteration cannot be included at the second update.)"
3/9/24 16:55,얼굴마사지하는 제이지,얼굴마사지하는 제이지님이 나갔습니다.
3/9/24 18:46,deee,감사합니다!!
3/9/24 19:16,초롱초롱 튜브,초롱초롱 튜브님이 들어왔습니다.
3/10/24 0:01,멋쩍은 튜브,"과제 5번을 할 때, @연산을 하는 코드를 만들어야 하는데 이때 A가 주어진 저 형태일 때만 먹히는 코드여도 괜찮은 게 맞나요?"
3/10/24 0:06,TA Hyojun,네 n>=r을 만족하는 일반적인 n과 r에 대해 문제와 같이 정의된 행렬 A를 대상으로 하는 @연산을 구현하시면 됩니다.
3/10/24 0:17,피스메이커 프로도,피스메이커 프로도님이 나갔습니다.
3/10/24 0:46,치맥하는 제이지,(코드가 들어있는)주피터 노트북을 pdf로 바꿔서 내면 코드 파일은 따로 제출하지 않아도 되는건가요?
3/10/24 0:48,TA Hyojun,아뇨 코드 파일도 .py나 .ipynb 형식으로 같이 제출해 주시길 바랍니다.
3/10/24 0:49,치맥하는 제이지,아 넵 알겠습니다
3/10/24 1:19,가을뮤트,가을뮤트님이 들어왔습니다.
3/10/24 1:33,벙찐 튜브,Can I use a theory of linear algebra solving problem 3?
3/10/24 1:36,TA Youngmin,"Yes, you may use well known properties of matrices or theorems from linear algebra class without any proof."
3/10/24 3:11,하트뽀뽀 어피치,하트뽀뽀 어피치님이 들어왔습니다.
3/10/24 11:22,튜브낀 튜브,사진
3/10/24 11:23,튜브낀 튜브,"How can we move from first line to second line? I don’t see where Yi, not P(Yi), comes from."
3/10/24 11:33,Ernest Lee,"웹사이트 상에서도 출석 비율이 없긴 한데, 이 분이 TA이신지 잘 몰라서요.. 다른 내용 읽어보니 'must attend'라는 이야기도 좀 있어서 혹시 출석 없는 게 맞는 건지 TA분께서 알려주실 수 있으실까요?"
3/10/24 11:35,시무룩한 튜브,"과제 3번에서 (로)가 spectral radius라고 하는데, 이게 고유값 중 최댓값이라고 생각했습니다.

근데 그 뒤에 i.e. (로)X^TX 도 largest eigenvalue라고 나옵니다.

(로)는 X의 최대고유값 그런식으로 해석해야되는 건가요?"
3/10/24 11:38,신난 어피치,로(A)가 A 행렬의 고윳값 중 최댓값을 의미해 로(X^TX)는 X^TX의 최대 고윳값을 의미합니다
3/10/24 11:50,TA Youngmin,"Keep in mind that P(Y) is defined to be (1 0)^T if Y = -1 and (0 1)^T if Y = 1 in this case.
By considering each cases with definition of H and f, you will obtain the second line."
3/10/24 11:50,TA Youngmin,"Yes, there is no attendance in this course."
3/10/24 11:51,열심히 일하는 네오,사진
3/10/24 11:54,열심히 일하는 네오,위에 어느 분께서 이거 관련해서 질문하셨는데 저도 수업들으면서 a벡터에 대한 length가 ||a||가 아닌 -b/||a||가 되어지는지 잘 모르겠네요. 애초에 저 length표현하신게 a벡터에 대한 length맞나요? b는 intersection이라 하셨는데 x-intersection으로 보시고 말씀하신거겠죠?
3/10/24 11:55,열심히 일하는 네오,수업에서도 교수님께서 그림을 저렇게 그리셨는데 그림 부분이 헷갈리네요
3/10/24 11:56,열심히 일하는 네오,Zoom으로는 44:09대입니다
3/10/24 11:59,열심히 일하는 네오,죄송합니다 Lecture 4 recording 44분대입니다
3/10/24 11:59,기지개 고양이,"(a1,a2)ㆍ(x1,x2)+b=0 ~ (0,0)  사이 거리
이니
|b|/sqrt(a1²+a2²) = -b/||a||"
3/10/24 12:00,음악듣는 어피치,"(d-1)-dim. surface 의 식을 x벡터를 a 방향으로 내적하여 구한 a방향 성분이 특정 수 (-b) 가 된다고 이해하면 편할듯 합니다.
단위벡터로 내적해야하니 ||a|| 로 나누어주고요
따라서 그림의 -b/||a|| 는 x의 a 방향 성분 크기(원점에서 평면까지 거리)에 해당되는것 같습니다"
3/10/24 12:00,퇴근하는 프로도,a의끝점이 초록직선 위에 있는게 아니에요~
3/10/24 12:01,퇴근하는 프로도,그림때문에 헷갈리시는듯
3/10/24 12:01,열심히 일하는 네오,아 그랬군요 감사합니다
3/10/24 12:01,열심히 일하는 네오,다들 너무 감사합니다!
3/10/24 12:02,Frodo leaving office,Frodo leaving office님이 나갔습니다.
3/10/24 12:11,운동하는 라이언,운동하는 라이언님이 나갔습니다.
3/10/24 13:16,시무룩한 튜브,5번
3/10/24 13:16,시무룩한 튜브,오타가 났네요. 죄송합니다.
3/10/24 13:16,시무룩한 튜브,혹시 5번 문제에서 결과적으로 무엇을 문제가 요구하는지가 헷갈려서 질문합니다.
3/10/24 13:17,시무룩한 튜브,최적화 문제와 문제 조건들을 나열해놓은 것 같은데… 뭔가를 show 하라거나 하는 게 없어서요!
3/10/24 13:18,TA Hyojun,"주어진 행렬곱 연산을 행렬 A를 만들어서 직접 계산하는 방식이 아닌, 더 효율적인 연산 방식을 구현하라는 것이 해당 문제의 요구사항입니다."
3/10/24 13:22,TA Hyojun,"즉, A@x와 A.T@x 연산을 구현하시면 됩니다."
3/10/24 13:24,TA Hyojun,그리고 문제를 다시 확인해 보니까 Problem 5에도 test code가 있어서 Problem 5도 실행 결과를 .pdf로 함께 제출해 주시면 감사하겠습니다. 번거롭게 해 드려 죄송합니다 ㅠㅠ
3/10/24 14:08,벌 서는 라이언,s/u로 성적평가방법을 전환할 시 평가 기준을 알 수 있을까요?
3/10/24 14:09,기지개 고양이,"아 저도 그리 생각해서 5번 pdf 제출했습니다
감사합니다 "
3/10/24 14:15,시무룩한 튜브,혹시 손글씨 pdf와 코드결과pdf를 합쳐서 내도 되는 건가요? 아니면 분리해서 제출해야 하나요?
3/10/24 14:29,TA Hyojun,둘 다 상관없습니다. 다만 분리해서 제출하시는 경우 잘 구분될 수 있도록 파일명을 적절히 바꿔주시기 바랍니다.
3/10/24 14:33,_,안녕하세요 문제풀이를 코드 ipynb에 라텍스로 넣어도 될까요? 결과 pdf에도 ipynb 변환이라 풀이가 들어갑니다
3/10/24 14:40,TA Hyojun,넵 가능합니다.
3/10/24 14:52,멋쩍은 튜브,실행 결과가 상수로 나오는데 그러면 .py 쓰는 사람들은 그 답을 캡쳐해서 추가하라는 말씀이 맞나요?
3/10/24 14:53,멋쩍은 튜브,Pdf 제출이 어떤 느낌인지 잘 모르겠네요...
3/10/24 15:02,쑥스럽게 인사하는 프로도,삭제된 메시지입니다.
3/10/24 15:15,부탁하는 무지,부탁하는 무지님이 들어왔습니다.
3/10/24 15:20,TA Hyojun,"네 TA들이 코드를 직접 돌려보지 않고도 실행 결과를 확인할 수 있도록 답을 캡쳐해서 추가하시거나, 주피터 노트북을 사용하시는 경우 .ipynb 파일을 .pdf 파일로 변환하셔서 제출하시면 됩니다."
3/10/24 16:58,.,Grading method를 snu로 바꿨는데 바뀐거 어떻게 확인하나요? 반영됐는지 여부가 궁금해서요
3/10/24 20:13,신나는 프로도,신나는 프로도님이 나갔습니다.
3/10/24 20:34,부탁하는 네오,여기서 1/2를 붙인 거는 미분했을 때 계수 1로 예쁘게 보이려고 하기 위한 목적뿐일까요? error square에 1/2이 붙으면 개념적으로 이상한 것 같아서 질문드립니다(물론 계수랑 상관없이 결과가 같은 것은 당연히 알고 있습니다)
3/10/24 20:34,부탁하는 네오,사진
3/10/24 20:37,베개를 부비적대는 라이언,2norm이라 그런 거 아닌가요
3/10/24 20:37,TA chaeju,"네, 그렇습니다. 말씀하신대로 양수를 곱하는건 minimization에 영향을 끼치지 않으니까요"
3/10/24 20:42,청소하는 튜브,Is it allowed for submitting the result of prob 4 and 5( and any code implementation & analysis problem after hw1) with the pdf file of prob 1 to 3? I'm curious about you're going to check the format of submission strictly.
3/10/24 20:46,TA Youngmin,We won't be strict on submission formats as long as we are able to recognize codes and results for the problems.
3/10/24 20:47,청소하는 튜브,Thank You!
3/10/24 21:26,신난 어피치,신난 어피치님이 들어왔습니다.
3/10/24 21:34,부탁하는 네오,"gradient의 row, column 표현 convention에서 좀 헷갈립니다
과제1번의 a에서 뒤에 붙는 게 X_i가 아니라 X_i^T이어야 하는 것 아닌가요? X_i는 column인데 gradient를 column으로 표현하는 게 맞는 건가요?
전반적인 흐름은 이해가 가는데, chain rule 적용할 때 곱의 순서라든지 차원이 잘 이해가 되지 않습니다"
3/10/24 21:34,부탁하는 네오,사진
3/10/24 21:35,부탁하는 네오,l_i(\theta)는 R^p에서 R로 가는 함수이고 따라서 그래디언트가 길이 p의 row로 표현되어야 한다고 생각했습니다
3/10/24 21:39,청소하는 튜브,분모중심표현을 검색해보세요
3/10/24 21:53,시무룩한 튜브,혹시 4번 문제에서 알파 =0.01일 때 최솟값으로 세타 = 0이 안나오고 10만 나오는데… 혹시 어떻게 다르게 할지 여쭤볼 수 있을까요?
3/10/24 21:59,Ernest Lee,잘못 세팅을 하신 거 같은데요 …
3/10/24 22:00,시무룩한 튜브,사진
3/10/24 22:00,시무룩한 튜브,문제 5번 conv1D.py 그래도 복사해서 시행했는데
3/10/24 22:00,Ernest Lee,그거 코드 짜고 실행하셔야 해요
3/10/24 22:00,시무룩한 튜브,저런 오류가 나는데 혹시 어떤 게 잘못된 걸까요?
3/10/24 22:00,인사하는 제이지,@를 구현하셔야합니다!
3/10/24 22:00,시무룩한 튜브,아아
3/10/24 22:00,Ernest Lee,아니면 주석처리 되어 있는 거 살리시면 될 거에요
3/10/24 22:01,Ernest Lee,Scipy 라이브러리요
3/10/24 22:01,시무룩한 튜브,이렇게 하니까 되네요!
3/10/24 22:02,기지개 고양이,"원본 파일 자체가 """""" """"""가 인덴테이션이 잘못돼있더라고요"
3/10/24 22:02,Ernest Lee,네 맞아요
3/10/24 22:02,시무룩한 튜브,그건 그냥 tab 해서 처리해주면 되더라구요
3/10/24 22:03,경례하는 프로도,경례하는 프로도님이 들어왔습니다.
3/10/24 22:12,애교뿜뿜 무지,"X_i는 row의 transpose인 것 같습니다. 아마 여기서는 M_(:,i)와 같은 표현이 column을 의미하는 것 같습니다!!"
3/10/24 22:16,부탁하는 네오,"아 넵 그건 이해했는데 그래디언트를 row로 쓸거냐 column으로 쓸거냐의 차이 때문에 헷갈린 것 같습니다
윗분이 말씀해주신 분모중심표현으로 그래디언트를 column으로 쓰니까 a가 풀리네요
X_i^T\theta-Y_i는 스칼라고 X_i는 p*1 column vector라고 이해했습니다
그렇게 하니까 b도 풀립니다
제가 이해한 것이 맞을까요?
뭔가 벡터*스칼라 노테이션이 좀 찝찝하긴 한데.."
3/10/24 22:18,애교뿜뿜 무지,넵 맞는 것 같습니다!!!
3/10/24 22:22,신난 어피치,신난 어피치님이 나갔습니다.
3/10/24 22:24,멋쩍은 튜브,멋쩍은 튜브님이 나갔습니다.
3/10/24 22:27,시무룩한 튜브,"from scipy.linalg import circulant
A = circulant(np.concatenate((np.flip(k),np.zeros(n-r))))[r-1:,:]

혹시 원래는 # 되어있던 이 코드의 역할이 뭔지 알 수 있을까요?
5번 문제 conv1D.py 에 있는 코드입니다."
3/10/24 22:29,General Trash,5번 문제에 나오는 행렬 A를 만들어주는 코드입니다.
3/10/24 22:29,건방진 제이지,code는 GPT도 설명 잘 해주니 사용해보시는거 권해드립니다
3/10/24 22:30,벌 서는 라이언,주석 처리를 해제한 뒤에 나온 결과와 주석처리된 상태에서 return 값을 바꾸어 나온 결과를 비교해서 맞게 만들었는지 확인할 수 있을 것 같습니다.
3/10/24 22:32,엄지척 튜브,"S/U로 성적평가 변경을 마이스누에서 했는데, 혹시 따로 승인을 기다려야 하나요??"
3/10/24 22:33,신난 어피치,"https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.circulant.html
위와 같은 circulant를 사용하여 문제에 주어진 행렬을 만드는 코드입니다."
3/10/24 22:34,시무룩한 튜브,감사합니다
3/10/24 22:38,밥줘,혹시 과제 제출한거 어떻게 지우나요? 조작 실수로 같은걸 여러번 제출했는데..
3/10/24 22:38,애교뿜뿜 무지,다시 제출하면 될겁니다
3/10/24 22:39,밥줘,다시 제출하다가 세개가 쌓여버렸는데 가장 최근꺼만 봐지나요?
3/10/24 22:39,애교뿜뿜 무지,아 그런거면 걱정하지 않으셔도 될겁니다!
3/10/24 22:40,TA chaeju,"네, 걱정하지 않으셔도 됩니다."
3/10/24 22:41,신난 어피치,신난 어피치님이 나갔습니다.
3/10/24 22:41,밥줘,두 분 모두 감사합니다.
3/10/24 23:09,벌 서는 라이언,"HW1 3번 문제에서 theta_0이 m차원이라고 나오는데, 혹시 p차원이라고 이해해도 되나요?"
3/10/24 23:11,TA chaeju,"아 네, R^p의 원소로 이해하시면 됩니다."
3/10/24 23:13,벌 서는 라이언,감사합니다.
3/10/24 23:36,시무룩한 튜브,"문제 5번에서
theta k+1 - theta star = (theta k - theta star)(1-alpha * X^TX) 처럼, some function of (theta k - theta star)로 나타나는 것까지는 했는데.. 이게 어떤 알파보다 커야 diverge 하는지 증명하는 것에서 어려움이 있습니다.. 조언해주실 수 있는 게 있을까요?"
3/10/24 23:37,건방진 제이지,spectral theorem을 검색해보세요!
3/10/24 23:38,시무룩한 튜브,네 찾아볼게요! 감사합니다
3/10/24 23:38,기지개 고양이,θk 점화식이 특정 행렬이 계속 곱해지는 형태로 나타나는데
3/10/24 23:39,기지개 고양이,행렬이 계속 곱해질때는 그 행렬을 대각화하는 것이 좋은 방법이죠
3/10/24 23:39,부탁하는 네오,"대충 해결은 했는데, 대각화 가능하다는 보장이 있을까요ㅕ?"
3/10/24 23:40,기지개 고양이,X^TX가 대칭행렬이니.. 흠흠
3/10/24 23:40,신난 어피치,Real symmetric이므로 orthogonally diagonalizable입니다
3/10/24 23:40,신난 어피치,(spectral thm)
3/10/24 23:40,부탁하는 네오,아 그렇군요 선대 들은지 한참 돼서 하하
3/10/24 23:40,부탁하는 네오,감사합니다
3/10/24 23:41,신난 어피치,추가로 positive definite이라서 모든 고윳값이 양수고 alpha가 잘 정의됩니다
3/10/24 23:45,아침햇살,코딩 문제 실행 결과가 .ipynb 안에 있다면 pdf 파일로 결과 그림을 따로 첨부하지 않아도 되나요?
3/10/24 23:47,애교뿜뿜 무지,혹시 5번에서 @를 구현하라는 것은 알겠는데 이때 A를 입력하는 코드도 추가로 입력해야 하는 것인가요?
3/10/24 23:48,신난 어피치,그 부분은 이미 conv1D에 구현되어 있는 걸로 알고 있습니다
3/10/24 23:48,TA Hyojun,.ipynb를 .pdf로 변환하여 같이 제출해주시기 바랍니다.
3/10/24 23:48,열심히 일하는 네오,문제 5번에서 A^T(huber_grad(Ax+b))와 같이 chain rule에 의한 Ax+b의 x에 대한 미분값을 A가 아닌 A^T라고 보는 건 단순히 dimension을 맞춰주기위한 거라고 봐야할까요? 
3/10/24 23:50,애교뿜뿜 무지,circulant 주석 처리된 부분에서 random한 instance들을 부여하는 코드가 있는데 이와 같은 부분이 없다면 뒤의 GD 부분 실행 결과가 나오지 않을 것 같아서 궁금합니다..
3/10/24 23:53,TA Hyojun,분모중심표현을 사용하므로 Ax+b의 x에 대한 미분이 A^T가 됩니다. Problem 1에서의 상황을 생각해보시기 바랍니다.
3/10/24 23:53,엄지척 프로도,py 파일 코드를 ipynb에서 처리했으면 py 파일 안내고 노트북만 pdf 변환해서 제출해도 괜찮을까요?
3/10/24 23:53,TA chaeju,"주석 처리된 부분에는 random instance를 부여하는 코드가 없습니다.

A=Convolution1d(k)가 A를 만드는 역할입니다."
3/10/24 23:53,신난 어피치,"np.random.seed(0)
k = np.random.randn(r)
b = np.random.randn(n-r+1)
A = Convolution1d(k)
#from scipy.linalg import circulant
#A = circulant(np.concatenate((np.flip(k),np.zeros(n-r))))[r-1:,:]

위 네 줄의 코드가 random한 filter을 만들어서 A에 대입하는 부분인 것 같아요"
3/10/24 23:54,TA Hyojun,아니요 .py 코드도 함께 제출해주시기 바랍니다. 채점 시 TA들이 직접 코드를 돌려봐야 할 경우도 생길 수 있기 때문에 소스 코드도 함께 제출해주세요.
3/10/24 23:54,TA chaeju,"어떠한 형식으로든

1. 코드 파일 원본 (py or ipynb)
2. 코드 파일을 시행한 결과 pdf

모두 첨부해주시기 바랍니다."
3/10/24 23:54,엄지척 프로도,감사합니다 TA님
3/10/24 23:55,하트뿅뿅 라이언,4번 문제에서는 정확히 어떤 결과를 pdf에 첨부해야 하나요? 
3/10/24 23:57,애교뿜뿜 무지,그냥 코드와 실행 결과를 볼 수 있는 pdf를 제출하면 문제 없을 것 같습니다 - 원본 같은 경우 실행 여건이 다르면 실행 시 다르게 나타날 수도 있으니까요
3/10/24 23:59,TA Hyojun,각 alpha마다 random starting points가 어디로 수렴하는지 잘 나타날 수 있도록 결과를 정리해서 첨부하시면 됩니다.
3/11/24 0:00,택배 상자를 든 네오,문제 5번에서 Hint처럼 np.asarray를 이용해서 한줄로 작성해야 답안이 인정되나요 아니면 행렬을 직접 만들지만 않으면 되는걸까요?
3/11/24 0:01,TA Hyojun,꼭 Hint처럼 구현하실 필요는 없습니다. 행렬 A를 직접 만들지 않고 @ 연산을 효율적으로 수행할 수 있도록 코드를 짜시면 됩니다.
3/11/24 0:07,애교뿜뿜 무지,감사합니다~
3/11/24 0:44,음료 마시는 어피치,죄송한데 혹시 코드가 효울적으로 수행한다는 것은 어떻게 알 수 있나요?
3/11/24 0:44,Ernest Lee,A를 implement하지 않고 component에 집중하시면 됩니다
3/11/24 0:45,Ernest Lee,redundant 0을 어떻게 하면 무시할까에 focusing한다고 생각하시면 될 거 같아요
3/11/24 0:45,심수기모띠,이건 invertible assumption 때문인거죠?
3/11/24 1:05,TA Hyojun,"주어진 행렬이 XTX 꼴이므로 positive semidefinite이며, invertible 조건이 더해져 positive definite가 됩니다."
3/11/24 1:08,시무룩한 튜브,"from scipy. linalg import circulant
A = circulant (np. concatenate((np.flip(k),np.zeros (n-r) ) )) [r-1:,:] 

이 코드가 답에 포함되어 있으면 안되는 건가요?"
3/11/24 1:08,Ernest Lee,저건 지우셔야 합니다
3/11/24 1:08,Ernest Lee,아니면 주석처리 하거나 ..
3/11/24 1:09,심수기모띠,네 감사합니다.
3/11/24 1:34,전커서교수님이될래요,"5번은 코드(.py)만 제출해도 상관없나요, 아니면 실행 결과도 출력을 캡쳐해서 PDF로 제출해야하나요?"
3/11/24 1:36,TA Hyojun,실행 결과도 pdf로 제출해 주시기 바랍니다
3/11/24 10:02,불 뿜는 튜브,불 뿜는 튜브님이 들어왔습니다.
3/11/24 11:38,이승환,이승환님이 들어왔습니다.
3/11/24 12:11,화난 라이언,"혹시 공지사항에 있는 pdf 변환 사이트에서
pyppeteer-install 을 code prompt에 입력하라고 되어있는데 
code prompt가 어딜 의미하는건가요..?"
3/11/24 12:18,TA chaeju,터미널입니다.
3/11/24 12:24,JX,"If I executed the code in ipynb file, the result is included in the coding file. Do I still need to capture the resuit and summit seperately in PDF?"
3/11/24 12:26,TA chaeju,"Yes, you can directly convert the ipynb file (and its result) into pdf file (and this is preferred). please check out the etl announcement."
3/11/24 12:42,택배 상자를 든 네오,택배 상자를 든 네오님이 들어왔습니다.
3/11/24 13:12,화나서 방방 뛰는 튜브,전치 컨볼루션하고 A.T@는 동일한 개념인가요?
3/11/24 13:15,부탁하는 네오,코드 좀 읽어보시면 아시겠지만 A.T를 정의할 때 전치를 처리해주는 과정이 있습니다
3/11/24 13:15,부탁하는 네오,@는 행렬곱 수행하는 거구요
3/11/24 13:17,화나서 방방 뛰는 튜브,네 감사합니다
3/11/24 13:54,츄리닝안경 네오,츄리닝안경 네오님이 들어왔습니다.
3/11/24 14:48,비옷입은 튜브,혹시 문제5 A크기가 (n+2(r-1))*n 아닌가요?
3/11/24 14:50,gg,gg님이 나갔습니다.
3/11/24 15:56,TA Hyojun,"답장이 늦었는데, (n-r+1)xn이 맞습니다."
3/11/24 16:01,Piepie,주피터에서 pdf로 변환할 때 에러가 발생하는데 뭐가 원인인지 아시는분 있을까요.
3/11/24 16:01,Piepie,사진
3/11/24 16:02,Piepie,사진
3/11/24 16:02,Piepie,터미널로 깔았는데 잘 안 되네요
3/11/24 16:09,멋쩍은 튜브,정 안 되면 prt screen 해서 제출하세요
3/11/24 16:09,멋쩍은 튜브,얼마 안남아서 걱정돼서요
3/11/24 16:17,휘파람 프로도,저도 그냥 html로 내보내기 한 후에 print -> pdf 저장해서 제출했습니다. 여러 방법으로 시도했는데 계속 에러가 나더라고요..
3/11/24 16:21,아침햇살,크로뮴까지 안 하셔도 되고 nbconvert랑 윈도우면 miktek만 깔면 변환됩니다
3/11/24 16:21,시무룩한 튜브,"챕터 1 ppt 42쪽 마지막 문장 : However, if the expectation is difficult to compute, GD is impractical and SGD is preferred. 라는 문장에서 질문이 있습니다.

expectation 이 계산하기 어려운 상황이라면, 왜 확률을 이용하는 SGD 가 더 선호되는지 이해가 잘 가지 않습니다."
3/11/24 16:23,멋쩍은 튜브,"Difficult to compute -> 계산량이 많다
그래서 계산이 빠른 sgd를 쓴다 아닐까요...? 제생각입니다"
3/11/24 16:27,DL하는 블루,"예를 들어 N=10^10이라고 하면...
한 스텝 내려가는 데 10^10번의 계산이 필요하겠죠.

그래서 차라리 한 성분씩만 가더라도 일단은 내려가는 게 더 빠르다는 게 SGD의 철학이고,

나아가 이왕 일부만 내려가는 김에 되는 만큼 많은 성분 내려가자는 게 mini batch SGD의 철학 같습니다."
3/11/24 16:34,음악듣는 어피치,음악듣는 어피치님이 들어왔습니다.
3/11/24 17:16,먹보 네오,조교님 혹시 메일 확인 부탁드려도 될까요?
3/11/24 17:22,TA chaeju,삭제된 메시지입니다.
3/11/24 17:22,TA Youngmin,"Solutions for hw1 are uploaded on ETL. 
Please feel free to reach TA for any questions regarding solutions."
3/11/24 17:27,마이크를 든 라이언,"조교님 혹시 과제 파일은 첨부가 되었는데, 코드 파일이 첨부 되지 않았는데 메일로 제출해도 괜찮을까요?"
3/11/24 17:43,먹보 네오,저도 같은 질문 메일로 드렸습니다 ㅠㅠ 자세히 안내해주셨는데 이런 세부사항을 놓쳐서 죄송합니다..
3/11/24 18:06,초롱초롱 어피치,초롱초롱 어피치님이 들어왔습니다.
3/11/24 18:25,열심히 일하는 네오,혹시 HW1 Problem 3답안에서 top eigenvector의 정의가 어떻게 되나요? 
3/11/24 18:26,눈빛 애교 어피치,눈빛 애교 어피치님이 나갔습니다.
3/11/24 18:29,초롱초롱 어피치,가장 큰 eigenvalue에 대응되는 eigenvector입니다
3/11/24 18:29,인사하는 제이지,과제 답안 어디에 올라와있나요?
3/11/24 18:29,열심히 일하는 네오,Etl이요
3/11/24 18:31,Tychonoff,Tychonoff님이 들어왔습니다.
3/11/24 18:36,열심히 일하는 네오,"감사합니다. 근데 답안에서 
The top eigenvector component of (theta0 - theta*) is nonzero라고 되어있는데 왜 이렇게 되는 거죠? 
혹시 theta* 안의 X^TX를 염두에 두고 한 말인가요?"
3/11/24 18:41,하트뿅뿅 라이언,lower dimensional이 아니라서 그런거 아닌가요
3/11/24 18:42,TA chaeju,삭제된 메시지입니다.
3/11/24 18:42,TA chaeju,"\Lambda는 diagonal에 X^T X의 eigenvalue들을 나열한 matrix인데요, X^T X의 largest eigenvalue가 \Lambda의 m번째 대각선 칸에 있다고 합시다. (즉, m번째 row&column)

만약 (theta0 - theta*)의 m번째 component가 0이 아니라면, (I - \alpha {\Lambda}^k)(theta0-theta*)의 m번째 component가 (nonzero) * (1-\alpha{\rho(X^t X)}^k)가 되어, 발산합니다."
3/11/24 18:44,N이 되고픈 엡실론,삭제된 메시지입니다.
3/11/24 18:49,열심히 일하는 네오,그런 의미에서 하신 말씀이셨군요. 상세한 답변 감사합니다!
3/11/24 18:50,.,"해설에서
The top Eigenvector equal to an (n-1) dimensional set
의 표현이 잘 이해되지 않습니다.

Top eigenvector 가
R^(n-1) 안에 있다고 보면 될까요?"
3/11/24 18:51,TA chaeju,"typo네요

the vectors such that top eigenvector component equal to 0 is an (n-1)-dimensional set

으로 읽어주시면 될 것 같습니다."
3/11/24 18:58,.,"아 그렇군요
추가로
 (n-1) dimensional set 이면 어떤점이 다른지 잘 모르겠습니다"
3/11/24 19:01,TA chaeju,"문제에서 보이라고 한 statement가, ""for most"" starting point theta0에 대해 diverge함을 보이는 것이었습니다.

measure theory에서, lower dimensional set은 measure 0 set입니다. (평면의 부피를 0, 직선의 넓이를 0이라 부르는 것과 비슷하다고 이해하셔도 좋습니다).

따라서, (n-1) dimensional set (top eigenvector component가 0인 벡터)을 제외한 모든 곳에서 발산함을 증명하였으므로, ""for most"" starting point에서 발산함을 증명한 것이 됩니다."
3/11/24 19:04,심수기모띠,"제가 이해를 정확히 했는지 몰라서 여쭙고 싶습니다.

비유하면 R^2에서 x축과 y축의 합집합이 measure zero라는 논리인가요?"
3/11/24 19:06,.,넵 감사합니다
3/11/24 19:06,TA chaeju,"그렇습니다. 

사실 P의 행/렬을 적절히 조작하여 top eigenvector component가 1이라 가정할 수 있으므로, 드신 예시에서 ""x축""이 ""R^2""에서 measure zero set이므로~ 하는 논리여도 충분합니다."
3/11/24 19:07,심수기모띠,아 이해했습니다. 친절한 설명 감사드립니다.
3/11/24 19:32,화나서 방방 뛰는 튜브,다음 과제는 언제 나오나요?
3/11/24 19:34,TA Hyojun,Homework 2 is uploaded on class website(not eTL).
3/11/24 19:35,애교뿜뿜 무지,삭제된 메시지입니다.
3/11/24 19:36,애교뿜뿜 무지,(There is a typo on the due date in HW 2)
3/11/24 19:47,TA Hyojun,Thank you for letting me know. There is a typo on the due data in HW2 pdf file. HW2 is due March 18(5pm)!
3/11/24 19:50,TA Youngmin,"There were some inquires regarding extra submission of hw1 due to missing files. 

We won't take any extra submissions on the homework because answers for the homework were posted a few minutes after the due time. Also, it was notified on ETL(and several times on this open chat room) that both source code and result should be submitted.

However, because it was our first homework on this course, if source code is included in submitted pdf file we will assume you have submitted the source code file with the same code.

Please make sure you have submitted all required files afterwards.

Thank you."
3/11/24 19:53,화나서 방방 뛰는 튜브,잠시 깜빡했네요 감사합니다
3/11/24 19:55,먹보 네오,감사합니다! 다음 제출 때는 주의하겠습니다ㅠㅠ
3/11/24 20:31,열심히 일하는 네오,Hw2 Problem1에서 나오는 log는 밑이 e라고 보면 되죠?
3/11/24 20:57,기지개 고양이,사진
3/11/24 20:58,기지개 고양이,"과제2번 problem 2의 accuracy 그래프가 problem1과는 다르게 상당히 이상하게 나오는데 정상인가요?
계속해서 잘못한 부분을 찾는데 안보이네요...
loss func.의 마지막 lambda항 때문에 저렇게 되는 것인가 하여 질문드립니다"
3/11/24 20:58,기지개 고양이,(만약 해당 사진이 문제가 된다면 지우겠습니다)
3/11/24 20:59,신난 어피치,alpha 값을 줄여보는 건 어떨까요??
3/11/24 21:01,기지개 고양이,alpha 값을 1e-4 까지 줄였는데
3/11/24 21:01,.,.님이 나갔습니다.
3/11/24 21:01,기지개 고양이,마치 푸리에변환 한거 같이 몇몇 점에서의 스파이크만 있고 나머지는 저조하네요
3/11/24 21:02,기지개 고양이,사진
3/11/24 21:03,기지개 고양이,삭제된 메시지입니다.
3/11/24 21:06,기지개 고양이,소스코드를 올릴수도 없고... 제가 질문하는 예의가 없군요 ㅠㅠ
3/11/24 21:06,기지개 고양이,조금더 스스로 고민해보겠습니다
3/11/24 21:08,송민창,해당 답변이 정확하게 이해되지 않아 다시 질문드립니다. (I-alphaX^TX)^K = P*(I-alpha*Lambda)^k*P^T 로 바꿔주면 (theta^0-theta^star)앞에 P^T가 와서 P^T*(theta^0-theta^star)의 m번째 component가 nonzero가 되게끔 설정해줘야하는 것 아닌지 의문이 생깁니다.
3/11/24 21:20,TA chaeju,삭제된 메시지입니다.
3/11/24 21:24,TA chaeju,"아 네 맞습니다. 제가 실수했네요

solution에서처럼 X^t X = P^t \Lambda P로 대각화하면
P의 각 row는 X^t X의 eigenvector의 transpose가 됩니다.
(편의상 eigenvalue들이 내림차순으로 Lambda의 diagonal에 자리했다 가정하겠습니다.)

이 때 P(theta0 - theta*)의 i-th component는 
(X^tX의 i-th eigenvalue의 corresponding eigenvector)와 (theta0-theta*)의 내적값이 되므로,
다르게 해석하면 (theta0 - theta*)벡터의 i-th eigenvector 방향 성분이라고 말할 수 있습니다. (이게 top eigenvector component라고 쓴 이유입니다)"
3/11/24 21:25,송민창,감사합니다☺️
3/11/24 21:34,TA chaeju,"여기서도 오타가 좀 있는데…
(I- \ alpha{\Lambda}^k)가 아니라 (I - \alpha\Lambda)^k입니다.
(solution에는 문제없습니다)"
3/11/24 21:36,눈물바다에 빠진 라이언,hw2에서 problem1~3처럼 스켈레톤 코드가 주어지지 않았지만 파이썬을 활용하는 경우에도 .py 또는 .ipynb 파일을 제출하면 될까요?
3/11/24 21:40,TA chaeju,"네, 같이 제출해주시면 됩니다."
3/11/24 21:40,눈물바다에 빠진 라이언,넵 감사합니다!
3/11/24 22:36,눈물바다에 빠진 라이언,사진
3/11/24 22:36,눈물바다에 빠진 라이언,"Chapter 1 코드 관련하여 질문드립니다!
그래프에서 (||Xθ-Y||^2)/(2N)의 값을 비교하지 않고 ||Xθ-Y||^2의 값을 비교한 것은 2N을 나누지 않아도 결과는 똑같고, 굳이 매 iteration마다 2N을 나누어서 속도를 저하시킬 필요가 없어서인가요?"
3/11/24 22:52,TA Hyojun,네 맞습니다. 값을 비교하기만 하면 되므로 2N으로 나누지 않은 것으로 보입니다.
3/11/24 23:06,눈물바다에 빠진 라이언,넵 감사합니다!!
3/11/24 23:15,건방진 제이지,"hw1_sol의 conv1D_sol.py, TransposedConvolution1d class의 matmul 함수에서 np.flip을 매 iteration(j for문) 마다 반복할 경우 iteration에 비례해서 시간이 늘어나게 되어서 충분히 효율적이지 않다고 생각이 드는데 이렇게 구현한 이유가 따로 있을까요?"
3/12/24 9:13,인사하는 프로도,can I use torch in HW2 except for prob 7?
3/12/24 9:59,Luca [루카],Has the chapter 1 notebook on the site been updated to include cyclic minibatch SGD (just via vector operations not using pytorch as in the chapter 2 notebook) yet?
3/12/24 12:42,TA Hyojun,코드를 간결하게 작성하기 위한 것 같습니다. 해당 문제는 sparse한 A matrix를 메모리에 직접 저장하는 것과 불필요한 0*a 연산을 수행하는 것을 피하는 것이 목표이므로 말씀해주신 부분은 크게 고려하지 않아도 괜찮을 것 같습니다.
3/12/24 12:44,TA Hyojun,"I recommend not using pytorch in HW2(Implementing SGD from scratch would be a good practice), but it is not necessary."
3/12/24 14:19,생각하는 라이언,생각하는 라이언님이 나갔습니다.
3/12/24 16:35,으쓱으쓱 어피치,"3.11강의의 코드에서, optimizer의 learning rate scaling을 이유를 조금 더 자세히 알 수 있을까요?"
3/12/24 16:56,인사하는 프로도,"in twolayerSGD, there is a line that saves the result in png. Do I have to submit this along with the pdf?"
3/12/24 17:01,음료 마시는 어피치,삭제된 메시지입니다.
3/12/24 17:06,시무룩한 튜브,Redo 제출은 채점결과가 나온 후 정해진 기한까지 제출하는 건가요?
3/12/24 17:15,건배하는 프로도,채점결과는 언제쯤 나오나요?
3/12/24 17:22,TA Youngmin,Yes. Redo will take re-do submissions after grading of homework is done.
3/12/24 17:22,TA Youngmin,Grading of homework will usually take 2-3 days.
3/12/24 17:44,TA Hyojun,"If the execution result(plot) is already included in the pdf file, you don't need to submit png image."
3/12/24 17:44,TA Hyojun,"lr에 255를 곱한 부분 말씀이신가요? 해당 코드는 DataLoader를 사용해 train/test에 사용되는 데이터를 취합하는데, 이때 0~255 범위의 이미지 픽셀 값이 0~1로 normalize됩니다. 이 점을 고려하여 learning rate에 255를 곱해주었다고 생각하시면 될 것 같습니다."
3/12/24 20:57,.,"For the SGDs in HW2, is there a preferred type of SGD we should use?"
3/12/24 21:39,청소하는 튜브,Vanilla SGD will suffice
3/12/24 22:30,손을 번쩍 든 무지,"혹시 이번 과제의 1,2번 문제도 plotting을 해야 하나요?"
3/12/24 22:32,손을 번쩍 든 무지,"그리고 문제 2번에서 ""Empirically, does the SGD ever encounter a point of non-differentiability?"" 라고 한 부분에 대하여 어떤 식으로 답을 해야 하는지도 더 자세히 알 수 있으면 감사하겠습니다!"
3/12/24 22:35,하트뽀뽀 어피치,"수업과 상관 없는 질문일 수도 있는데,
어떤 데이터 (xi, yi)들에 f_{a,b,c,...}(x)로 피팅할 때 아래 사진과 같은 chi-square을 최소가 되는 것을 목표함수로 피팅하더라고요.
여기서 yi의 표준편차 sigma_i가 반영된 것인데,
혹시 xi의 표준편차도 식에 반영된다면, chi square, 혹은 목표함수의 식이 어떻게 바뀌어야 통계적으로 맞는지 궁금합니다.
관련하여 답을 아시거나, 관련 키워드를 아시거나 하는 분들 모두 답변해주시면 감사할 것 같습니다."
3/12/24 22:35,하트뽀뽀 어피치,사진
3/12/24 22:36,수학하는 라이언,수학하는 라이언님이 나갔습니다.
3/12/24 22:56,TA Hyojun,"네 1, 2번 문제도 loss function을 plot해주시면 됩니다. 주어진 random data points로 SGD를 수행할때 non-differentiable point에 도달하는 경우가 있는지 확인해보시면 될 것 같습니다."
3/12/24 22:57,손을 번쩍 든 무지,감사합니다!
3/12/24 23:36,TA Hyojun,사진
3/12/24 23:36,TA Hyojun,사진
3/12/24 23:36,TA Hyojun,삭제된 메시지입니다.
3/12/24 23:38,TA Hyojun,x와 y가 모두 uncertainty를 갖는 경우 다음과 같은 effective variance를 sigma_i 대신 사용하여 fitting을 수행할 수 있습니다. x의 uncertainty가 y보다 상당히 작으면 effective variance를 이용한 fitting이 valid하다고 하는데 저도 자세히는 잘 모르겠네요...
3/12/24 23:41,하트뽀뽀 어피치,"답변 감사드립니다. 혹시 말씀하신 ""x의 uncertainty가 y보다 상당히 작으면 effective variance를 이용한 fitting이 valid하다"" 내용의 기저 논리도 궁금한데, 혹시 이 내용이 담긴 ref를 알려주실 수 있으신가요?"
3/12/24 23:48,TA Hyojun,"https://pubs.aip.org/aapt/ajp/article-abstract/42/3/224/1049655/Simple-Method-for-Fitting-Data-when-Both-Variables?redirectedFrom=fulltext
3페이지에 해당 내용이 언급되어 있습니다."
3/12/24 23:55,하트뽀뽀 어피치,"조교님, 감사합니다! 정말 큰 도움 되었습니다! 좋은 밤 되세요!"
3/13/24 0:55,신나는 프로도,신나는 프로도님이 들어왔습니다.
3/13/24 10:20,빈털터리 제이지,"과제1 3번 문항 해설에서, (theta.k - theta.*) = P^T*{I - \alpha*\Lambda}^k*P 로 식을 전개하는 것까진 잘 이해가 되는데, 설명주신 부분에서 행렬 P를 포함한 P^T*{I - \alpha*\Lambda}^k*P(theta.0-theta.*) 식이 아닌 (I -{ \alpha*\Lambda}^k)(theta.0-theta.*)만의 m번째 component를 확인하는 것이 헷갈립니다. 단순하게 '행렬 P^T, P는 k와 무관해서 고려하지 않는 것이다' 라고 생각하는게 맞을까요?"
3/13/24 10:23,TA chaeju,"그 설명에 실수가 있었습니다.
수정된 답변을 참고해주세요"
3/13/24 10:34,빈털터리 제이지,이해되었습니다 감사합니다!
3/13/24 10:44,눈물바다에 빠진 라이언,눈물바다에 빠진 라이언님이 나갔습니다.
3/13/24 11:21,기타치는 튜브,"training 과정에서 epoch가 정확히 무슨 뜻인가요? chapter 2 코드를 공부하다가 앞서 SGD를 직접 scratch에서 구현할 때는 iteration 횟수를 지정했는데, torch 활용 코드에서는 이에 관한 부분은 없어 sgd에서 몇 번이나 iteration을 돌리는지 어떻게 결정되는지도 궁금합니다"
3/13/24 11:22,초롱초롱 어피치,데이터셋의 모든 점들을 평균적으로 1번씩 거쳐가면 1 epoch이라고 합니다
3/13/24 11:28,기타치는 튜브,아 그러면 iteration = epoch * batch size라고 이해하면 될까요?
3/13/24 11:29,N이 되고픈 엡실론,Epoch = iter * batch size / total data size로 생각하시면 될듯 합니다
3/13/24 11:29,초롱초롱 어피치,iteration = epoch * (dataset size / batch size) 이죠
3/13/24 11:30,기타치는 튜브,아 batch 개수네요
3/13/24 11:30,기타치는 튜브,감사합니다 !!
3/13/24 11:32,초롱초롱 어피치,epoch의 수는 경험적으로 결정된다고 보면 될거같습니다
3/13/24 11:52,쑥스럽게 인사하는 프로도,"저도 3/11 강의에서 DataLoader를 도입했을 때 lr의 변화와 관련하여 질문이 있습니다.
raw data와 비교해서 DataLoader에서 얻을 수 있는 image는 pixel value scale이 1/255로 줄어드는 셈인데, 그러면 AX+b output을 동일하게 유지하기 위해서는 파라미터 A값이 255배가 되어야 하고, 이에 따라 local minima가 A-loss 그래프 상에서 높이는 유지되고 너비는 넓어지면서 더 완만해지는 것이라고 이해했습니다. training 과정에서 이러한 변화를 상쇄하기 위해 lr에 255를 곱해주는 것이 맞을까요?"
3/13/24 14:23,Tube als Cheerleader,Which variant of SGD are we supposed to use in problem 1? The formula for the loss iterates over all N entries before making a step. I.e. sgd mini batch method with batch size N. Is that what it's supposed to be?
3/13/24 15:11,비옷입은 튜브,혹시 CH1 코드에서 sig를 왜 sig = np.random.rand(p)+0.2 로 정의했는지 궁금합니다 
3/13/24 15:11,열심히 일하는 네오,"If you want to make it from the scratch, I guess uniform random sampling over 1 to N is needed with taking term in the sum to be f_i"
3/13/24 15:12,열심히 일하는 네오,using vanilla SGD
3/13/24 15:12,손을 번쩍 든 무지,"교수님이 이 코드는 신경쓰지 말라고 하셨던 거로 기억하는데, 그냥 임의로 sig를 정하려고 그렇게 정의하신 것 같습니다"
3/13/24 15:15,Tube als Cheerleader,Vanilla SGD as introduced in the lecture doesn't have a sum over all elements though
3/13/24 15:17,청소하는 튜브,"Yes, so you should use the inner function as the definition of SGD: ""STOCHASTIC"" gradient descent uses the function f_i that satisfies E[f_i] = L where L is our real loss formula"
3/13/24 15:19,청소하는 튜브,"Here, i is a random variable such that you extract the index randomly in a code."
3/13/24 15:20,Tube als Cheerleader,Ohhh right. Thank you!!
3/13/24 15:32,비옷입은 튜브,감사합니다!
3/13/24 16:16,피자 먹다 자는 무지,피자 먹다 자는 무지님이 나갔습니다.
3/13/24 16:47,눈물 흘리는 제이지,re-do 의 경우 기존의 hw1 submission에 다시 제출하였는데 이렇게 하면 되는지 아니면 해당 문제만 새로 만들어진 세부문제 항목에 제출하면 되는지 궁금합니다
3/13/24 16:47,.,공지 보시면 후자라고 합니다
3/13/24 16:59,부끄러워하는 라이언,"안녕하세요, HW1 5번문항 코드만 제출하면 되나요, 아니면 결과값을 같이 적어서 제출해야 하나여?"
3/13/24 17:01,비옷입은 튜브,"혹시 이번 중간, 기말 오픈북인가요?"
3/13/24 17:03,TA chaeju,"We made a homework 1-redo submission tab.

As we noticed, you have a chance for re-submission for the problems that you got 1 points.

For the problems that got 0 point, you don't have a chance to redo.

The due for re-submission is 3/20 5:00 pm"
3/13/24 17:07,초롱초롱 튜브,혹시 re-do는 같은 문제에서 답안을 수정해서 제출하면 되는 걸까요?
3/13/24 17:08,TA chaeju,삭제된 메시지입니다.
3/13/24 17:13,TA chaeju,"We will take re-submission for each problem through assignments HW1P1 ~ HW1P5.

Due for re-submission is 3/20 5:00pm.


For those who have recieved 2 points on each problems, you may simply ignore HW1P1 ~ HW1P5.

sorry for the confusion."
3/13/24 17:24,TA Jaewook,전자기기를 사용하지 못 하는 선에서 오픈북일 예정입니다
3/13/24 17:38,신난 어피치,수업 교재가 별도로 없는 걸로 아는데 혹시 오픈북이 페이퍼 만들어가도 된다는 건가요?
3/13/24 17:42,TA Gyeongmin,"네, 전자기기를 제외한 수업 자료, 본인이 공부한 노트, 출력물 등의 반입이 가능합니다."
3/13/24 17:42,건방진 제이지,학점(letter grade) 기준은 어떻게 되나요
3/13/24 17:46,TA Jisun,학점 및 성적 기준과 관련해서는 교수님께 직접 상담하시기 바랍니다.
3/13/24 18:05,부끄러워하는 라이언,"안녕하세요, HW1 5번문항 코드만 제출하면 되나요, 아니면 결과값을 같이 적어서 제출해야 하나여? 그리고 혹시 결과값 없이 코드만 제출했을 시 1점이 부여되는건가요?"
3/13/24 18:12,TA Youngmin,"Please submit both source code and results. No points were deducted due to  missing results in submission.

However, points were deducted if error ocurred when program was ran manually to check results."
3/13/24 18:12,부끄러워하는 라이언,감사합니다!
3/13/24 18:19,택배 상자를 든 네오,안녕하세요! 개인 컴퓨터나 노트북에 외장그래픽카드가 없다면 cuda환경을 사용할 방법이 없는건가요? 아니면 내장 그래픽카드로도 사용가능한 방법이 있나요?
3/13/24 18:20,초롱초롱 어피치,google colab을 이용하시면 됩니다
3/13/24 19:43,열심히 일하는 네오,삭제된 메시지입니다.
3/13/24 19:49,TA Hyojun,"해당 상황에서 주어진 loss의 gradient를 계산해 보시면 X에 대해 linear한 부분이 있음을 확인하실 수 있습니다. normalize하기 전에 lr=1e-4에서 SGD가 잘 수행되었으므로 X가 1/255배가 된 지금은 lr에 255를 곱한 것으로 이해하시면 될 것 같습니다. Decision boundary의 관점에서 생각해 보시면 input값이 모두 1/255배가 되더라도 decision boundary의 normal vector는 달라지지 않을 것이므로 A가 255배가 된다고 보기에는 어려울 것 같습니다. 다만, learning rate는 많은 경우 empirical하게 결정하므로 해당 부분을 너무 엄밀하게 이해하고자 하실 필요는 없을 것 같습니다."
3/13/24 22:35,멋쩍은 튜브,이게 약간 좀 바보같은 질문일 수도 있는데 ㅠㅠ 과제 1의 4번이랑 과제 2의 1번이랑 어떤 차이인지 모르겠어요... 과제 1의 4번은 GD고 2의 1번은 SGD이긴 한데 dataset이 주어지지 않은 상황에서는 같은 거 아닌가요...
3/13/24 22:56,TA Juhyeok,"I believe that the difference between two problems is essentially at the difference between GD and SGD. When using SGD, as described in the slide of Chapter 1, we assume that the data points are given, and we estimate the expectation of the function value using the given finite datasets. So without dataset, I think that the problem would be considered similar, but I also think that using SGD with dataset instead of using GD will make the implementation slightly different."
3/13/24 23:29,멋쩍은 튜브,"아 지금은 X_i, y_i들이 dataset이군요 복습하고 깨달았습니다... 감사합니다 조교님"
3/14/24 11:13,손을 번쩍 든 무지,사진
3/14/24 11:13,손을 번쩍 든 무지,"SGD 코드에 관한 사소한 질문인데요, 위 쪽 노란 부분에서 minimize하고자 하는 값은 ( || Xθ - Y ||^2 ) / 2N 인데 아래쪽 노란 부분에 append하는 값은 || Xθ - Y ||^2 인 특별한 이유가 있는지 알고 싶습니다. 두 개를 minimize하는 게 equivalent하기 때문인 것으로 이해하면 될까요?"
3/14/24 11:17,TA chaeju,네 맞습니다.
3/14/24 11:19,손을 번쩍 든 무지,삭제된 메시지입니다.
3/14/24 11:20,손을 번쩍 든 무지,"감사합니다, 조교님! 그렇다면 ( || Xθ - Y ||^2 ) / 2N 의 minimization을 하고 싶다면 그냥 f_val에 append하는 값을 (linalg.norm(X@theta - Y)**2) / (2*N)으로 하면 되는 것이지요?"
3/14/24 11:28,라이언,라이언님이 나갔습니다.
3/14/24 11:28,TA chaeju,"네, 원래 함수인 ||X@theta - Y||^2 / 2N의 값을 추적하고 싶다면, 말씀하신 대로 코드를 바꾸면 됩니다."
3/14/24 11:28,손을 번쩍 든 무지,감사합니다!
3/14/24 11:58,멋쩍은 튜브,사진
3/14/24 11:58,멋쩍은 튜브,3번 문제에 error/iteration plot도 해야 하나요 아니면 그냥 scatter plot만 하는 것으로 분류가 된다는 것을 보이기 충분할까요?
3/14/24 12:39,TA Hyojun,data point랑 decision boundary만 보여주셔도 충분합니다.
3/14/24 12:46,멋쩍은 튜브,감사합니다!
3/14/24 17:20,건방진 제이지,4번 문제에서 -log x 가 convex 임을 보이기 위해 이계도함수가 양수임을 이용해도 될까요? 아니면 주어진 정의를 이용해야만 하나요?
3/14/24 17:26,기타치는 튜브,저는 정의 이용하는 걸로 이해했습니다
3/14/24 17:31,청소하는 튜브,정의는 적혀있는게 맞는데 convex 성질 때문에 두 번 미분해도 될 거 같습니다
3/14/24 17:42,DL하는 블루,이계도함수가 양수임과 주어진 정의가 동치라는 걸 보여야 하지 않을까요?
3/14/24 18:14,소심한 네오,HW2 에서 LR 이나 SVM 을 이용할 때 직접 구현해서 이용해야 하나요? 아니면 강의 주피터 코드처럼 pytorch 내장 함수를 사용해도 되는건가요?
3/14/24 18:16,일하기 싫은 네오,.
3/14/24 18:17,소심한 네오,이전에 같은 질문이 있었군요 감사합니다!
3/14/24 19:44,치맥하는 제이지,제가 내용을 잘못 이해한건지 잘 모르겠는데 2번 문제에서 하라는 대로 sgd를 사용하게 되면 앞에 1/N이 붙지 않아야 하는것 아닌가요?
3/14/24 19:44,치맥하는 제이지,사진
3/14/24 20:15,청소하는 튜브,이거 참고하시면 됩니다
3/14/24 20:19,치맥하는 제이지,감사합니다!
3/14/24 23:57,권투하는 무지,hw2 prob3에서 decision model은 aTx+b가 아니라 그냥 aTx를 써야 하는 건가요...? 전자로 하는게 더 일반적인 것 같아서 그렇게 했는데 결과는 잘 안나오고 힌트 보니 후자로 해야 하는 것 같아서 질문드립니다
3/14/24 23:59,신난 어피치,후자로 하면 잘 나오는 것 같습니다!
3/15/24 0:00,신난 어피치,저도 전자는 결과가 안 나왔습니다..
3/15/24 0:01,옐로카드 프로도,혹시 일반적인 LR 쓰신걸까요?? 이것저것 해보고 있는데 잘 안나와서 질문드려요!
3/15/24 0:01,A+ is All you need,저는 전자로 해서 결과 냈는데 aTx 해도 되나요?
3/15/24 0:02,멋쩍은 튜브,저는 ax해도 잘 나온 거 같아요
3/15/24 0:03,멋쩍은 튜브,*해서
3/15/24 0:03,멋쩍은 튜브,Loss function은 1번 거로 했어요
3/15/24 0:16,옐로카드 프로도,그렇군요... 감사합니다!! 제가 뭔가 잘못했나보네요
3/15/24 1:47,초롱초롱 어피치,"hw2 prob1, 2에서 loss가 줄어들지 않고 계속 osciallate하는데 제가 뭔가 잘못하고 있는 거겠죠?"
3/15/24 1:48,이민준,전 그때 lr을 줄이고 epoch를 키우니 해결되었습니다
3/15/24 1:48,멋쩍은 튜브,사진
3/15/24 1:48,멋쩍은 튜브,저도 2번은 이런 식으로
3/15/24 1:48,멋쩍은 튜브,좀 이상하네요ㅠㅠ
3/15/24 1:48,빈털터리 제이지,Scaler가 있기 때문에 loss가 0까지 안가는게 맞습니다
3/15/24 1:50,빈털터리 제이지,*regularizer 
3/15/24 1:50,멋쩍은 튜브,저 같은 경우에는 이런 식으로 진동하는데 이건 뮨제가 있는거겠죠?
3/15/24 1:51,빈털터리 제이지,Sgd니까 진동하는것도 맞죠
3/15/24 1:51,멋쩍은 튜브,근데
3/15/24 1:51,멋쩍은 튜브,진동하면서 감소해야 되는데
3/15/24 1:51,멋쩍은 튜브,너무 횡보하는 느낌이네뇨
3/15/24 1:51,멋쩍은 튜브,Lr 조절해 봐도 횡보는 똑같더라고요
3/15/24 1:51,치즈케이크,epoch 더 돌려보시죠
3/15/24 1:52,멋쩍은 튜브,사진
3/15/24 1:52,멋쩍은 튜브,만번 해도 이러면
3/15/24 1:52,멋쩍은 튜브,뭔가 오류가 있는 걸로 확인하는게
3/15/24 1:52,멋쩍은 튜브,*생각하는게
3/15/24 1:52,멋쩍은 튜브,맞을까요?
3/15/24 1:52,치즈케이크,괜찮은 거 같은데요
3/15/24 1:52,멋쩍은 튜브,아 그런가요
3/15/24 1:53,멋쩍은 튜브,사진
3/15/24 1:53,멋쩍은 튜브,1번은 되게 예쁘게 감소했어서
3/15/24 1:53,멋쩍은 튜브,좀 신경쓰였는데 도움 주신 분들 감사합니다
3/15/24 1:53,치즈케이크,다른 분이 말씀해주신 것처럼 regularizer 때문에 일정 이상 떨어지진 않을 거에요
3/15/24 1:53,치즈케이크,정의상 parameter의 norm이 항상 loss에 들어가서
3/15/24 1:54,멋쩍은 튜브,아
3/15/24 1:54,멋쩍은 튜브,그렇군요
3/15/24 1:54,멋쩍은 튜브,감사합니다...!!
3/15/24 9:50,권투하는 무지,"regularizer term은 grad 계산할때는 포함되고 total loss 계산할 
때는 포함하지 않는 것이 맞나요?"
3/15/24 10:16,TA Gyeongmin,"일반적으로 loss를 계산할 때에도 함께 고려합니다. 
다만 minimize하는 식이 다르기에 regularizer term을 사용하지 않은 모델과 total loss의 크기를 단순 비교할 수는 없습니다."
3/15/24 10:18,권투하는 무지,아 알겠습니다 감사합니다!
3/15/24 10:36,눈물바다에 빠진 라이언,"Problem 2에서 regularizer을 쓰는 이유에 대해서 질문 있습니다!
regularizer term을 없애고 optimization을 하면 loss가 0으로 잘 감소하는 반면, regularizer term이 있을 때는 lambda가 아무리 작아도 loss가 0보다 큰 값으로 수렴하는 것 같습니다. regularizer term을 더해서 생긴 오차라기에는 그 term을 빼고(gradient에서는 고려) loss function 그래프를 그려봐도 0보다 큰 값으로 수렴하는데, 이런 페널티를 감수하고도 regularizer을 쓰는 이유가 있는지 궁금합니다."
3/15/24 10:37,눈물바다에 빠진 라이언,위 내용들도 읽어봤는데 의문이 잘 풀리지 않아서 질문드립니다..
3/15/24 10:42,멋쩍은 튜브,그게 아마
3/15/24 10:42,멋쩍은 튜브,Overfitting 방지 효과도 있다고 알고 있습니다
3/15/24 10:43,멋쩍은 튜브,Loss 가 작아도 overfitting이 되면 결과적으로 좋은 model이 아니니
3/15/24 10:43,멋쩍은 튜브,Loss가 좀 크더라도? 그게 무조건 나쁜 거라고 볼 수는 없을 거 같은 느낌이네요
3/15/24 10:43,눈물바다에 빠진 라이언,hard margin보다 soft margin을 선호한다는 건가요?
3/15/24 10:43,멋쩍은 튜브,샵검색: #overfitting
3/15/24 10:44,청소하는 튜브,지금 보고 있는 것이 training error라서 좋은 모델을 평가할 수 없습니다. 현재 loss가 0인지 아닌지보다 eval data에서 더 낮은지가 조금 더 robust한 것으로 알고 있씁니ㅏㄷ
3/15/24 10:46,눈물바다에 빠진 라이언,overfitting에 대해서는 개념적으로 이해하고 있는데
3/15/24 10:46,멋쩍은 튜브,약간 이런 느낌일 수도 있을 거 같아요...! 아마 람다 term을 통해 과적합에 페널티를 주고 그에 따라서 overfitting과 underfitting 사이에 적당히 균형을 맞춘 결과물이 나온다는 느낌이었던 거 같네요 튜브님 말대로 error가 작다고 좋은 모델이 아니라 test data에서 잘 적용되어야 좋은 모델이라서...?
3/15/24 10:47,눈물바다에 빠진 라이언,뭔가 SVM이 linearly separable인 데이터에 대해서도 정확도가 높은 편이 아닌 것처럼 나와서
3/15/24 10:47,눈물바다에 빠진 라이언,뭔가 찝찝하네요..
3/15/24 10:47,권투하는 무지,2번에서 learning rate나 iteration을 아무리 조정해도 모든 data point를 올바르게 분류하는 정도까지는 학습되지가 않는데 정상인가요...?
3/15/24 10:48,전커서교수님이될래요,삭제된 메시지입니다.
3/15/24 10:49,눈물바다에 빠진 라이언,그래도 일단 답변해주신 분들 감사합니다!
3/15/24 10:50,눈물바다에 빠진 라이언,좀 더 고민해볼게요
3/15/24 10:52,멋쩍은 튜브,https://daeson.tistory.com/m/184
3/15/24 10:52,멋쩍은 튜브,참고하세요! 설명 잘 된 거 같습니다
3/15/24 11:18,눈물바다에 빠진 라이언,"아아 그러면 regularizer의 역할은 θ_1, ..., θ_p 중에서 최소한의 variable을 사용하여 loss를 minimize하는 것이므로, 당연히 모든 θ 값을 사용하는 Problem 1보다 loss는 크게 나올 수 밖에 없지만, overfitting의 문제를 해결했다고 보면 되겠네요"
3/15/24 11:18,눈물바다에 빠진 라이언,다들 정말 감사합니다
3/15/24 13:18,Joshua,"I think it was said earlier, that we should try implementing sgd ourselves (eventhough its okay to just use pytorch). How should we go about calculating the gradient of SVM in problem 2?"
3/15/24 13:21,General Trash,"It should be done by hand, using the given loss function formula in the problem."
3/15/24 14:10,건배하는 프로도,"코드 pdf제출시에, 문제 푼 pdf에 코드 캡쳐한 사진을 이어붙여서 하나의 pdf를 만들어서 제출해도 되냐요?"
3/15/24 14:10,멋쩍은 튜브,둘다 된다고 하신거같아요
3/15/24 14:10,멋쩍은 튜브,저는 저번에 그렇게 내서 다 점수 제대로 받았어요
3/15/24 14:11,건배하는 프로도,네 감사합니다
3/15/24 16:11,소심한 네오,"2번은 잘 되는데, 1번 loss가 감소하다가 갑자기 계속 증가하는데 왜 이런지 아시는분 계신가요..
lr값도 조절을 해보았습니다"
3/15/24 16:11,건방진 제이지,혹시 train-test split을 하셨나요
3/15/24 16:11,건방진 제이지,하셨다면 Overfitting으로 인한 자연스러운 현상입니다
3/15/24 16:12,소심한 네오,split하지 않았습니다 ㅜ
3/15/24 16:12,소심한 네오,사진
3/15/24 16:17,기지개 고양이,저도 2번 이상합니다
3/15/24 16:17,기지개 고양이,ㅠㅠ
3/15/24 16:18,손을 번쩍 든 무지,저도 lr값을 계속 조정해 봤는데 그래프가 계속 올라가더라고요
3/15/24 16:18,소심한 네오,*2번도 regularizer term을 없애니 계속 증가하네요 ㅜㅜ
3/15/24 16:19,옐로카드 프로도,혹시 derv 초기화를 iteration마다 하셨나요? 저는 그것때문에 증가한 적이 있었습니다.
3/15/24 16:20,손을 번쩍 든 무지,derv가 그래디언트 말씀하시는 건가요?
3/15/24 16:22,ㅇㅇ,can we use minibatch sgd for problem 3?
3/15/24 16:22,옐로카드 프로도,네
3/15/24 16:23,손을 번쩍 든 무지,"2번은 max{ 0 , -(Y_i)*(X_i@θ)}라서 매 iteration마다 그래디언트 초기화를 어쩔 수 없이 하긴 했는데, 더 좋은 방법이 있을까요 ㅠㅠ"
3/15/24 16:47,시무룩한 튜브,사진
3/15/24 16:47,시무룩한 튜브,수업 피피티 슬라이드와
3/15/24 16:47,시무룩한 튜브,다운로드 받은 파일에서의 내용이 조금씩 차이가 있는데 수업 영상이 제일
3/15/24 16:47,시무룩한 튜브,정확한 걸까요?
3/15/24 16:48,General Trash,혹시 다운받으신 파일이 2022년 자료인가요?
3/15/24 16:52,힙합맨 제이지,힙합맨 제이지님이 나갔습니다.
3/15/24 16:57,시무룩한 튜브,아뇨 그냥 최신 자료 다운받은 걸로 기억합니다..
3/15/24 16:57,시무룩한 튜브,최신 자료 다시 확인해봤는데 강의자료와 아주 조금씩 다르네요!
3/15/24 17:05,초롱초롱 어피치,사진
3/15/24 17:05,초롱초롱 어피치,loss가 이렇게 나오면 뭔가 단단히 잘못하고 있는 거겠죠?
3/15/24 17:20,하트뿅뿅 라이언,혹시 loss를 구할 때 하나의 요소에 관해서만 계산을 했나요?
3/15/24 17:21,하트뿅뿅 라이언,모든 요소를 고려하고 평균내면 감소하는 모양이 보일 것 같은데
3/15/24 17:25,초롱초롱 어피치,아 평균을 내지 않았네요
3/15/24 17:25,초롱초롱 어피치,정말 감사합니다
3/15/24 17:25,초롱초롱 어피치,이모티콘
3/15/24 20:26,A+ is All you need,코딩 문제들은 묶어서 하나의 노트북파일로 작업해서 제출해도 상관없나요?
3/15/24 20:28,TA Youngmin,Yes. You may submit them in a single file.
3/15/24 20:43,소심한 네오,*plot을 잘못한거였습니다..!
3/15/24 22:10,손을 번쩍 든 무지,사진 2장
3/15/24 22:10,손을 번쩍 든 무지,"아까 나온 거 또 반복하는 것 같긴 한데, 2번 문제에서 똑같은 lr 값에서 regularizer를 넣으면 전자로, regularizer를 없애면 후자로 나오는 것 같은데, 이게 정상일까요...? ㅜㅜ"
3/15/24 22:13,청소하는 튜브,둘 다 아닌 거 같아서 그라디언트나 코드 실수 등 잘못된 게 있는지 확인해 보셔야 할 거 같아요
3/15/24 22:20,소심한 네오,"저도 비슷한 그림이 나왔었는데요, loss를 구할때 tensor끼리 곱하는 과정에서 문제가 있었어요"
3/15/24 22:21,소심한 네오,"hadamard product를 하고싶었는데, 모양을 맞춰주지 않아서 n by 1과 1 by n의 곱셈으로 n by n 행렬이 나온다던지"
3/15/24 22:23,손을 번쩍 든 무지,한번 더 확인해 봐야겠네요.. 두 분 모두 감사합니다!
3/15/24 22:33,송민창,사진
3/15/24 22:34,송민창,저는 2번 그래프가 loss 값이 감소하긴 하는데 진동이 심해서 이런 경우에 어떻게 해결하는지 아시는 분 있나요?
3/15/24 22:36,TA chaeju,lr을 적절히 조정해 보면 해결될 것 같습니다. lr이 조금 큰 것 같네요
3/15/24 22:38,송민창,현재 lr이 1e-5인데 더 줄이게 되면 진동은 조금 줄긴하지만 loss값이 매우 천천히 감소합니다
3/15/24 22:40,신난 어피치,혹시 Loss 값을 평균내지 않으신 거 아닐까요??
3/15/24 22:41,신난 어피치,이 케이스와 비슷하신 것 같은데
3/15/24 22:42,송민창,"저도 처음에 저런 식으로 나왔어서 N으로 나눠서 평균내는 방식으로 1번은 잘됐는데, 2번에서 평균 냈는데도 저런식으로 나오네요"
3/15/24 23:06,부탁하는 네오,알파 0.001 이터레이션 만번으로 해서 대충 일케나오는데 괜찮은거 맞나요?
3/15/24 23:06,부탁하는 네오,사진
3/15/24 23:06,부탁하는 네오,2번문제입니다
3/15/24 23:06,부탁하는 네오,레귤러라이저 때문에 저정도가 한계인거같은데
3/15/24 23:09,열심히 일하는 네오,수렴했다고 보시면 될 거 같네요
3/15/24 23:09,N이 되고픈 엡실론,저도 저정도로 나오네요
3/15/24 23:10,N이 되고픈 엡실론,사진
3/15/24 23:11,부탁하는 네오,답변 감사합니다
3/15/24 23:19,Is Ryu optimal?,Is Ryu optimal?님이 나갔습니다.
3/15/24 23:24,부탁하는 네오,"혹시 3번 문제 데이터생성하는 코드의 의미를
(1, 1)과의 거리가 sqrt(0.7)보다 작은 점을 -1로, 큰 점을 +1로 라벨링한 거라고 봐도 되나요?"
3/15/24 23:40,TA Hyojun,네 맞습니다
3/16/24 0:49,부탁하는 네오,근데 transformation에서 1이 있으니 이게 affine term b를 포함하는 것 아닌가요?
3/16/24 0:49,부탁하는 네오,그냥 a^T로 하셨다는 게 무슨 말인지 잘 이해가 안 갑니다..
3/16/24 1:12,부탁하는 네오,일단 저는 대강 이렇게 나오는데 이게 affine term이 있어서 이렇게 나오는건지는 잘 모르겠습니다
3/16/24 1:12,부탁하는 네오,사진
3/16/24 1:21,멋쟁이 프로도,개인적인 의견으로는 변환의 첫째 항(힌트에서 w[0])가 상수항과 같은 의미를 갖지 않나 싶습니다.
3/16/24 1:22,부탁하는 네오,네 그것까진 알겠는데 위에서 몇몇분께서 상수항 없이 하셨다는 말씀을 하셔서
3/16/24 1:22,부탁하는 네오,문제의 조건상 상수항이 w[0]으로 존재할 수밖에 없어서 뭐지 싶네요
3/16/24 1:24,멋쟁이 프로도,"문제 1번,2번에서 쓰인 모델 자체에는 상수항이 없는 게 맞는데, 이건 파라미터의 성분이 모두 변수라서 그런 거고 3번에 한해서는 파라미터에 상수 성분이 하나 있으니 문제 없어보입니다."
3/16/24 1:25,멋쟁이 프로도,저는 파랑 빨강 완전히 분리되게 나오긴 했습니다.
3/16/24 1:25,부탁하는 네오,어.. iteration이랑 alpha를 좀 바꿔봐야겟네요
3/16/24 1:25,부탁하는 네오,답변 감사합니다
3/16/24 1:26,신난 어피치,사진
3/16/24 1:26,신난 어피치,반복 수를 늘리니 나오더라고요
3/16/24 1:26,신난 어피치,저도 맨 처음에는 이렇게 나왔었습니다
3/16/24 1:26,부탁하는 네오,알파 0.01에 반복 십만번 하니까 잘 나오긴 하네요
3/16/24 1:26,부탁하는 네오,좀 더 좋은 방법 없으려나..
3/16/24 1:26,신난 어피치,제가 3만이였습니다
3/16/24 1:26,부탁하는 네오,십만번 하니까 로딩시간이 3초 정도 걸리네요
3/16/24 1:27,부탁하는 네오,3만으로 해도 잘 나오는군요
3/16/24 1:27,부탁하는 네오,알파랑 반복횟수는 그냥 시행착오로 막 바꿔보는 수밖에 없을까요?
3/16/24 1:28,ㅇㅇ,저는 minibatch sgd를 사용하니까 빠르게 되긴 했습니다
3/16/24 1:28,부탁하는 네오,저는 그냥 sgd로 해서 아무래도 minibatch보다는 횟수가 오래 걸리는거 같네요
3/16/24 1:31,멋쟁이 프로도,알파 좀 키워도 되지 않나요?
3/16/24 1:32,부탁하는 네오,얼마정도로 하셨어요?
3/16/24 1:45,Piepie,알파를 횟수별로 달리 하면 더 효율적일거 같은데요
3/16/24 1:45,Piepie,첨엔 좀 크게 하고 학습 좀 하면 작게 하고
3/16/24 1:51,멋쟁이 프로도,제가 0.01부터 1까지 해봤는데 다 되긴 하는거같아요
3/16/24 1:52,멋쟁이 프로도,근데 0.1보다 작으면 좀 오래걸려서..
3/16/24 1:55,부탁하는 무지,"1번과 2번 문제에서, SGD를 손으로 직접 그라디언트를 구해 구현하는 것이 아니라 pytorch loss function을 만들어서 작동하게끔 해도 괜찮나요?"
3/16/24 1:57,손을 번쩍 든 무지,.
3/16/24 9:00,청소하는 튜브,혹시 홈페이지에 올라와있는 코드 저만 html로 다운로드 되나요??
3/16/24 9:01,신난 어피치,pdf 로 하면 자꾸 오류나서 저는 html로 바꾸고 pdf로 인쇄하기 해서 냈어요
3/16/24 9:04,청소하는 튜브,제출하는 방식이 아니라 lecture 홈페이지에 올라온 예시 파이썬 파일이 html로 다운로드 되는 거 같습니다
3/16/24 9:28,애교뿜뿜 무지,사진
3/16/24 9:28,애교뿜뿜 무지,1번에 alpha가 크면 이렇게 되는데
3/16/24 9:29,애교뿜뿜 무지,괜찮지 않은 것이겠죠?
3/16/24 9:30,청소하는 튜브,loss가 1200이 나오는 게 이상한 거 같습니다
3/16/24 9:30,청소하는 튜브,다른 분들 보니까 전부 ~5 정도 였던 거 같습니다
3/16/24 9:30,애교뿜뿜 무지,log의 sum을 해놓긴 했는데/...
3/16/24 9:31,신난 어피치,N을 안나눈거 아닐까요..?
3/16/24 9:32,애교뿜뿜 무지,네 맞아요
3/16/24 9:35,신난 어피치,7번 개형 이런 식으로 plot하는 게 맞나요..? 청강생이라 수업을 잘 따라가고 있나 궁금합니다..
3/16/24 9:35,신난 어피치,사진
3/16/24 10:40,열심히 일하는 네오,"굉장히 기본적인 질문일 수도 있는데 문제 4, 5번의 random variable X is in C while C is a subset of R^m에 대해 질문이 있습니다. Random variable X는 일단은 함수(sample space가 domain인)인데 R^m의 subset에 속한다는건 그냥 X의 codomain이 R^m의 subset이라 그렇게도 표기하는 건가요?"
3/16/24 11:59,초롱초롱 어피치,네 그렇습니다
3/16/24 12:18,열심히 일하는 네오,제 질문에 답변 주신거 맞죠? 감사합니다!
3/16/24 12:20,눈물바다에 빠진 라이언,Problem 7의 경우 final trained function을 plot하라는 거는 스켈레톤 코드에 있는 2000 iteration마다 plot하는 코드를 지우고 K=10000일 때의 그래프만 plot하라는 건가요?
3/16/24 12:21,신난 어피치,아 이거 스켈코드가 있었군요..
3/16/24 12:25,TA Hyojun,2000 iteration마다 plot하셔서 training이 진행됨에 따라 f_theta가 f*에 가까워지는지 확인해보시면 됩니다.
3/16/24 12:26,눈물바다에 빠진 라이언,아하 감사합니다!
3/16/24 14:56,신난 어피치,딥러닝에서 initial point를 0으로 초기화하면 안 된다고 하셨는데 그러면 shallow neural network에서도 initial point를 0으로 설정하면 안 되나요?
3/16/24 15:08,ys,ys님이 나갔습니다.
3/16/24 15:39,손을 번쩍 든 무지,사진
3/16/24 15:40,손을 번쩍 든 무지,"이 부분을 '3개의 32*32 행렬의 원소들을 일렬로 늘어놓은 것이 X'라고 이해했는데요, 혹시 제대로 이해한 게 맞을까요?"
3/16/24 15:44,치즈케이크,저도 그렇게 이해했습니다
3/16/24 15:55,열심히 일하는 네오,Problem 7의 l_theta식 내부의 제곱은 element wise하게 적용된다고 보는거 맞죠?
3/16/24 15:59,열심히 일하는 네오,식을 잘못 본거였네요ㅎㅎ
3/16/24 16:32,멋쩍은 튜브,여러분 혹시
3/16/24 16:32,멋쩍은 튜브,1번 2번- error- iteration 그래프
3/16/24 16:32,멋쩍은 튜브,3번-plot 결과물만 첨부하면 되는 게 맞나요?
3/16/24 16:32,멋쩍은 튜브,따로 더 첨부할 게 있는지 애매해서 여쭙니다
3/16/24 16:36,손을 번쩍 든 무지,문제에서 요구하는 결과물은 그게 다인 것 같은데 따로 뭘 구할 필요는 없지 않을까요?
3/16/24 16:36,멋쩍은 튜브,아하 넵 감사합니다
3/16/24 16:36,멋쩍은 튜브,theta 스샷이라ㅏ도 찍어야 하나 고미냈어요
3/16/24 16:38,손을 번쩍 든 무지,혹시 pdf 변환 문제 때문인가요?
3/16/24 16:43,N이 되고픈 엡실론,끼어들어서 죄송합니다...! 혹시 3번 문제에서 logistic regression을 이용해서 데이터가 linearly separable하다는 것을 보일 수 있는 이유가 뭔가요? SVM의 경우는 데이터가 l.s.하면 loss가 0이라는 것을 알 수 있음을 이해했는데 l.r.의 경우는 어떻게 되는 것인지 잘 이해가 안됩니다...
3/16/24 16:47,N이 되고픈 엡실론,앗 SVM의 경우 loss가 0이면 l.s.함을 알 수 있다는 것이네요...그래도 logistic regression을 사용할 수 있는 이유가 잘 이해가 안됩니다
3/16/24 17:19,멋쟁이 프로도,"모델링한 식이 f=ax²+by²+cx+dy+e 꼴인데, LR을 통해서 얻은 f에 대해서 f>0인 영역(타원 외부)에서는 label=+1이고, f<0인 영역(타원 내부)에서 label=-1인 걸 확인할 수 있으니 linearly separable이라고 할수있지않을까요?"
3/16/24 17:19,N이 되고픈 엡실론,아 그럼
3/16/24 17:20,N이 되고픈 엡실론,LR을 통해 찾아낸 theta를 다시 2차원으로 보낸 decision boundary를 그림으로써
3/16/24 17:20,N이 되고픈 엡실론,데이터가 분리됨을 보이시오 의 느낌인 것인가요? loss function를 직접 언급할 필요 없이
3/16/24 17:22,ㅈㅅㅇ,네 이 문제 목적은 선형 분리가 안 되는 데이터를 고차원으로 옮기면 선형 분리가 된다는 걸 보여주는 문제같아요
3/16/24 17:23,N이 되고픈 엡실론,감사합니다~
3/16/24 17:23,손을 번쩍 든 무지,"2차원에서의 decision boundary가 5차원에서는 hyperplane (of dimension 4(=5-1))의 부분집합이니까, 기존의 2차원에서는 되지 않던 문제를 5차원으로 끌고 가면 해결할 수 있다는 문제 같습니다"
3/16/24 17:24,손을 번쩍 든 무지,제대로 된 답변인지는 모르겠네요 ㅠㅠ
3/16/24 17:29,멋쩍은 튜브,"혹시 7번에서 주어진 f_th 함수가 잘 작동되시나요 다들....? 1024개의 점이 있는 array인 xx를 인풋으로 넣는 순간 에러가 나는데, 제가 짠 함수가 아니라 주어진 함수라서 고민하다 올립니다!"
3/16/24 17:29,신난 어피치,저는 f_th 잘 작동합니다!
3/16/24 17:30,열심히 일하는 네오,f_th의 input은 X인데 잘못 넣으신거 아닌가요
3/16/24 17:31,열심히 일하는 네오,Problem 7에서의 png파일도 같이 제출해야하나요?
3/16/24 17:32,열심히 일하는 네오,"If the execution result(plot) is already included in the pdf file, you don't need to submit png image."
3/16/24 17:32,열심히 일하는 네오,안 넣어도 되네요
3/16/24 17:32,멋쩍은 튜브,사진
3/16/24 17:32,멋쩍은 튜브,이 부분 말한 거였습니다ㅜㅜ
3/16/24 17:32,부탁하는 네오,넵 그게 타원 등ㄱㅗ선 느낌으로 그려질 거예요
3/16/24 17:32,멋쩍은 튜브,제가 뭘 잘못했나보네용
3/16/24 17:34,멋쟁이 프로도,저도 같은 에러 떠서 걍 수정해서 쓰긴했어요
3/16/24 17:42,멋쩍은 튜브,"지금 7번에서 X[i], Y[i]들의 쌍이 dataset인 거죠...?"
3/16/24 17:43,멋쩍은 튜브,f_th가 inpur이 상수인 줄 알았는데 갑자기 1024개짜리 점들을 넣으니까
3/16/24 17:43,멋쩍은 튜브,당황스럽네요
3/16/24 17:56,nnd,nnd님이 나갔습니다.
3/16/24 18:00,애교뿜뿜 무지,다른 분 생각 있을까요?
3/16/24 18:04,열심히 일하는 네오,청소하는 튜브님이 잘 말해주신거 같아요
3/16/24 18:04,기타치는 튜브,N 나누고 해 보셨나요?
3/16/24 18:32,애교뿜뿜 무지,Loss에 N 나누는 것은 sgd 상에서는 문제 없지 않나요?
3/16/24 18:32,애교뿜뿜 무지,갑자기 loss가 내려가는게 이상했었습니다
3/16/24 18:36,멋쩍은 튜브,다들 7번 잘 나오시나요
3/16/24 18:36,멋쩍은 튜브,사진
3/16/24 18:36,멋쩍은 튜브,이러면 제출하면 안되겠죠...?
3/16/24 18:39,N이 되고픈 엡실론,사진
3/16/24 18:39,N이 되고픈 엡실론,저는 이렇게 나옵니다
3/16/24 18:39,멋쩍은 튜브,음 ..... 제출하면 안되겠네요 감사해요
3/16/24 19:08,멋쟁이 프로도,잘못된거같지는 않습니다. 초기 데이터에 따라서도 달라질 수 있고..
3/16/24 19:22,애교뿜뿜 무지,그런 거라면 다행이네요 감사합니다~
3/16/24 20:12,리듬타는 제이지,사진
3/16/24 20:12,리듬타는 제이지,pmf p가 R^n에 있다는 것은 어떤 의미인가요? 여기서 summation을 n개만 한것으로 봐서는 p와 q가 확률을 부여하는 RV가 가질 수 있는 값이 n개뿐이라는 것인가요?
3/16/24 20:15,멋쩍은 튜브,아마 n개 다 더하면 1인 거 아닐까요
3/16/24 20:16,리듬타는 제이지,네 만약 RV가 가질 수 있는 값이 n개면 각각 확률이 부여될테니 그 n개의 확률값을 더하면 1이 나오겠죠
3/16/24 20:17,리듬타는 제이지,이렇게 이해하는게 맞는지 궁금하네요
3/16/24 20:17,초롱초롱 어피치,맞습니다
3/16/24 20:17,리듬타는 제이지,감사합니다!
3/16/24 20:26,휘파람 프로도,혹시 아직 청강생 받으실까요? 청강으로 전환을 할까 고민중에 있어서요...
3/17/24 0:25,인사하는 제이지,"3번에서 LR이나 SVM을 사용하려면 변수가 벡터 a, 상수 b 2개가 있는데 어떻게 LR/SVM을 쓸 수 있나요?"
3/17/24 0:30,인사하는 제이지,해결했습니다!
3/17/24 1:32,Apeach enjoys music,"혹시 3번에서 X = X + np.array([[1] ,[1]]) 이 부분 연산에서 두 행렬의 차원이 안 맞아보이는데 덧셈 연산이 제대로 수행되어서 혹시 np내장 어떤 기능이 있는 것인지 궁금합니다.."
3/17/24 1:33,눈물바다에 빠진 라이언,numpy broadcasting 한번 검색해보시면 좋을 것 같습니다
3/17/24 1:35,Apeach enjoys music,아 넵 이해했습니다 넘 감사해요!
3/17/24 1:36,Apeach enjoys music,"혹시 궁금하실 분들을 위해
So effectively, np.array([[1], [1]]) is broadcasted to become [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]], making it compatible for addition with X. "
3/17/24 4:07,떨고있는 어피치,떨고있는 어피치님이 나갔습니다.
3/17/24 11:43,벌 서는 라이언,삭제된 메시지입니다.
3/17/24 12:23,.,7번 스켈레톤 코드에서 np.reshape을 하는 이유가 있나요
3/17/24 12:27,멋쩍은 튜브,그거 아마 나중에
3/17/24 12:28,멋쩍은 튜브,이거 때문이 아닐라요
3/17/24 12:28,멋쩍은 튜브,아 저는 결국 주어진 코드로 잘 됐습니다...
3/17/24 12:28,불 뿜는 튜브,불 뿜는 튜브님이 나갔습니다.
3/17/24 12:31,.,아 그래프 도식할려고 이렇게 한 거였군요 답변 감사드립니다
3/17/24 12:35,치맥하는 제이지,HW2 3번에서 아무리 alpha하고 반복횟수 바꿔봐도 이 이상으로 정확하게 안나오는데 원래 그런건가요
3/17/24 12:35,치맥하는 제이지,사진
3/17/24 12:35,신난 어피치,10만정도로 때려박아도 안되나요..?
3/17/24 12:35,치맥하는 제이지,미니배치 sgd로 30000까지 해봤는데 계속 저래 나오네요
3/17/24 12:36,치맥하는 제이지,100000한번 해보고 오겠습니다
3/17/24 12:36,DL하는 블루,혹시 regularizer 때문일 수도 있어요
3/17/24 12:36,손을 번쩍 든 무지,혹시 LR이나 SVM을 위한 그래디언트를 잘못 구하신거 아닐까요?
3/17/24 12:36,소심한 네오,SGD는 잘 안되고 LR이 잘되더라구요
3/17/24 12:37,치맥하는 제이지,앗 과제에 lr이나 svm 쓰라고 나와있었네요
3/17/24 12:37,치맥하는 제이지,sgd로 100000 해봐도 잘 안되는것 같아요
3/17/24 12:37,열심히 일하는 네오,저는 SVM에 람다 0.001로 한번 해놓고 돌렸는데 잘 되더라고요
3/17/24 12:38,신난 어피치,저는 LR로 알파 0.01로 두고 했습니다
3/17/24 12:38,치맥하는 제이지,조언 감사드립니다!
3/17/24 12:39,으쓱으쓱 어피치,"전 a 벡터와, batch로 들어오는 x를 broadcast하기 위해서라고 이해했습니다"
3/17/24 12:44,인사하는 제이지,"Lr/svm을 쓰는 문제는 Lr/svm 식에 sgd를 적용하면 되는거죠?
Sgd만 사용하는게 어떤건지 잘 모르겠네요 ㅠㅠ"
3/17/24 12:45,열심히 일하는 네오,Sgd만 loss func없이는 못 써요. 아마 잘못 보신거 같네요
3/17/24 12:47,손을 번쩍 든 무지,1번이랑 2번을 참조하시면 될것같아요
3/17/24 12:47,소심한 네오,아 SVM을 잘못말했네요
3/17/24 12:48,인사하는 제이지,아하 그런거군요
3/17/24 12:48,인사하는 제이지,감사합니당
3/17/24 12:48,초롱초롱 어피치,1번이랑 2번 error plot도 첨부해야하나요? 문제에서는 그냥 use SGD to solve라 되어있어서 최종 theta값만 pdf로 제출하면 되는걸까요?
3/17/24 12:49,열심히 일하는 네오,Error plot만 제출해도 되는걸로 알고있어요
3/17/24 12:49,손을 번쩍 든 무지,.
3/17/24 12:49,초롱초롱 어피치,감사합니다
3/17/24 14:16,벙찐 튜브,"Problem 2의 Empirically, does the SGD ever encounter a point of non-differentiability?에 대한 답변도 적어야 할까요"
3/17/24 14:16,TA Hyojun,네 해당 내용에 대한 설명도 적어주시기 바랍니다
3/17/24 14:17,벙찐 튜브,넵 감사합니다:)
3/17/24 14:41,말썽쟁이 네오,"크로스 엔트로피를 줄이는 것이 f_theta를 f_*에 근사할 수 있는 이유가 뭔지 모르겠습니다. 
f_theta 가 f _*에 잘 근사됐다면 크로스 엔트로피가 줄겠지만 데이터의 비율만 맞췄을 뿐인데 그 역이 성립하는지는 잘 모르겠습니다."
3/17/24 14:51,기타치는 튜브,잘 근사된다는 척도를 cross entropy라는 거리 함수 하에서 가까워지는 것으로 정했다고 이해했습니다.
3/17/24 15:01,멋쟁이 프로도,강의자료에 있는 MLE와 D_KL의 관계를 설명한 부분이 근거가 될수있지 않을까요?
3/17/24 15:09,말썽쟁이 네오,방금 그 부분을 읽어봤는데 MLE는 X가 적은 가지수의 라벨로 좁혀진 정도 같은데 만약 완전히 엉뚱한 함수로 근사됐지만 적은 가짓수의 라벨로 근사됐고 라벨의 전체 비율도 실제 주어진 데이터와 비슷할 수도 있을 것 같아서요 라벨끼리의 차이를 계산하는 게 아니라 전체 나온 값의 빈도의 차이만 계산하는 게 뭔가 믿음이 안 갑니다. 제가 이해를 잘 한 것인지 모르겠네요
3/17/24 15:11,부탁하는 네오,7번 이정도면 괜찮은건가요?
3/17/24 15:11,부탁하는 네오,사진
3/17/24 15:11,부탁하는 네오,아니 코드 개대충 짰는데 이게 왜되는지 모르겠네요;;
3/17/24 15:12,손을 번쩍 든 무지,사진
3/17/24 15:12,손을 번쩍 든 무지,삭제된 메시지입니다.
3/17/24 15:12,손을 번쩍 든 무지,저도 대충 이렇게 나왔습니다
3/17/24 15:23,열심히 일하는 네오,"def logistic_loss(output, target): 
    return -torch.nn.functional.logsigmoid(target*output)           이번 주 월요일 수업 내용인데요. 여기서 왜 -를 꼭 넣어줘야하는지 모르겠네요. 혹시 pytorch를 사용하면 자체적으로 maximization을 해서 그런건가요?"
3/17/24 15:24,TA chaeju,"17페이지 (Relaxed supervised learning setup)을 참고하시면 도움이 될 듯 합니다.

label들의 distribution을 estimate하는 것이 목표라고 할 수 있겠습니다."
3/17/24 15:25,말썽쟁이 네오,감사합니다
3/17/24 15:26,TA chaeju,log sigmoid function의 형태를 생각해 보시면 됩니다.
3/17/24 15:26,TA chaeju,사진
3/17/24 15:27,TA chaeju,target*output이 양수일 때 (즉 target label과 predicted label이 일치할 때) loss가 작게 하려면 (-1)을 곱해줘야겠죠
3/17/24 15:27,열심히 일하는 네오,Log sigmoid가 수업에서 나오던 그래프하고 다르게 생겼었네요 감사합니다!
3/17/24 15:29,TA chaeju,"아 네, 이런 경우 PyTorch documentation을 참고해 보시면 도움이 됩니다"
3/17/24 15:29,열심히 일하는 네오,넵 감사합니다!
3/17/24 18:25,치맥하는 제이지,HW2 no2는 어느 정도 정확도가 나와야지 성공했다고 볼수 있을까요? 80%대에 계속 머물러 있는데 alpha나 iterations을 쫌 더 바꿔봐야 할까요
3/17/24 18:50,열심히 일하는 네오,"HW1,2번 모두 train_loss plot만 보여주면 되나요? 아니면 test accuracy까지 계산해야 하나요?"
3/17/24 18:51,멋쩍은 튜브,저는 plot만 그렸어요
3/17/24 18:51,멋쩍은 튜브,2번은 그
3/17/24 18:51,멋쩍은 튜브,미분 안더ㅣ는 점 도알하는지도
3/17/24 18:51,멋쩍은 튜브,도달
3/17/24 18:51,멋쩍은 튜브,서술하셔야 된대요
3/17/24 18:52,열심히 일하는 네오,아하 넵넵 감사합니다 !!
3/17/24 19:27,열심히 일하는 네오,"혹시 3번 구현할 때, LR이나 SVM을 사이킷런에서 불러오지 않고, 직접 train 시킨 후에 시각화해야 하는 걸까요.. !!"
3/17/24 19:27,화난 라이언,1번 2번 하셨으면 그냥 그거 복붙하시면 될 듯해요
3/17/24 20:14,열심히 일하는 네오,어머 그러네요 감사합니다 ㅋㅋ ㅜㅜㅜ !!!
3/17/24 20:15,하트뿅뿅 라이언,혹시 이게 무슨 뜻인지 알 수 있을까요 ??
3/17/24 20:24,멋쟁이 프로도,그 앞 문제에서 LR이랑 SVM을 이미 구현했으니 그 코드로 학습시키면 되겠죠
3/17/24 20:27,하트뿅뿅 라이언,이모티콘
3/17/24 20:35,손을 번쩍 든 무지,3번 혹시 SVM에서 aTx + b로 하신 분 계실까요? 저는 aTx로 하면 잘되는데 b넣으면 잘 안되네요
3/17/24 20:39,기타치는 튜브,저도 그렇고 위에 보시면 다른 분들도 b 안 넣으신 것 같습니다
3/17/24 20:42,벙찐 튜브,사진
3/17/24 20:42,벙찐 튜브,b를 넣어서 이렇게 나오긴 했는데
3/17/24 20:42,벙찐 튜브,대부분의 값에 대해선 잘 안 되더라고요
3/17/24 20:43,벙찐 튜브,근데 SVM을 aTx로 해도 되나요
3/17/24 20:45,전커서교수님이될래요,"x = [1, u, u2, v, v2]이라 되어있으니까"
3/17/24 20:45,전커서교수님이될래요,aTx만 사용해도 a[0]이 bias 역할을 해주는거같아요
3/17/24 20:47,좌절하는 라이언,좌절하는 라이언님이 나갔습니다.
3/17/24 20:50,손을 번쩍 든 무지,네 저도 그렇게 생각했습니다
3/17/24 20:50,손을 번쩍 든 무지,모두 감사드립니다!
3/17/24 20:51,벙찐 튜브,감사합니다 수정해보겠습니다ㅠ
3/17/24 22:20,하트뽀뽀 어피치,"1,2번 X,Y가 서로 관련이 없는데.. 학습이 되나요?? 문제를 이해를 잘못했나해서요"
3/17/24 22:29,기지개 고양이,그냥 어떤 데이터셋이든 주어진 loss func.의 값을 최소화시키는 방향으로 학습하는 거니까
3/17/24 22:30,기지개 고양이,실제로 관련이 있고 없고 여부는 중요하지 않은거 같아요
3/17/24 22:30,손을 번쩍 든 무지,"X_i를 data, Y_i를 label (+1 또는 -1) 이라고 한다면, 1,2번은 binary classification에 관한 문제입니다."
3/17/24 22:32,하트뽀뽀 어피치,"맞죠맞죠. 푸는데는 지장은 없지만, 그냥 교수님께서 출제하신 심오한 의도가 있을거 같은데 전혀 모르겠어서 여쭤봤어요."
3/17/24 22:32,화난 라이언,아마 데이터 사이즈에 비해 feature 크기가 큰 편이라 binary classification이 잘 되는 것 같아요
3/17/24 22:33,하트뽀뽀 어피치,엥 classification이 되나요?
3/17/24 22:34,손을 번쩍 든 무지,1번을 예로 들면 GSD를 이용해서 loss의 optimal value가 0으로 가게 하는 거니까 classification이 된다고 봐야겠죠..?
3/17/24 22:35,하트뽀뽀 어피치,좀더 생각해볼께요. 다들 감사합니다.
3/17/24 22:35,손을 번쩍 든 무지,* loss의 value인데 오타냈네요 ㅠ
3/17/24 22:39,하트뽀뽀 어피치,삭제된 메시지입니다.
3/17/24 23:23,쑥스럽게 인사하는 프로도,"안녕하세요, HW2 4번 5번에서 p_i, q_i 값 중 0인 것이 있다고 가정하면 log 값이 +/- ∞가 될 것 같은데, p_i>0, q_i>0이라고 가정하고 문제를 풀어도 되나요? 아니면 혹시 제가 잘못 이해하고 있는 부분이 있는 것일까요?"
3/17/24 23:27,TA Hyojun,사진
3/17/24 23:27,TA Hyojun,위 슬라이드를 참고하시기 바랍니다
3/17/24 23:27,신난 어피치,Convention 해당하는 경우는 따로 빼서 보였어요
3/17/24 23:29,하트뿅뿅 라이언,qi만 0인 경우를 따로 처리해 줘야 할까요?
3/17/24 23:30,리듬타는 제이지,리듬타는 제이지님이 나갔습니다.
3/17/24 23:30,쑥스럽게 인사하는 프로도,앗 저 부분을 놓쳤네요 늦은 시간인데도 도와주셔서 감사합니다 조교님
3/17/24 23:32,TA Hyojun,넵 해당 경우도 간단하게 설명만 해 주시면 될 것 같습니다
3/18/24 0:00,눈빛 애교 어피치,안녕하세요. 혹시 HW2 2번에서 lambda * ||theta||^2 는 시그마 밖에 있는 건가요?
3/18/24 0:00,인사하는 프로도,네네
3/18/24 0:27,벙찐 튜브,"SGD에서 gradient를 계산할 때, 2lambda * theta를 N으로 나눈 값을 이용하는 것이 맞나요 아니면, regularizer을 gradient한 것을 그대로 사용하는 것이 맞나요"
3/18/24 0:27,벙찐 튜브,아 problem 2에 대한 질문입니다
3/18/24 0:30,Tube cleaning,lambda * ||theta||^2가 sigma 밖에 있어서 후자가 맞는 것 같아요!
3/18/24 1:08,눈물바다에 빠진 라이언,혹시 과제 파일 여러 개 한 번에 제출했다가 하나만 다시 제출해도 채점에 영향 없나요..?
3/18/24 1:11,TA Hyojun,여러 파일 중 하나만 수정해서 해당 파일만 다시 제출하신 건가요?
3/18/24 1:13,눈물바다에 빠진 라이언,"아직 제출은 안했는데, 이럴 경우에 수정한 파일만 다시 제출하면 되는지, 아니면 그 파일을 포함해서 모든 파일을 다시 제출해야 하는지 궁금합니다! 로그는 남는 거 같은데 하나만 다시 제출하면 채점하시기 힘들까 싶어서요"
3/18/24 1:16,TA Hyojun,아 네네 해당 파일을 포함해서 모든 파일을 다시 제출해 주시면 채점이 좀 더 편리할 것 같습니다. 해당 파일만 제출하시는 경우 관련 내용을 댓글로 적어주시면 감사하겠습니다.
3/18/24 1:19,눈물바다에 빠진 라이언,네 늦은 시간에 감사합니다!
3/18/24 3:13,눈물바다 라이언,"안녕하세요, 혹시 문제 1-3까지는 plot 이미지만 제출하고 코드는 7번에 해당하는 것만 제출하는 것이 맞나요..?"
3/18/24 9:00,TA Hyojun,1-3번과 7번문제 모두 코드와 실행결과를 제출해 주시기 바랍니다.
3/18/24 9:03,멋쩍은 튜브,1-3번의 실행결과는 뭔가요....?
3/18/24 9:04,멋쩍은 튜브,아
3/18/24 9:04,멋쩍은 튜브,코드가 포인트였군요 죄송합니다
3/18/24 9:45,눈물바다 라이언,감사합니다!
3/18/24 11:49,열심히 일하는 네오,"저번 수업에 gpu에 데이터를 올릴 때 loss function으로 cross entropy를 택하고 optimizer로 cyclic SGD 진행했는데 loss function과 optimizer는 gpu에 따로 안 올려도 잘 작동을 하네요. 혹시 loss function, optimizer둘다 torch로 정의되다보니 GPU에 있는 데이터도 접근가능한건가요?"
3/18/24 11:53,초롱초롱 어피치,https://discuss.pytorch.org/t/what-does-it-mean-to-move-a-loss-function-to-device-gpu/52832
3/18/24 11:53,초롱초롱 어피치,이걸 읽어보시면 좋을듯합니다
3/18/24 11:56,열심히 일하는 네오,감사합니다!
3/18/24 13:41,경례하는 프로도,1번 sgd문제에서 쎄타가 어디로 수렴해야 맞게 푼 것일까요? 친구랑 대조해보는데 둘다 돌아가긴 하지만 수렴하는 벡터가 많이 달라서 걱정하고 있습니다
3/18/24 13:42,비옷입은 튜브,더 작은 함수값이 어디로 수렴하나요?
3/18/24 14:08,경례하는 프로도,"2]
# SGD formula
#B is batch size for each step
def sgd (theta, X, Y, Alpha, B): 


  theta = theta - Alpha*fdiff(theta,X,Y,B)

  return theta

results = []

[[ 0.75981536]
 [-0.28300271]
 [-0.65600254]
 [ 1.24212014]
 [-1.46856482]
 [ 0.84505869]
 [-0.58419883]
 [-0.53001073]
 [ 1.48760145]
 [-0.58826318]
 [-1.40027027]
 [ 3.05731597]
 [ 0.32553167]
 [ 0.38512138]
 [ 0.3440332 ]
 [-1.10932338]
 [ 0.34204915]
 [-0.57689548]
 [ 1.13273286]
 [ 0.49915785]]"
3/18/24 14:08,경례하는 프로도,저는 여기로 가는것 같습니다
3/18/24 14:18,손을 번쩍 든 무지,삭제된 메시지입니다.
3/18/24 14:19,손을 번쩍 든 무지,"문제 1,2번에서 iteration이 다 끝난 후의 theta도 명시해 줘야 하나요??"
3/18/24 14:20,경례하는 프로도,헐 저는 오히려 로스펑션을 이벨류에이트를 안 했네요 세타만 구하고
3/18/24 14:24,멋쩍은 튜브,Loss ftn plot은 해야돼요
3/18/24 14:24,멋쩍은 튜브,.
3/18/24 14:24,멋쩍은 튜브,세타 얘기는 모르겠네요
3/18/24 14:27,손을 번쩍 든 무지,loss function plot은 했는데 theta까지 구하라는 건 문제에서 안 보였어서요..
3/18/24 14:28,멋쩍은 튜브,저도 안했아요...
3/18/24 14:30,벌 서는 라이언,저도요…
3/18/24 14:31,하트뿅뿅 라이언,하트뿅뿅 라이언님이 나갔습니다.
3/18/24 14:34,경례하는 프로도,제가 그냥 쓸데없이 신경쓴 것 같습니다
3/18/24 14:34,경례하는 프로도,plot하라고 알려주셔서 고맙습니다
3/18/24 14:39,TA Jongchan,"Theta는 initial value와 step size 등에 따라 다른 값으로 수렴할 수 있습니다. 정해진 정답은 없지만, loss가 잘 수렴하는지는 확인하시기 바랍니다."
3/18/24 14:49,TA Jongchan,"문제에서 ""solve"" the optimization problem 이라고 명시했기 때문에 theta를 구하고 결과로 보여주는 것이 이상적입니다.
다만 1, 2번 문제의 경우 SGD 구현이 핵심이기 때문에, loss plot 등을 통해 SGD가 잘 동작함을 보여주셨다면 theta 값을 명시하지 않았더라도 괜찮습니다."
3/18/24 15:05,손을 번쩍 든 무지,넵 감사합니다!
3/18/24 16:03,째려보는 어피치,2번 문제에서 accuracy를 계산하면 0.5까지밖에 안 올라가는데 정상인가요?
3/18/24 16:03,째려보는 어피치,사진
3/18/24 16:36,눈물바다에 빠진 라이언,조금 이상한 것 같긴 합니다. 저는 hyper-parameter를 막 조정하지도 않았는데 83%까진 올라갔었어요
3/18/24 16:56,서영호,서영호님이 들어왔습니다.
3/18/24 17:01,비옷입은 튜브,줌 녹화영상은 어디에서 볼 수 있는건가요?
3/18/24 17:01,손을 번쩍 든 무지,etl에 올라와 있습니다!
3/18/24 17:02,.,Lecture Recording 문서에 주소 있어서 그거 보시면 됩니다
3/18/24 17:02,비옷입은 튜브,아 넵 감사합니다!
3/18/24 17:09,경제학도,"혹시 ""벡터 혹은 행렬의 원소들 중 하나 이상이 무한대이면 해당 벡터 혹은 행렬은 무한대이다""라고 할 수 있나요?"
3/18/24 17:11,초롱초롱 어피치,보통 그런 표현은 안쓰죠
3/18/24 17:19,경제학도,그렇다면 혹시 벡터 혹은 행렬이 발산(diverge)한다고 할 때 그 정의는 어떻게 되나요?
3/18/24 17:23,N이 되고픈 엡실론,벡터/행렬의 수렴을 성분별 수렴으로 정의하고 그게 실패하면 발산한다고 부를 것 같네요
3/18/24 17:25,멋쩍은 튜브,행렬이나 벡터의 norm이 무한대이다? 까지는 말할 수 있을 거 같네요
3/18/24 17:33,TA chaeju,"vector나 matrix의 norm이 적절히 정해져 있을 때, 그 norm에 대해 수렴함을 정의합니다.

자주 사용되는 l2 norm (성분들의 제곱의 합의 제곱근)을 기준으로 보았을 때, vector 혹은 matrix의 특정 항이 무한대로 발산하는 상황이면, vector 혹은 matrix가 수렴하지 않음은 쉽게 보일 수 있습니다.
(정확히는, vector 혹은 matrix가 수렴한다면, 각 원소들도 수렴함을 보일 수 있습니다.) "
3/18/24 17:34,멋쩍은 튜브,혹시 그러면 norm 이 수렴한다면 그 vector/matrix가 수렴하는 거라는 의미인가요...?
3/18/24 17:37,TA chaeju,"vector들의 sequence v1, v2, v3…이 v로 수렴함은
""||vn - v||가 0으로 수렴한다""로 정의합니다"
3/18/24 17:41,경제학도,HW1의 Problem3 solution에서 (1-α*ρ)^k가 발산함에 따라 (Ι-α*Λ)^k 또한 발산함을 이해하기 위해 여쭈어 본 질문이었습니다. 모두 답변 감사드립니다.
3/18/24 17:41,TA chaeju,유한 차원에서는 이 정의와 동치입니다.
3/18/24 17:56,멋쩍은 튜브,잠깐 놓쳤는데
3/18/24 17:56,멋쩍은 튜브,사진
3/18/24 17:56,멋쩍은 튜브,여기 why에 대한 부분이 max pooling을 해주는 거니까 활성함수를 씌우는 게 부자연스럽다
3/18/24 17:56,멋쩍은 튜브,정도인가요...?
3/18/24 17:57,화난 라이언,직접 생각해보라고 하십니다
3/18/24 17:58,멋쩍은 튜브,"헉 ㅋㅋㅋ 그랬군요,,,"
3/18/24 18:28,멋쟁이 프로도,부자연스럽다기보단 MaxPool 이후의 ReLU는 redundant하다고 말씀하셨습니다.
3/18/24 18:29,멋쩍은 튜브,앟 감사합니다
3/18/24 19:02,떨고있는 어피치,떨고있는 어피치님이 나갔습니다.
3/18/24 19:13,클라인,클라인님이 들어왔습니다.
3/18/24 22:32,눈물바다에 빠진 라이언,"Problem 5.(b)에서 Show that softplus has Lipschitz continuous ""derivatives""라는 것은 σ'뿐만 아니라 모든 n에 대해서 σ^(n)도 Lipschitz continuous라는 것을 보이라는 건가요?"
3/18/24 22:49,TA Hyojun,σ'에 대해서만 보이시면 됩니다.
3/18/24 22:50,눈물바다에 빠진 라이언,넵 감사합니다!
3/18/24 23:35,베개를 부비적대는 라이언,예정된 휴강 날짜가 있을까요?
3/18/24 23:36,옐로카드 프로도,25일 휴강인 것으로 알고 있습니다
3/18/24 23:37,베개를 부비적대는 라이언,감사합니다!
3/18/24 23:39,쑥스럽게 인사하는 프로도,수업 홈페이지에도 나와있어요
3/18/24 23:40,베개를 부비적대는 라이언,찾아봤는데 못 찾았어요 능력부족 핑프였습니다
3/18/24 23:41,손을 번쩍 든 무지,https://ernestryu.com/courses/deep_learning.html
3/18/24 23:41,베개를 부비적대는 라이언,헉 바로 위에 있었네요
3/18/24 23:41,손을 번쩍 든 무지,주요 공지사항은 교수님 홈페이지에 올라와있습니다!
3/19/24 0:15,베개를 부비적대는 라이언,전치 컨볼루션의 연산이 와닿지 않는데 간단하게만이라도 intuition을 받을만한 설명을 해주실 수 있을까요
3/19/24 0:16,아침햇살,컨볼루션을 행렬과의 곱으로 표현할 수 있는데 전치컨볼루션은 그때의 행렬을 transpose한 것과의 곱으로 표현할 수 있습니다.
3/19/24 0:27,손을 번쩍 든 무지,삭제된 메시지입니다.
3/19/24 0:28,손을 번쩍 든 무지,제 뇌피셜이긴 한데
3/19/24 0:28,손을 번쩍 든 무지,"2D 텐서곱(시그마 β) 
--> 3D 텐서곱(시그마 α) 
--> 4D 텐서곱(시그마 γ)
 
이 형식으로 점점 차원을 올려 가는 것 같은데, (이때 텐서곱은 벡터의 내적처럼 서로 대응되는 원소들끼리 곱하여 더하는 형식으로 이루어짐) 저도 잘 와닿지는 않네요 ㅠㅠ"
3/19/24 0:31,베개를 부비적대는 라이언,다들 감사합니다!!
3/19/24 3:18,인사하는 프로도,"늦은 시간에 죄송합니다. HW3 6, 7번은 저희가 직접 코드를 짜서 푸는 문제인가요, 손으로 푸는 문제인가요?"
3/19/24 9:17,기타치는 튜브,저는 둘 모두 그냥 손으로 증명했습니다
3/19/24 10:15,청소하는 튜브,1번 문제에서 K 값 수정해도 괜찮을까요. 주어진 K로 하니 약간 성능이 많이 떨어지는 거 같고 K를 높이니 test data 성능이 매우 좋아져서 문의드립니다. 
3/19/24 13:51,하트뿅뿅 라이언,Problem2 해보세욥
3/19/24 15:05,ㅇㅇ,혹시 hw2 솔루션 올라왔나요?
3/19/24 15:07,TA chaeju,"네, 파일 탭에서 확인하실 수 있습니다"
3/19/24 15:55,치맥하는 제이지,"self.layer2 = nn.Linear(64, 64, bias=True) 이렇게 설정하면 input을 #x64로 받아서 #x64를 output으로 내보내는 layer가 생기는게 맞나요?"
3/19/24 15:55,치맥하는 제이지,128x64크기 행렬을 넣어보니까 dimesion이 안맞다고 계속 떠서요...
3/19/24 15:56,초롱초롱 어피치,맞습니다
3/19/24 15:57,초롱초롱 어피치,오류 메시지가 어떻게 나오나요?
3/19/24 15:58,치맥하는 제이지,RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x64 and 1x64)
3/19/24 15:58,치맥하는 제이지,이렇게 나옵니다
3/19/24 15:58,인사하는 프로도,아하 dimension 에러네요
3/19/24 15:59,인사하는 프로도,이 에러는 마지막 레이어에서 나는 것 같습니다
3/19/24 16:00,치맥하는 제이지,"line 50, in forward
    x = nn.functional.sigmoid(self.layer2(x))
                              ^^^^^^^^^^^^^^
근데 에러 메시지 윗부분 보면 이래 되있어서"
3/19/24 16:03,치맥하는 제이지,아 원인을 찾았습니다...ㅋㅋㅋㅋㅋㅋㅋ
3/19/24 16:03,치맥하는 제이지,"model.layer1.weight.data = torch.normal(0, 1, model.layer1.weight.shape)
model.layer1.bias.data = torch.full(model.layer1.bias.shape, 0.03)
(과제에 나와있는) 이 코드 가지고 initialize를 하고 있었는데"
3/19/24 16:04,치맥하는 제이지,복붙하는 과정에서 dimesion 을 다 layer1.shape으로 맞춰버렸내요ㅋㅋㅋ
3/19/24 16:05,인사하는 프로도,아하 그렇군요 다행입니다!
3/19/24 16:59,붕붕,Is it just me or is the jupyter notebooks on the course website supposed to be unavailable?
3/19/24 16:59,붕붕,I only got a weird html file
3/19/24 17:01,Tube cleaning,You can simply change it’s extension to .ipynb to view the file
3/19/24 17:01,벌 서는 라이언,저는 ipynb 파일로 잘 다운받아집니다. 그런데 예전에 비슷한 질문이 올라온 걸 보고 폰으로 다운받았을 때에는 html로 받아졌던 것 같습니다.
3/19/24 17:13,기지개 고양이,가끔씩 .ipynb.html 같이 확장자가 이상하게 되는 경우 있더라고요
3/19/24 17:43,화나서 방방 뛰는 튜브,혹시 청강 전환 하셨나요? 저도 고민중이라..
3/19/24 18:20,손을 번쩍 든 무지,5번 문제 (b)에서 ReLU 함수의 derivative는 z = 0인 경우는 고려하지 않아도 되는 건가요?
3/19/24 18:32,뿅뿅 네오,뿅뿅 네오님이 나갔습니다.
3/19/24 18:36,멋쩍은 튜브,"개인적인 궁금증인데 .py 파일을 ipynb에서 쓰려면 복붙하는 법밖에 없나요? 현재 과제용 .py 파일들을 복붙해서 쓰는 중인데, ipynb와 py의 탭 띄어쓰기 길이 차이 때문에 불편해서 좀 올바르게 옮기는 법이 있나 해서요"
3/19/24 18:37,택배 상자를 든 네오,저도 궁금합니다
3/19/24 18:50,베개를 부비적대는 라이언,코랩에서 업로드 해서는 안 되나요?
3/19/24 19:09,멋쩍은 튜브,Ipynb가 아니면 저는 업로드가 안되는거같았어요
3/19/24 19:11,열심히 일하는 네오,"전 업로드하긴 했는데, ipynb로 안 열리더라구요"
3/19/24 19:12,열심히 일하는 네오,py to ipynb하는 라이브러리쓰거나 vscode 쓰는 것도 방법일 것 같습니닷
3/19/24 19:53,엄지척 튜브,"In problem 3, I don't understand how \ell^{CE} ( \lambda e_y , y ) works.
Is it right that if j = 1, exp(f_j) denotes exp( \lambda e_1 ) ?"
3/19/24 21:39,열심히 일하는 네오,Hw3 1번 문제가 잘 이해가 안 가네요. 스켈코드에서 test_dataloader가 있는데 문제에서 사실상 모든 hyperparameter값을 지정해줬으면 test data가 사실상 안 쓰이는 거 아닌가요? 문제도 최종 train된 함수를 plot하는건데
3/19/24 21:52,ㅈㅅㅇ,"문제의 발문이 저도 잘은 이해가 안 가긴 하는데 제가 풀어본 바로는 1, 2번 문제의 상황에서 train_loss를 구하는게 어떠한 의미를 갖기는 했습니다"
3/19/24 21:53,ㅈㅅㅇ,제가 잘못 구한 것일수도 있지만...
3/19/24 21:53,ㅈㅅㅇ,아니
3/19/24 21:53,ㅈㅅㅇ,test loss네요
3/19/24 21:53,ㅈㅅㅇ,잘못 썼습니다
3/19/24 21:53,열심히 일하는 네오,혹시 주어진 모든 조건대로 해서 곡선에 잘 fit되셨나요?
3/19/24 21:54,열심히 일하는 네오,사진
3/19/24 21:55,열심히 일하는 네오,저는 이렇게 나오는데 다른 분들은 제대로 잘 나오나요?
3/19/24 21:55,ㅈㅅㅇ,파라미터 initialization안 하신 것 같네요
3/19/24 21:55,열심히 일하는 네오,문제에서 주어진 코드대로 파라미터 조정하긴 했어요
3/19/24 21:55,ㅈㅅㅇ,그러면 잘 돼야 하는데
3/19/24 21:56,ㅈㅅㅇ,코드에 문제가 있으신 것 같아요
3/19/24 21:56,열심히 일하는 네오,그런가요 일단 다시한번 확인해볼게요 감사합니다
3/19/24 21:57,TA chaeju,"Note that f is R^k vector, and f_j denotes the j-th component of the f. thus for each j, f_j is a scalar."
3/19/24 22:12,선풍기 바람 쐬는 어피치,"problem 3-a에서 k=1일때는 loss가 0나오는데, k>1일때만 고려하면 되나요?"
3/19/24 22:13,TA chaeju,"네, k>1을 가정해주시면 됩니다."
3/19/24 22:14,TA chaeju,"*실제로 CE loss가 사용되는 multi-class classification에서 k는 category의 개수를 의미하므로, k>1은 매우 자연스러운 가정이라 할 수 있겠습니다"
3/19/24 22:17,파이팅하는 무지,전 initialization 잘못했을때 이렇게 나왔어요
3/19/24 22:17,열심히 일하는 네오,감사합니다
3/19/24 22:18,JX,Sorry for asking. May I know how is the grading scale if I change to S/U? 
3/19/24 22:26,ㅇㅇ,.
3/19/24 22:34,DL하는 블루,혹시 18일 수업 영상은 언제 업로드될까요?
3/19/24 23:03,비옷입은 튜브,15일 영상이 18일 영상인 거 같은데요?
3/19/24 23:04,비옷입은 튜브,15일로 들어가면 18일게 나오는 거 같아요
3/19/24 23:07,DL하는 블루,오 감사합니다!
3/20/24 1:21,멋쩍은 튜브,사진
3/20/24 1:22,멋쩍은 튜브,혹시 문제 1번에서 저거는 1개의 1*64*64 크기의 레이어가 3개 있어야 한다는 의미인가요?
3/20/24 1:28,멋쩍은 튜브,오 제가 CNN이랑 헷갈린 것같습니다 늦은 밤에 죄송합니다
3/20/24 7:31,눈물바다에 빠진 라이언,"혹시 Problem 5.(c)에서 'represent identical mappings'라는 것이 각 y_l이 값으로 같은 것을 말하는 것인지, 또는 sigmoid와 tanh를 사용했을 때 y_l을 계산하는 과정(구조)가 같은 것인지 여쭤봐도 될까요?"
3/20/24 9:46,기타치는 튜브,최종 y_L이 같도록 할 수 있음을 보이라는 것 같아요
3/20/24 10:13,눈빛 애교 어피치,"1, 2 번에서 test data로는 무엇을 해야하나요? 문제에서 명시한 부분이 없는데 마음대로 하면 되나요?"
3/20/24 11:06,하트뽀뽀 어피치,과제2 성적 입력된건가요??
3/20/24 11:07,화나서 방방 뛰는 튜브,저는 안보여요
3/20/24 11:07,화나서 방방 뛰는 튜브,근데 HW2P4가 누락된것같습니다
3/20/24 11:07,애교뿜뿜 무지,그런 것으로 보입니다만 아직 공개는 안 된 것으로 보입니다
3/20/24 11:07,애교뿜뿜 무지,아직 생성 중인것 같은데 기다려봅시다!~
3/20/24 13:34,TA Gyeongmin,HW2 grading is completed. Please check your eTL.
3/20/24 13:37,멋쩍은 튜브,혹시 hw2에서 어느 부분이 틀렸는지는 어떻게 알 수 있나요?
3/20/24 13:38,애교뿜뿜 무지,각 문항별 점수 보시면 될 것 같습니다
3/20/24 13:38,열심히 일하는 네오,HW2P1~P7 눌러보시면 돼요
3/20/24 13:39,멋쩍은 튜브,아 ㅠㅠ 네 몇번인지는 아는데 감점된 이유가 궁금해서요!
3/20/24 13:40,멋쩍은 튜브,Etl에 주석이나 그런 것이 없길래 그냥 해설을 보고 깨달아야? 하는 건지 궁금했습니다
3/20/24 13:40,화난 라이언,HW2 Graded 공지 아래에 Common mistakes 먼저 확인해보시고 그래도 모르시겠으면 조교님께 메일 보내보시는 게 어떨까요
3/20/24 13:47,TA Gyeongmin,"세부적으로 어떤 부분이 틀렸는지 궁금하시면 공지에 있는 메일로 연락주시면 감사하겠습니다.

If you wonder why you were deducted, please email us as announced on eTL."
3/20/24 14:09,멋쩍은 튜브,넵 메일드렸습니다 조언 주신 분들 다들 감사합니다!
3/20/24 17:04,애교뿜뿜 무지,지금 줌 잘 들리나요?
3/20/24 17:04,눈물바다에 빠진 라이언,넵
3/20/24 20:57,시무룩한 튜브,hw2 솔루션은 아직 공개 안된 상태인가요>
3/20/24 20:59,General Trash,.
3/20/24 22:59,시무룩한 튜브,감사합니다
3/20/24 23:51,건배하는 프로도,과제3 제출란은 아직 안생긴건가요?
3/21/24 9:08,기지개 고양이,이제 생겼네요
3/21/24 9:14,눈물바다에 빠진 라이언,"뭔가 답에 관련있을 것 같아서 안 여쭤보고 고민해보려 했는데 의문이 안 풀려서 질문드립니다. Problem 5.(c)에서 각 y_1,...,y_L이 같도록 C,d를 찾아야 하는 건가요, 아니면 최종 y_L만 같도록 하면 되나요? 양쪽에서 같은 y_1,...y_L notation을 사용하고 있어서 전자로 생각했었는데, sigmoid는 R->(0,1)인 함수고 tanh는 R->(-1,1)인 함수라서 불가능하다고 생각이 들었습니다.."
3/21/24 9:21,눈빛 애교 어피치,x->y_L  로 가는 게 identical하다고 해서 저는 y_L만 같도록 했습니다. y_1부터 y_L-1까지 같도록 만들기 힘들어 보이기도 했고요..
3/21/24 9:25,눈물바다에 빠진 라이언,넵 감사합니다.. Notation 때문에 오해했나 보네요 ㅜ
3/21/24 10:22,비옷입은 튜브,ch3 code 의 Data Augmentation의 첫번째 transform에 있는 transforms.RandomCrop(32) 에서 CIFAR 데이터는 원래 32*32인데 pad(4) 된 40*40 에서 32*32로 자른다는 말인가요?
3/21/24 12:20,TA Hyojun,넵 맞습니다. https://blog.joonas.io/193 도 참고하시면 도움이 될 듯 합니다.
3/21/24 13:29,TA chaeju,HW1 redo is graded. please check the Etl announcement.
3/21/24 13:45,멋쩍은 튜브,"제가 조금 헷갈려서 그러는데, ch3 1번에서 optimizer 사용 가능하고 lr = 0.1 로 하면 되는 것이 맞나요?"
3/21/24 13:50,TA chaeju,네 그렇습니다.
3/21/24 14:12,TA chaeju,We have posted an announcement regarding late submission of assignments. please make sure to read it.
3/21/24 14:42,nnd,nnd님이 들어왔습니다.
3/21/24 15:28,옐로카드 프로도,"혹시 Question 1,2 가 다들 fit 되시나요? 계속 해도 일직선만 나와서 한번 여쭤봐요."
3/21/24 15:29,기타치는 튜브,삭제된 메시지입니다.
3/21/24 15:29,기타치는 튜브,삭제된 메시지입니다.
3/21/24 15:29,신난 어피치,네 잘 나오는 것 같습니다
3/21/24 15:30,옐로카드 프로도,감사합니다~
3/21/24 15:31,신난 어피치,혹시 nonlinear layer 잘 넣으셨나요??
3/21/24 15:31,옐로카드 프로도,sigmoid로 다 넣었습니다
3/21/24 15:42,ㅈㅅㅇ,파라미터 초기화 하셨어요?
3/21/24 15:43,옐로카드 프로도,derivative 말씀하시는건가요??
3/21/24 15:43,옐로카드 프로도,zero_grad
3/21/24 15:44,ㅈㅅㅇ,"아뇨 과제 힌트에 For initialization, ... 부분이요"
3/21/24 15:44,ㅈㅅㅇ,그거 안 하면 처음 선에 그냥 멈춰있어요
3/21/24 15:45,옐로카드 프로도,그 코드가 주어져 있지 않나요? 
3/21/24 15:45,옐로카드 프로도,아마 되있을겁니다
3/21/24 15:45,멋쩍은 튜브,ㅠㅠ저도요
3/21/24 15:46,멋쩍은 튜브,이거 했는데도 그러는데 해결되신분들 조언 주시면 감사히 받겠슴니다...
3/21/24 15:47,옐로카드 프로도,사실 일직선이라기보다도
3/21/24 15:48,옐로카드 프로도,뭔가 y=0을 학습하려는 느낌입니다
3/21/24 15:48,옐로카드 프로도,y=c네요
3/21/24 16:15,멋쩍은 튜브,사진
3/21/24 16:15,멋쩍은 튜브,저도네요
3/21/24 16:15,멋쩍은 튜브,음 뭐가 문젠지 봐야겠어요
3/21/24 16:16,눈물바다에 빠진 라이언,제가 저번에 실수로 iteration마다 seed를 고정시켰을 때랑 비슷하게 나오는 것 같은데
3/21/24 16:17,눈물바다에 빠진 라이언,혹시 data point 고를 때 랜덤으로 뽑히는지 살펴보세요
3/21/24 16:18,늦게 일어나는 새,그래프를 보니 마지막 layer에도 sigmoid를 적용한 것 같네요. 마지막 layer는 linear layer만 사용해보세요
3/21/24 16:20,옐로카드 프로도,혹시 마지막에는 비선형함수를 안쓰는 이유를 알 수 있을까요..? 들었었던 것 같은데 정확히 기억이 안나네요..
3/21/24 16:21,손을 번쩍 든 무지,그냥 관행이라고 들었던것같습니다
3/21/24 16:22,눈물바다에 빠진 라이언,Sigmoid나 tanh 같은 함수를 쓰면 치역이 제한되어서 그런 거 아닌가요?
3/21/24 16:22,눈물바다에 빠진 라이언,그래서 여기도 trained fn 값이 0과 1 사이에만 있고요
3/21/24 16:22,옐로카드 프로도,그러네요
3/21/24 16:22,손을 번쩍 든 무지,아 그렇네요
3/21/24 16:24,옐로카드 프로도,근데 아쉽게도 gradient가 inf로 가네요.. 다른 layer관해서도 비선형함수 유무로 경우 나눠서 해봤는데 발산합니다..
3/21/24 16:30,으쓱으쓱 어피치,"20일 강의에서 Data augmentation을 제가 이해한 바로는, 원래 데이터에 변형을 가하는 것인데, 그럼 이름의 직관과 다르게, train data의 수는 변하지 않고 transform만 해주는 것이 맞나요?"
3/21/24 16:33,열심히 일하는 네오,"훈련 데이터 자체를 transform 시켜서 물리적으로 적재(?)하는 것이 아니라, 늘린 epoch 마다 transform한 데이터셋을 써서 훈련시키는 걸로 알고 있어요 "
3/21/24 16:33,신난 어피치,삭제된 메시지입니다.
3/21/24 16:41,멋쩍은 튜브,사진
3/21/24 16:41,멋쩍은 튜브,다들 이정도 나오시나요!
3/21/24 16:41,멋쩍은 튜브,저는 1. 말해주신 대로 마지막 레이어 시그모이드 제거 2. nn.MSELoss에서 input reshape해줘서 사이즈 맞춰주기로 해결했습니다 아직 막히신 분들을 위해서 올립니다
3/21/24 16:42,으쓱으쓱 어피치,"Epoch의 수가 늘어나는 방식으로 데이터가 증가하는 건가요?? custorm dataset tutorial들을 찾아보니, sample에 transform을 가하는 부분만 있고, 데이터 수를 키우는 부분은 찾지 못해서요"
3/21/24 16:42,으쓱으쓱 어피치,사진
3/21/24 16:42,으쓱으쓱 어피치,https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html
3/21/24 16:42,멋쩍은 튜브,제가 전에 수업을 다른 곳에서 들었을 때는 기존 데이터 + 변형한 데이터를 새로운 데이터셋으로 해서 사용하는 것도 본 것 같아요
3/21/24 16:51,열심히 일하는 네오,"데이터의 수를 키울 필요가 없는 게, 각 epoch마다 데이터로더가 생성하는 미니배치에 기존 데이터셋 + transform된 데이터셋이 포함되어 있지 않을까요? 

예를 들어, 첫번째 에포크에서는 데이터로더가 불러온 기존 데이터셋을 쓰지만 그 이후의 에포크에서는 transform 데이터셋에서 나온 미니배치를 사용하는 거죠.

저는 이렇게 이해하고 있었는데, 틀린 부분이 있을수도 있습니다 ㅎㅎ.."
3/21/24 16:54,옐로카드 프로도,input이 label이랑 이미 shape가 같지 않나요??
3/21/24 16:55,으쓱으쓱 어피치,"GPT한테 물어보니 ㅎㅎ,, Dataset class의 transform 자체는 data 수를 바꾸지 않고 (말 그대로, load한 data에 transform만 수행), 만약 원본 데이터와 transformed data를 모두 학습에 사용하고 싶다면, 아래와 같이 custom dataset에서 index마다 확인하면서 뱉어줘야한다고 하네요"
3/21/24 16:55,으쓱으쓱 어피치,사진
3/21/24 16:58,멋쩍은 튜브,앗 제가 프린트해보니 하나는 1d/하나는 2d였어용
3/21/24 16:58,멋쩍은 튜브,"제가 멀 다르게 한 걸수도 있는데 하나가 [1, 2, 3...] 이라면 다른 하나는 [[1], [2],...] 로 되어 있어서 저는 그 부분 조정했습니다"
3/21/24 17:34,치즈케이크,"Epoch의 수와 관계없이, 학습용 데이터를 선택할 수 있는 가짓수를 늘리는 거라고 보시면 될 것 같습니다"
3/21/24 17:36,옐로카드 프로도,아 한번 체크해보겠습니다! 감사해요
3/22/24 0:35,Ernest Lee,사진
3/22/24 0:36,Ernest Lee,1번 저는 이 정도 나옵니다… 근데 test dataset이 필요없지 않나요?
3/22/24 0:36,옐로카드 프로도,문제 푸는데는 상관없습니다
3/22/24 0:36,Ernest Lee,그렇군요
3/22/24 0:36,Ernest Lee,이모티콘
3/22/24 0:40,멋쩍은 튜브,어라
3/22/24 0:40,멋쩍은 튜브,ㅠㅠ
3/22/24 0:40,멋쩍은 튜브,좀다르네요
3/22/24 1:07,멋쩍은 튜브,앗 해결이용...ㅎ
3/22/24 1:07,멋쩍은 튜브,"되게 신기한게 loss를 반으로 나눠주는 걸 잊어서 뒤늦게 나눠 주었더니 그래프 모양이 되게 다르게 나왔는데, 무슨 이유일까요?"
3/22/24 1:07,멋쩍은 튜브,그냥 상수배가 된 건데 그래프 개형 자체가 확 달라지는 게 신기해서요
3/22/24 1:12,기타치는 튜브,어떻게 다르게 나오셨는데요?
3/22/24 1:13,멋쩍은 튜브,사진 2장
3/22/24 1:13,멋쩍은 튜브,왼-> 오가 되었어요
3/22/24 1:21,화나서 방방 뛰는 튜브,Loss를 반으로 나누면 gradient도 반으로 줄어들어서 덜 학습한 효과가 나는 것 아닐까요?
3/22/24 1:22,청소하는 튜브,윗분 말이 맞는 거 같네요
3/22/24 1:23,기타치는 튜브,왼 --> 오 면 덜 학습된 게 아니라 더 잘 된 거 아닌가요?
3/22/24 1:23,화나서 방방 뛰는 튜브,그렇네요 잘못 읽었습니다
3/22/24 1:30,초롱초롱 어피치,lr을 상수배하는것과 같은 효과가 있습니다
3/22/24 13:15,Apeach enjoys music,사진
3/22/24 13:17,Apeach enjoys music,chapter2 code에서 cifar10 with multilayer perceptron 부분의 57번째 줄부터 나오는 모델의 forward 부분인데 제가 수업 때 이해하기로는 linear layer에 들어가려면 input인 x가 1차원이 되어야하는데 여기서는 x가 2차원인거 같아서 이게 어떻게 가능한건지 궁금하여 질문드립니다!
3/22/24 13:20,초롱초롱 어피치,1차원일 필요는 없고 마지막 차원이 layer의 input dim과 같으면 됩니다
3/22/24 13:21,초롱초롱 어피치,https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
3/22/24 13:21,초롱초롱 어피치,공식문서를 참조해주세요
3/22/24 14:47,눈물 흘리는 제이지,[0 -1 0 ; 0 0 0 ; 0 1 0] 인 filter 랑 [1 2 3; 4 5 6; 7 8 9] 인 행렬의 convolution은 8-2 인지 4-6 인지 했갈리는데 알려주실수 있나요?
3/22/24 16:23,Apeach enjoys music,정말감사합니다!
3/22/24 16:25,멋쩍은 튜브,Convolution하면 아마 뒤집어서 곱할 거 같은데 그러면 -1이랑 8이 곱해지지 않을까 싶어요
3/22/24 16:26,멋쩍은 튜브,사실 저도 확실한건 아닌데... correlation이 모양 그대로 곱하는 거고 convolution이 필터를 뒤집어서 곱하는 거라고 (그래야 좋은 성질들을 더 만족함) 다른 수업에서 들은 기억이 있어서 그렇게 생각했어요
3/22/24 16:29,일하기 싫은 네오,"수학적인 convolution의 정의는 말씀하신대로 서로 역순으로 곱하는 것이나
CNN에서는 필터가 포개어지는 위치의 값 그대로 곱합니다. "
3/22/24 16:31,청소하는 튜브,여기서 만족하는 성질이 commutativeness라서 큰 이미지 같은 거를 여러 필터에 씌워야할때 상대적으로 작은 필터끼리 먼저 연산을 하고 이미지와의 행렬곱을 가장 마지막에 해도 결과가 같아서 많이 사용되는 걸로 알고 있습니다
3/22/24 16:32,멋쩍은 튜브,헉 왜죠??
3/22/24 16:33,멋쩍은 튜브,애초에 필터를 원래 필터를 뒤집는 거로 정의하는 건가요 아니면 그냥 그런 건가요...저는 돌려서 적용한 적 있는 거 같아서 (기억 확실하지 않아요) 헷갈리내ㅛ
3/22/24 16:36,멋쩍은 튜브,"그리고 갑자기 궁금해진 건데
1000* 1000 이미지에 3*3 필터를 씌우면 대충 998*998 이미지가 (패딩 없이) 나올 거 같은데 반대로 교환법칙을 써서 3*3 필터에 1000*1000 이미지를 씌우면 저 크기가 안 나올 거 같은데 제가 어디서 잘못 생각한 것인지 궁금합니다. 사이즈가 같아야 하나요?"
3/22/24 16:39,벌 서는 라이언,삭제된 메시지입니다.
3/22/24 16:39,A+ is All you need,신호및시스템에서랑 cnn에서 convolution정의가 달라요
3/22/24 16:42,일하기 싫은 네오,파라미터를 학습한다는 관점에서 원본 이미지와 필터 내 개별 값의 매핑은 영향이 없는 것으로 알고 있습니다
3/22/24 16:42,벌 서는 라이언,아 죄송합니다... 강의에서 cnn을 예시로 들어주셔서 헷갈렸습니다.
3/22/24 16:44,초롱초롱 어피치,삭제된 메시지입니다.
3/22/24 16:44,초롱초롱 어피치,삭제된 메시지입니다.
3/22/24 16:48,초롱초롱 어피치,사진
3/22/24 16:48,초롱초롱 어피치,pytorch implementation 상에서는 kernel이 반전 없이 input과 elementwise로 곱해집니다
3/22/24 16:49,초롱초롱 어피치,반전된 kernel로 학습하는 것이 딥러닝 상에서는 동치이기 때문에 굳이 반전하지 않는것으로 생각됩니다
3/22/24 18:37,쑥스럽게 인사하는 프로도,사진
3/22/24 18:38,쑥스럽게 인사하는 프로도,6번 문제에서 이 기호가 무슨 의미인가요?
3/22/24 18:43,기지개 고양이,무슨 기호요
3/22/24 18:44,기지개 고양이,∈
3/22/24 18:46,쑥스럽게 인사하는 프로도,그 오른쪽이요! Y 비슷하게 생긴거요
3/22/24 18:46,초롱초롱 어피치,Yn들이 속하는 집합입니다
3/22/24 18:48,쑥스럽게 인사하는 프로도,아아 그 이상의 의미가 있는건 아닌거군요 감사합니다
3/22/24 18:51,건방진 제이지,5(b)에서 Lipschitz continuous 에 대한 수업자료(위)와 위키백과(아래)의 정의가 다른데 lipschitz continuous 임을 보일 때는 수업자료에 나온 정의를 이용하는 것이 맞을까요?
3/22/24 18:51,건방진 제이지,사진
3/22/24 18:55,춤추는 튜브,제 기억으로는 수업자료의 저 식은 grad f가 Lipschitz continuous 하다는 의미였던 걸로 기억합니다
3/22/24 18:55,건방진 제이지,아하 자세히 보니 그렇군요
3/22/24 18:55,건방진 제이지,그럼 똑같은 정의네요 감사합니다!!
3/22/24 20:46,인사하는 제이지,사진
3/22/24 20:46,인사하는 제이지,1번 코드에서 x_train이랑 x_val로 나누어져 있는 이유가 무엇인가요오..?
3/22/24 20:48,밥줘,모델 만들고 따로 validation 하기 위한 데이터셋인데 이 문제에선 굳이 안 쓰는 것 같아요 아님말구요
3/23/24 0:22,인사하는 제이지,혹시 이거 왜 반으로 나눠야하나요..?
3/23/24 0:23,인사하는 제이지,저도 반으로 나누니까 딱 깔끔해지긴 하네요
3/23/24 0:42,TA Hyojun,alpha=0.1은 문제에서 주어진 형태의 loss function에 대해 잘 작동하는 lr값이라 loss를 반으로 나눠주지 않으면 training이 잘 되지 않는 거 같습니다.
3/23/24 0:44,인사하는 제이지,앗 식에 1/2가 있군요!
3/23/24 0:44,인사하는 제이지,감사합니당
3/23/24 14:13,소심한 네오,1번 문제에서 shuffled cyclic SGD 이용하라고 되어있는데 그러면 shuffled를 위해 dataloader에서 shuffle=True로 설정해도 되는건가요?
3/23/24 14:28,청소하는 튜브,저도 그렇게 햇어요!
3/23/24 14:32,소심한 네오,엇 감사합니다!
3/23/24 16:24,화난 라이언,교수님 웹사이트에 MFDNN Programming prerequisites tutorial code가 접근이 안되네요
3/23/24 16:42,TA Hyojun,혹시 Lecture material에 있는 tutorial code도 접근이 안 되시나요?
3/23/24 16:42,화난 라이언,아니요! 그건 잘 되네요
3/23/24 16:43,TA Hyojun,아하 둘이 같은 파일이라 해당 코드 참고하시면 될 것 같습니다.
3/23/24 16:44,화난 라이언,아 그렇군요 감사합니다~
3/23/24 19:19,으쓱으쓱 어피치,"1단계: overfit
모델의 복잡도가 낮아, spurious correlation은 커녕 key feature들도 학습하지 못함
-> 복잡도가 올라가면서, key feature들 학습. 성능 상승
-> (1단계 내에서는) 과도하게 복잡도가 올라가, spurious correlation도 학습. 성능 하락(peak 근접)

2단계: second descent
복잡도(모델의 지능을 떠올렸습니다)가 특정 임계치 이상 높아지면, key feature과 spurious feature를 구분할 정도가 됨. 그 전까지는 spurious feature도 진실이라고 믿고 학습했는데, 그걸 구분하기에 성능 상승"
3/23/24 19:19,으쓱으쓱 어피치,"3.20강의의 Double descent에 대한 직관을 나름대로 생각해봤는데, 이런식의 이해는 문제소지가 다분할까요? 😅

사람도 어려운 내용을 학습할 때, 일정 단계까지는 길을 못잡아서 학습할수록(~=지능이 높아질수록) 헛똑똑이(?)가 되지만, 내용을 일정 수준이상 소화한 후에는 공부할 수록 추가적인 통찰을 얻는다는 느낌으로 생각해보았습니다"
3/23/24 19:20,으쓱으쓱 어피치,수학 수업인데 좀 뜬구름 잡는 것 같은 질문 죄송합니다..
3/23/24 19:28,으쓱으쓱 어피치,"특정 복잡도 이상 높아지면, overfitting을 만들지 않고도(dataset의 feature들을 잘 구분하게 됨) trainset을 소화하는 objective function을 구성할 수 있게 되는 것이고,

그 전까진 파라미터 수의 부족 문제로 그 function을 만들지 못해, trainset을 소화하는 방법이 function을 trainset에 overfit(test를 포기)밖에 없다라고 말할 수도 있을 것 같습니다"
3/23/24 19:43,옐로카드 프로도,"제가 보기에는 맞는 것 같습니다!
헌데 개인적인 생각이지만, 보다 엄밀한 이유를 제시하지 못한다면, 어떻게 이를 쉽게 설명하려 시도하던간에, 그저 정성적인 분석일뿐이라고 생각했습니다. 그 의미 이상이하도 아닌 것 같아요. 물론 정확한 이유를 몰라도 잘 활용하고 있으니 이 논리가 유효할지 모르겠습니다!
여러분들과 조교님께서는 이러한 시각에 대해 어떻게 생각하시는지 궁금합니다!
추가적으로 이런 현상들을 어떠한 방법이 되었든간에 일단 설명하고자 하는 연구가 있는지 궁금하기도 합니다!"
3/23/24 20:03,경제학도,Hw3의 problem3에서 log 내부 분수의 형태가 벡터 나누기 벡터인 것 같은데 벡터의 나눗셈은 정의가 불가능한 것으로 알고 있습니다. 그렇다면 혹시 프로그래밍 언어에서 말하는 벡터의 나눗셈을 말하는 것일까요? 즉 각 벡터의 같은 자리 원소끼리 나눗셈을 하는 것일까요?
3/23/24 20:03,벙찐 튜브,f_j는 f의 j번째 항입니다
3/23/24 20:10,경제학도,감사합니다!
3/23/24 20:15,N이 되고픈 엡실론,혹시 문제 1번에서 loss가 nan 나오는 문제가 있는데 Gaussian의 표준편차를 0.1로 줄이면 해결되는 것 같은데...다른분들도 이 문제 경험하신 분 계신가요 ㅠ
3/23/24 20:19,화난 라이언,저는 sigmoid 안 넣었을 때 발산했었어요
3/23/24 20:20,N이 되고픈 엡실론,앗 감사합니다...!
3/23/24 20:25,화난 라이언,에포크랑 forward에서 x 다 출력하면서 넘어가면 관찰하기 좋은 것 같습니다 저는 이렇게 찾았어요 아주 기본적인 실수였지만요..
3/23/24 20:26,얼굴마사지하는 제이지,"5(c)에 identical x->y_L mapping이라는 게 y_1, ..., y_L이 전부 같을 필요는 없고 y_L만 같으면 된다는 뜻인가요?"
3/23/24 20:28,멋쟁이 프로도,네 애초에 활성함수 치역이 달라서 중간단계에서 같을 수는 없을거같아요
3/23/24 20:28,얼굴마사지하는 제이지,앗 그렇군요! 감사합니다.
3/23/24 21:02,베개를 부비적대는 라이언,Problem 4에서 differentiability는 다음 그림에서 DL의 경우를 따른다고 이해하면 될까요?
3/23/24 21:02,베개를 부비적대는 라이언,사진
3/23/24 21:26,기지개 고양이,Loss func.에 1/N 해주니까 해결되더라고요
3/23/24 21:27,기지개 고양이,그거 안하니까 weight가 계속 커지다 float 최대값 도달학서 nan으로 바뀌더라고요
3/23/24 21:28,N이 되고픈 엡실론,감사합니다!
3/23/24 21:37,열심히 일하는 네오,loss func에 MSELoss()쓰면 안에 1/N들어가있지않나요?
3/23/24 21:37,N이 되고픈 엡실론,1/2을 의도하신거 같네요
3/23/24 21:38,열심히 일하는 네오,아 그렇군요
3/23/24 21:41,기지개 고양이,그거 그냥 쓰면
3/23/24 21:41,기지개 고양이,"SGD로 하면 한번에 X, Y 한쌍씩만 계산돼서"
3/23/24 21:42,기지개 고양이,/1 되지 않나요
3/23/24 21:46,열심히 일하는 네오,그렇긴 하네요 근데 전 웃긴게 위에서 1/2이라던가 1/N안해도 제대로 fitting이되던데 뭔가 잘못된걸수도 있겠네요
3/23/24 21:50,기지개 고양이,그러게요 잘 fitting됐다면 문제없지 않을까요
3/23/24 21:51,기지개 고양이,어쨌든 전 nan 나오는 문제를 저리 하니 해결됐습니다
3/23/24 21:54,열심히 일하는 네오,알겠습니다 감사합니다
3/23/24 22:01,벙찐 튜브,Problem 6과 7에서 j-th ReLU output은 무엇을 의미하는 것인가요?
3/23/24 22:02,기지개 고양이,"aX+b가 애초에 열벡터입니다.
σ(aX+b)의 j번째 성분을 의미하는것 같습니다."
3/23/24 22:03,벙찐 튜브,감사합니다
3/23/24 22:49,치맥하는 제이지,HW3 #3에서는 k=1인 경우는 모델이 아무것도 분류하지 않게 되니까 고려하지 않아도 되는건가요?
3/23/24 22:54,기뻐하는 라이언,참고하시면 될 것 같습니다
3/23/24 23:59,눈물 흘리는 제이지,filter의 크기가 1*1인 2d convolution은 그냥 nn.linear로 계산되는 연산과 동일하다고 볼수 있을까요?
3/24/24 0:28,김민우,p개의 input과 p개의 output이 있을 때 nn.linear은 p*p matrix로 각 output이 input의 모든 component의 weighted sum으로 나오고 1-filter conv2d는 weight가 1-dim이고 n번째 output element는 n번째 input element에만 의존하기 때문에 다른 layer입니다
3/24/24 2:11,렐?루,"Non-linear activation function에서 여러 층을 쌓기 위해선 Non-linear이여야 하는것 까지는 이해했는데, leakly-ReLU에서 굉장히 작은 alpha 값을 가지는 이유가 있을까요?
Sigmoid 함수는 잘 안 쓰인다고는 했지만 x<0에서 유의미한 기울기를 가지고 있는데, ReLU/leakly-ReLU 는 x<0을 죽여버리는 이유가 궁금합니다"
3/24/24 2:20,초롱초롱 어피치,"이론적으로는 non polynomial 이기만 하면 임의의 연속함수를 근사할 수 있습니다 (정확히는 uniform convergence on compact sets)
relu나 leaky relu가 sigmoid에 비해 가지는 이점은 연산량이 압도적으로 적고 x가 클때 gradient가 vanish하지 않는다는 것입니다."
3/24/24 2:28,렐?루,"선형이 아니기 위해서는 alpha 값이 1/2과 같아도 괜찮을 것 같은데, alpha를 작은 양수로 잡는 이론적인 이유가 있을까요?"
3/24/24 2:34,초롱초롱 어피치,"x<0일 때 relu에서 gradient가 죽어버리는 문제를 해결하기 위해, relu와 비슷하지만 x<0일때도 작은 gradient가 있도록 정의한 함수가 leaky relu라고 생각하시면 되겠습니다"
3/24/24 2:59,부탁하는 네오,공식문서 보니까 파라미터 아무것도 안 넣어주면 평균을 계산해주는(1/N) 게 맞습니다. 식에 있는 1/2를 표현하려면 그냥 MSELoss()/2 해주면 되는 것 같구요.
3/24/24 9:36,기지개 고양이,"그러니까 MSELoss() 함수는, 입력받는 변수의 크기를 N으로 잡고 그걸로 나눠주는건데"
3/24/24 9:38,기지개 고양이,SGD로 하면 한번에 한쌍씩 (하더라도 하나의 batch 크기로) 하는건데 그걸 데이터셋 크기 N으로 나눈거랑은 다른 의미라고 생각합시다
3/24/24 9:38,기지개 고양이,*합니다
3/24/24 11:28,튜브낀 튜브,혹시 이번 과제 하려면 강의 어디까지 봐야 하나요?
3/24/24 12:08,열심히 일하는 네오,저번주 강의정도로 충분합니다ㅎㅎ
3/24/24 12:09,튜브낀 튜브,감사합니다 ㅠㅠ
3/24/24 12:11,기지개 고양이,???: 치타는 웃고있다....
3/24/24 16:25,ㅇㅇ,Is there an optimal/widely practiced method of finding the optimal weight decay parameter? I think cross validation like classical ridge should be best but was wondering if there was a computationally less expensive way.
3/24/24 16:40,Ernest Lee,mseloss에는 원래 1/n 이 있지 않나요?
3/24/24 17:47,Ernest Lee,"7번 근데 vanish 될 수도 있지 않나요 ..? uj = 0 이면 될 수도 있을 거 같은데, typically 한 case만 고려하면 될까요 조교님?"
3/24/24 17:48,Ernest Lee,아 아니면 uj 가 계속 업데이트되니까 그냥 무시해도 되려나요
3/24/24 17:49,치즈케이크,Random하게 weight를 고르고 학습하는 과정에서 u_j가 정확히 0이 되는 것이 확률 0이라고 썼습니다
3/24/24 17:49,치즈케이크,HW1의 3번처럼..
3/24/24 17:50,말썽쟁이 네오,계속 0만 나오는게 vanish 같아요
3/24/24 17:50,말썽쟁이 네오,똑같은 값
3/24/24 17:50,벙찐 튜브,정확히 0이 되더라도 그 순간에만 vanish되지 않나요?
3/24/24 17:51,Ernest Lee,그런 거 같아요
3/24/24 17:51,Ernest Lee,모든 iteration에서 기적적으로 vanish 가
3/24/24 17:51,Ernest Lee,될 수도 있을 수도 있다고 생각해서 … 그런데 
3/24/24 17:51,Ernest Lee,안 될 거 같긴 해요 ㅋㅋㅋ
3/24/24 17:53,머리 빗는 네오,"과제 2번을 읽으면서 궁금한게 생겨서 질문을 드립니다! overfit하다는것이 어떤 상황을 의미하는지, 그리고 노이즈가 추가가 되어도 학습이 잘 된다는 것은 overfit하지 않음을 의미하는지 질문드립니다!"
3/24/24 17:54,Ernest Lee,overfit한다는 게 … label noise가 들어가면 그 label noise에 맞게 그래프가 완전 복잡하게 나와야 하는데
3/24/24 17:54,Ernest Lee,1번처럼 나오니 overfit하지 않다는 겁니다
3/24/24 18:03,머리 빗는 네오,"아 overfit하게 될경우에는 노이즈에 영향을 많이 받아 결과가 이상해 지는 경우를 이야기하는데, 실제 이 경우에서는 그렇지 않아서 overfit하지 않음을 확인할 수 있다는 뜻이군요. "
3/24/24 18:03,Ernest Lee,넹
3/24/24 18:04,머리 빗는 네오,넵 감사합니다!
3/24/24 21:40,손을 번쩍 든 무지,5-(c)에 관련해서 질문드립니다. 윗분들 말씀들어보니 y_L만 똑같이 설정해주면 된다고 하셨는데 그럼 b_L과 d_L만 적절히 조절해주면 나머지 값에 상관없이 같아질수 있는거 아닌가요..? 문제 의도가 이게 맞는지 모르겠어서 질문드립니다ㅜㅜ
3/24/24 21:43,치즈케이크,그 전 y값들이 어떻게 쌓여오는지를 모르는 시점에서 마지막 상수만 딱 결정해준다고 y_L값이 같아질 거라고 생각하지 않습니다
3/24/24 21:44,손을 번쩍 든 무지,아 모든 x에 대해 성립해야하는군요..  감사합니다!!
3/24/24 22:58,전커서교수님이될래요,"6번 문제에서 ""dead"" output에 대해 설명할때 a_j와 b_j에 붙은 첨자 ^0은 무슨 의미인가요? "
3/24/24 22:58,전커서교수님이될래요,"신경쓰지 않고 a_j, b_j라고 생각해도 되나요?"
3/24/24 22:58,신난 어피치,초기값으로 이해했어요
3/24/24 22:59,전커서교수님이될래요,아 초기값으로 생각하면 말이 되네요. 감사합니다!
3/24/24 23:12,택배 상자를 든 네오,"안녕하세요! Chapter 2 강의슬라이드 22페이지에 두번째에서 세번째줄로 넘어가는 것 직접 해보신 분 있나요? 강의영상을 보면 앞에있는 엔트로피 식에 대입하면 나온다고 하셨는데 잘 안되네요..
질문이 살짝 뒤늦은 감이 있지만 궁금해서 질문 드립니다.."
3/24/24 23:12,택배 상자를 든 네오,사진
3/24/24 23:19,Joyful Apeach,"P(Y) = [1,0]인 경우와 [0,1]인 경우 각각 계산해보시면 됩니다!"
3/24/24 23:23,부탁하는 네오,5번에서 derivative가 lipschitz-continuous인걸 보이는건데 순수하게 부등식을 풀어야 하나요? 아니면 second derivative가 bounded되어있음을 보이면 될까요?
3/24/24 23:23,Ernest Hemingway,"In problem 5 (c), do I have to assume that ρ is applied element-wise? Or is it a consequence of σ being applicable element-wise? "
3/24/24 23:23,Ernest Hemingway,Aren't they equivalent? 
3/24/24 23:25,부탁하는 네오,동치인지는 잘 기억이 안 나는데 f' is bounded->f is lipschitz continuous는 맞습니다
3/24/24 23:25,부탁하는 네오,근데 이 정리를 그냥 증명 안 하고 사용해도 되는지 궁금했습니다
3/24/24 23:25,기타치는 튜브,평균값 정리 간단하게 한 줄만 적으면 되니까 그냥 썼습니다
3/24/24 23:26,Ernest Hemingway,<->
3/24/24 23:26,초롱초롱 어피치,미분가능하면 동치요
3/24/24 23:27,부탁하는 네오,아 그렇군요
3/24/24 23:27,부탁하는 네오,근데 이 상황에서는 동치까지는 필요없고 한방향만 있어도 되는 게 맞죠?
3/24/24 23:27,부탁하는 네오,어차피 relu 미분한거는 불연속이라 이야기가 다르고
3/24/24 23:34,택배 상자를 든 네오,아하 제가 헷갈렸었네요 감사합니다!!
3/24/24 23:46,치즈케이크,"Both σ and ρ denote activation functions, so IMO it would only make sense if the two acts similarly given a vector of some length."
3/24/24 23:49,부탁하는 네오,"5(c) 내용을 정리해보면
sigmoid함수를 활성화함수로 사용하는 모델의 A와 b가 주어진 상황에서, 그 모델과 동일한 input x에 대해서 동일한 output y_L을 뱉는 새로운 모델(\rho와 C, d를 이용해서)을 만들 수 있음을 보여라"
3/24/24 23:49,부탁하는 네오,이게 맞을까요?
3/24/24 23:49,치즈케이크,저도 그렇게 이해했습니다
3/25/24 0:32,부탁하는 네오,5(c)에서 explicit한 solution을 구해야 하는 걸까요??
3/25/24 0:33,벙찐 튜브,"저는 일단 C_k, D_k를 A_k, b_k로 적당히 표현해서 y_n이 같도록 setting을 하긴 했는데"
3/25/24 0:34,벙찐 튜브,다른 좋은 풀이가 있을 수도 있을 것 같아요
3/25/24 0:34,벙찐 튜브,*있을 것 같아요
3/25/24 0:36,눈빛 애교 어피치,수학적 귀납법으로 충분히 간단하게 표현되지 않나요
3/25/24 0:37,벙찐 튜브,네네 간단하긴 해요
3/25/24 0:38,벙찐 튜브,값을 구하는 방법 말고 적당한 설명으로 넘어가도 되나 해서요^^
3/25/24 1:14,부탁하는 네오,이게 되나요..?? sigmoid랑 tanh의 관계를 이용해서 요리조리 해봤는데 잘 안 되는 것 같아서요..
3/25/24 1:15,부탁하는 네오,지피티한테 물어봐도 자꾸 straightforward한 solution은 없다는 식으로 돌려서 얘기하네요
3/25/24 1:15,부탁하는 네오,귀납법 쓰려면 각 레이어마다 어떤 관계가 유지되어야 하는거같은데
3/25/24 1:15,부탁하는 네오,어렵네요
3/25/24 1:15,벙찐 튜브,"인터넷에 sigmoid, tanh relation?"
3/25/24 1:16,음료 마시는 어피치,"맞는지는 모르겠지만 c,d를 계수를 잘 조정하면 같게 할 수 있지 않나요...?"
3/25/24 1:16,인사하는 제이지,지피티는 저런거 못풀어요
3/25/24 1:16,벙찐 튜브,뭐 이런 식으로만 쳐봐도 관계식 나올거에요
3/25/24 1:16,인사하는 제이지,고급지피티면 풀수도
3/25/24 1:16,부탁하는 네오,관계식은 알고 그거 가지고 좀 조작해봤는데 좀 더 해봐야겠네요
3/25/24 1:16,부탁하는 네오,gpt4긴 합니다ㅠㅠ
3/25/24 1:17,부탁하는 네오,지피티는 코딩은 잘하는데 수학이랑 통계는 가끔 뻘소리를 하더라고요 ㅎㅎ
3/25/24 1:47,부탁하는 네오,대충 해결한거 같은데 index 1일때랑 아닐때랑 식이 다른 거 맞나요?
3/25/24 1:47,아침햇살,"2부터는 변수 조작해서 할 수 있는데 C1, d1은 애초에 시그마와 로의 치역이 다른데 y1에 따라서 정할 수 없는 경우가 생기는 게 아닌가요?"
3/25/24 1:47,부탁하는 네오,엇 똑같은 거 말씀해주셨네요
3/25/24 1:47,부탁하는 네오,2부터는 수학적귀납법 생각하면 잘 되는데
3/25/24 1:48,부탁하는 네오,1에서는 식을 조금 다르게 세팅해줘야 하는 것 같아요
3/25/24 1:48,부탁하는 네오,맞는진 모르겠지만
3/25/24 1:48,부탁하는 네오,그리고 2부터 마지막까지는 식 똑같습니다
3/25/24 1:48,부탁하는 네오,틀린거있으면 지적해주십쇼..
3/25/24 1:49,아침햇살,저는 index가 1일 때 함수의 치역이 다른데 값을 세팅한다고 해결할 수 있는 건지가 궁금했습니다
3/25/24 1:49,아침햇살,아 시그마의 치역을 로의 치역이 포함하니 식으로 어떻게 표현할 수는 없어도 그런 C와 d를 찾을 수는 있겠네요. 이해했습니다.
3/25/24 1:49,부탁하는 네오,"index i에 대해서 C_i와 d_i를 A_i와 b_i로 표현하는데, i>=2일때랑 i=1일때랑 식이 다르게 나옵니다"
3/25/24 1:54,아침햇살,음.. 저만 그런 건지는 모르겠는데 d1은 식이 많이 복잡하네요
3/25/24 2:21,멋쩍은 튜브,저도 대충 이런식으로
3/25/24 2:21,멋쩍은 튜브,할 수 있다 정도만 적었어요
3/25/24 10:05,머리 빗는 네오,"과제 1, 2번에서 loss graph 그려야 할까요?"
3/25/24 10:58,TA Jongchan,"아뇨, 그리지 않아도 괜찮습니다."
3/25/24 11:08,ㅇㅈ,ㅇㅈ님이 나갔습니다.
3/25/24 12:08,손을 번쩍 든 무지,제일 마지막도 다르지 않나요?
3/25/24 12:35,부탁하는 네오,네네 그런 ㄱ것 같아요
3/25/24 15:15,손을 번쩍 든 무지,감사합니다!
3/25/24 16:47,Ernest Lee,minibatch w/o replacement sgd 와 shuffled cyclic sgd의 차이점이 무엇인가요?
3/25/24 16:47,Ernest Lee,minibatch shuffled cyclic sgd요
3/25/24 17:04,TA Hyojun,"minibatch w/o replacement는 각 iteration마다 N개의 데이터 중 중복되지 않도록 B개를 뽑는 방식이고, minibatch shuffled cyclic sgd는 각 epoch마다 permutation을 만들어서(데이터를 shuffle하여) 순서대로 B개씩 뽑는다고 이해하시면 됩니다.
shuffled cyclic sgd는 각 epoch마다 데이터 전체를 사용하지만, minibatch w/o replacement는 데이터 전체를 사용하지 않을 수 있습니다."
3/25/24 17:05,부끄러워하는 라이언,"Hello, how many points are deducted if only ipynb code is submitted, without pdf file?"
3/25/24 17:05,부끄러워하는 라이언,Should I also submit pdf file although it is late submission?
3/25/24 17:10,.,.님이 들어왔습니다.
3/25/24 17:12,.,"Hi, while I turned in the rest of the assignment in time, the ipynb to pdf converter was acting up so I couldn't attach the PDF version of the run result for the coding assignment — can I turn it in separately through email?"
3/25/24 17:13,Ernest Lee,완벽히 이해했습니다 감사합니다!
3/25/24 17:16,Apeach enjoys music,사진
3/25/24 17:17,Apeach enjoys music,HW3 PB1에서 test loss를 그려보고 싶었는데 이런식으로 밖에 안나옵니다... 혹시 어떤게 문제일까요... gpt해보고 인터넷 찾아봐도 잘 모르겠어요..
3/25/24 17:17,열심히 일하는 네오,Test loss가 단 하나밖에 안 나와서 그래요
3/25/24 17:18,.,Model이 modify되고 있지 않으니까요
3/25/24 17:18,열심히 일하는 네오,Batch size를 보시면 test data loader에 들어가는게 단 하나의 batch만 들어갑니다
3/25/24 17:20,열심히 일하는 네오,그래서 loss도 값하나만 나오는거죠. 저도 비슷한걸 경험해서 뭔가했는데 batch size가 test data set크기보다 크더군요
3/25/24 17:21,ㅈㅅㅇ,test할 때는 iteration을 통해 모델이 트레이닝 하는게 아닙니다 그냥 train된 모델을 통과시켜서 loss 하나 딱 나와야 하는게 정상이에요
3/25/24 17:24,TA Hyojun,"As long as your answer is correct, I'm not going to deduct points for HW3. But please make sure you submit all your files next time."
3/25/24 17:24,.,"Okay, will do. Thanks a lot."
3/25/24 17:27,부끄러워하는 라이언,Thank you so much. I'll keep that in mind🙃
3/25/24 18:44,씩씩거리는 무지,HW4에 5번 질문드립니다. 이 문제는 직접 CE-loss 와 Non-CE loss를 코딩해서 비교하는 문제인가요? 아니면 이론적으로만 비교해 보는 문제인가요
3/25/24 18:46,TA chaeju,코딩으로 performance를 비교하는 문제입니다.
3/25/24 19:02,씩씩거리는 무지,알겠습니다! 감사합니다
3/25/24 19:05,애교뿜뿜 무지,"안녕하세요, 과제 3 하면서 생긴 질문이 두 가지 정도 있습니다!

Chap. 2 코드에서 logistic_loss함수를 대입하는 것과 비교했을 때 nn.MSELoss()를 대입하는 것에서 '()' 여부 차이는 어떤 의미를 갖는 것인지 궁금합니다!

또한 'CIFAR10 with Multilayer perceptron' 부분을 살펴보고 있었는데요, output = model(image)에서 output.item()이 어떤 역할을 하는 것인지 기억이 나지 않는데 알려주시면 감사하겠습니다."
3/25/24 19:30,화난 라이언,"nn.MSELoss()는 class instance를 선언하는 거고, logistic_loss는 함수입니다. 제가 언뜻 보기에는 새로운 instance를 만드는게 더 무거워보이기는 하는데, 혹시 torch.nn class가 backward 같은 method를 더 효율적으로 하는 기전이 있는지는 모르겠네요"
3/25/24 19:30,초롱초롱 튜브,"MSELoss는 클래스라서 생성 시 ()가 있어야 객체가 생성되고 그 이후에는 __call__메서드가 있어서 함수와 같은 형태로 활용될 수 있을 거에요

Output.item()은 만약 Output이 텐서라면 그 값을 내뱉어주는 메서드 입니다. 보통 계산 그래프에서 제외하여 값을 추출할 때 씁니다"
3/25/24 19:31,애교뿜뿜 무지,아하 __call__이 있군요~ 감사합니다!
3/25/24 19:31,초롱초롱 튜브,"효율을 제외하고서도 torch에서 제공하는 loss들은 클래스의 형태를 띄는데, 대표적으로 ce loss의 경우 weight를 지정하는 등의 활용에서 더 간편한 면이 있습니다"
3/25/24 19:31,초롱초롱 튜브,(객체로 쓸 때)
3/25/24 19:32,초롱초롱 튜브,물론 내부 코드를 뜯어보면 각 loss클래스에서 forward 콜이 오면 결국 loss값을 계산하는 함수를 호출합니다. Wrapper 같은 느낌이에요
3/25/24 19:34,애교뿜뿜 무지,이번 과제의 경우 Output이 텐서가 아니기 때문에 item을 잡으려고 하면 에러가 나는 거겠죠>
3/25/24 19:34,화난 라이언,"음 제가 torch docs를 좀 찾아보다보니까 말씀하신 class에서 내장된 메소드가 거의 functional 모듈 안에 구현이 되어 있는 것 같던데, 해당 torch.nn.functional을 불러왔을 때는 어떻게 동작하는지 아시나요?"
3/25/24 19:34,화난 라이언,만약 별 차이 없다면 그냥 functional에서 함수 받아다 쓰는게 나아보여서요
3/25/24 19:36,화난 라이언,아 만약 loss에서 공유하는 parameter 같은게 있으면 코드가 좀 더 깔끔해보이긴 할 것 같네요
3/25/24 19:36,초롱초롱 튜브,"동일하게 동작합니다. 대신 functional의 경우 연산마다 argument에 weight, reduction 등의 파라미터를 커스터마이징할 경우 따로 넣어줘야 하며 활용에 있어서 조금 덜 직관적일 수 있습니다."
3/25/24 23:04,열심히 일하는 네오,Pooling의 경우 Convolution처럼 필터 sliding하면서 겹치는 경우는 없다고 보나요? 
3/25/24 23:21,기타치는 튜브,항상 filter size = stride 되도록 잡는 것 같습니다
3/25/24 23:26,열심히 일하는 네오,감사합니다!
3/26/24 14:08,멋쩍은 튜브,저희 이번 과제 4-1의 경우에는 convolution에서의 그 필터 뒤집기? 없이 그냥 겹쳐진 모양대로 적용하는 게 맞는 거죠?
3/26/24 14:09,멋쩍은 튜브,수업 피피티는 그런 거 같은데 혹시나 해서요...!!
3/26/24 14:33,TA Hyojun,네 수업 피피티에 나온 정의대로 생각하시면 됩니다.
3/26/24 14:33,멋쩍은 튜브,감사합니다!
3/26/24 15:10,눈물 흘리는 제이지,이번 과제 5번은 어떤걸 보이라는 건가요? 수업시간에 했던 cross-entropy방식과 least square를 비교하는 문제인가요?
3/26/24 15:11,기타치는 튜브,.
3/26/24 16:24,손을 번쩍 든 무지,"문제 2번에서는 필터 ω의 사이즈가 명시돼 있지 않던데, 그냥 ω ∈ R^(1×k×k) with stride k라고 보면 되나요?"
3/26/24 16:27,A+ is All you need,사이즈 상관없지 않나요
3/26/24 16:28,손을 번쩍 든 무지,AvgPool2D는 사이즈 상관없는 건가요?
3/26/24 16:31,TA Hyojun,Avgpool2d operation with kernel size k를 convolution으로 표현할 수 있도록 convolution filter와 stride 등을 정하시면 됩니다. 
3/26/24 16:32,손을 번쩍 든 무지,넵 감사합니다!
3/26/24 17:05,으쓱으쓱 어피치,"HW4 P5에서 Hint로 나온 l(z, y) 유도해보신 분 계신가요? 식이 저 형태로 유도가 안되네요..  \sigma (z) + \sigma (-z) = 1 관계에서 Hint로 준 식도 더 정리될 여지가 있어보이고요"
3/26/24 17:17,신난 어피치,저는 저 형태로 유도 됐는데 y 케이스 나눠서 해보세요
3/26/24 17:17,멋쩍은 튜브,Y가 -1 이면 앞에거 발동 1이면 뒤에거 발동이에요
3/26/24 17:18,멋쩍은 튜브,"아마 l(z,y)를 반으로 나눠야 sum of square loss가 최종으로 나올 듯 하네요 1/N term은 고려 안한 식 같아서요"
3/26/24 17:19,신난 어피치,"아 네 l(z, y)의 평균을 loss function 으로 쓰면 되는듯해요"
3/26/24 17:34,으쓱으쓱 어피치,"아 전 P(y)를 일반화된 식으로 두고 했었는데, 케이스 나눠서 해보겠습니다!"
3/26/24 18:46,부탁하는 무지,pooling은 항상 stride = filter size 인가요?
3/26/24 18:48,손을 번쩍 든 무지,.
3/26/24 19:28,손을 번쩍 든 무지,사진
3/26/24 19:28,손을 번쩍 든 무지,어... 제가 찾아봤는데 꼭 그런 건 아닌 것 같네요... ㅠㅠ
3/26/24 19:29,손을 번쩍 든 무지,사진
3/26/24 19:30,손을 번쩍 든 무지,"여기서도 2x2 filters and stride 2라고 한 걸 보면 filter size하고 stride는 별개인 것 같은데, 혹시 제가 생각한 게 맞을까요??"
3/26/24 19:44,치즈케이크,"Size보다 stride가 크면 원본 데이터에서 누락되는 component가 존재하고, 작으면 pooling 결과 사이에 dependency가 발생할 수 있겠죠?"
3/26/24 19:46,손을 번쩍 든 무지,네 그쵸.. 그러면 그냥 이렇게 가정하는 게 맞으려나요?
3/26/24 19:52,화난 라이언,뭐가 맞고 틀리고는 없는 것 같습니다...
3/26/24 19:54,피자 먹다 자는 무지,보통은 그렇게 적용하지만 그렇게 하지 않을 수도 있습니다.
3/26/24 20:06,손을 번쩍 든 무지,아 그렇군요.. 어차피 이번 과제 문제 푸는 데는 상관없을것같긴 해요. 세 분 모두 감사합니다!
3/26/24 20:09,건방진 제이지,과제4의 5번에서 주어진대로 loss를 바꾸니 test accuracy가 49%까지 떨어졌는데 반보다 못 맞춘다는건 모델이 학습을 아예 못했다는 의미인 것 같아 좀 이상합니다.
3/26/24 20:09,건방진 제이지,혹시 다른분들도 저만큼 떨어지셨나요..?
3/26/24 20:11,경례하는 프로도,네 저도 그렇게 나와서 loss 말고도 다른 부분을 좀 더 손봤습니다
3/26/24 20:12,하트뿅뿅 라이언,Loss만 바꿔도 비슷한 결과 나옵니다
3/26/24 20:23,열심히 일하는 네오,저는 둘다 비슷하게 accuracy 나왔어요
3/26/24 20:25,으쓱으쓱 어피치,"Mnist 데이터에는 0~9까지 모두 다 있어서, 전 그거 전처리를 좀 했습니다"
3/26/24 20:27,파이팅하는 무지,"아마 test accuracy를 계산하는 코드가 잘못된 것 같습니다 label이 {-1,1}이라서 prediction을 계산할 때 추가적인 코드가 필요해요"
3/26/24 22:17,하트뽀뽀 어피치,"KL-divergence 를 사용할 때에도 label이 {-1,1}이었던 것 같은데 test accuracy 계산 코드가 달라질 이유가 있나요?"
3/26/24 22:40,열심히 일하는 네오,Chapter 2 code를 보시면 됩니다
3/26/24 22:42,파이팅하는 무지,"저는 sigmoid activation을 모델에 포함하고 nn.BCELoss로 훈련해서요. 이 경우에는 torch.argmax로 prediction을 계산하면 {0,1}범위가 나와서 말씀드렸습니다."
3/27/24 10:31,초롱초롱 어피치,HW re-do는 ETL 과제란 각 문제에 제출하면 되는 것인가요?
3/27/24 11:05,애교뿜뿜 무지,.
3/27/24 11:05,애교뿜뿜 무지,이번주도 똑같이 하면 될것 같습니다:)
3/27/24 11:19,초롱초롱 어피치,감사합니다!!
3/27/24 11:40,엄지척 프로도,다들 hw4 5번 베이스라인 코드 없이 어떻게 하셨나요? hw2 1번 코드로 하면 될까요?
3/27/24 11:42,DL하는 블루,저는 챕터2 수업 코드 썼습니다
3/27/24 11:59,엄지척 프로도,감사합니다
3/27/24 12:46,빈털터리 제이지,빈털터리 제이지님이 들어왔습니다.
3/27/24 13:37,힙합맨 제이지,사진
3/27/24 13:38,힙합맨 제이지,"과제3 5-c solution에서, d_i = (1/2)b_i + C_i 라고 되어있는데, C_i 에 1-vector를 곱한 형태가 되어야 하지 않나요?"
3/27/24 13:50,멋쟁이 프로도,네 그래서 칼럼벡터를 모두 더한 꼴이 돼야할거같습니다
3/27/24 14:29,콘이 웃긴 무지,혹시. redo의 경우 감점된 부분문제만 고쳐서 제출해도 되나요?
3/27/24 14:48,기타치는 튜브,원래 그렇게 하는 것 아닌가요
3/27/24 15:25,TA chaeju,"소문제 하나를 말씀하시는 거라면, 그렇게 제출하셔도 됩니다.
대문제 번호만 잘 맞춰주세요"
3/27/24 15:29,TA chaeju,"네, C_i * [1, 1, 1… 1]^T를 생각하시면 됩니다."
3/27/24 17:49,하트뿅뿅 라이언,사진
3/27/24 17:50,하트뿅뿅 라이언,hw2 sol 5-b에서 <=가 아니고 <인 이유가 있을까요?
3/27/24 17:54,TA chaeju,"등호가 성립하려면 z' = 0이어야 하는데,
y=e^x / (1+e^x)의 그래프를 그려보시면, z'가 0이 되는 z1과 z2는 존재하지 않음을 알 수 있습니다."
3/27/24 17:54,TA chaeju,사진
3/27/24 18:03,하트뿅뿅 라이언,감사합니다!!
3/27/24 18:16,멋쩍은 튜브,오늘 강의에서 g랑 m이 뭐가 다른거죠... g가 gradient고 gradient 의 기댓값이 m이라고 말하셨는데 ... 잘 모르겠어요
3/27/24 18:17,기지개 고양이,m1은 g값들의 가중평균이라고 이해하시면 될 것 같습니다
3/27/24 18:19,기지개 고양이,"m2는 ||g||²의 가중평균, 정확히는 (1-β)의 할인률을 갖는"
3/27/24 22:45,손을 번쩍 든 무지,저희 이번 과제 5번 문제가 2단원 코드에서 loss function을 KL-divergence에서 sum-of-squares로 바꾸고 전과 후의 performance를 비교하는 것이라고 보면 되나요?
3/27/24 23:01,열심히 일하는 네오,"Problem 6(b)에서 
두번째로 보여야하는 식의 y_l-1의 transpose 이거 혹시 typo인가요?
(dy_L/dy_l)^T (y_l-1)^T이
(dy_L/dy_l)^T (y_l-1)여야 하는거 아닌가요?
"
3/27/24 23:03,청소하는 튜브,분모중심표현 분자증심표현 섞어써서 헷갈리네요 ㅠㅠ 문제마다 있는 remark랑 shape 보고 맞춰야 할 거 같아요
3/27/24 23:06,하트뿅뿅 라이언,Problem 5번 결과가 두 방법의 퍼포먼스 차이가 거의 없다는 결론이 나왔는데 다들 어떻게 나오셨나요?
3/27/24 23:07,열심히 일하는 네오,그러네요 ㅠ
3/27/24 23:07,열심히 일하는 네오,저도 그렇게 나왔어요
3/27/24 23:14,Ryan with his pillow,Ryan with his pillow님이 나갔습니다.
3/27/24 23:22,부끄러워하는 라이언,"혹시 7번 training 하는데 원래 오래걸리나요?
거의 30분째 하고 있는데 이제 7번째 epoch하고 있네요"
3/27/24 23:26,벙찐 튜브,전 cpu만 써서 6분 30초 걸리긴했어요
3/28/24 1:39,손을 번쩍 든 무지,"혹시 5번문제 MSELoss 썼는데 average loss = nan, accuracy = 0% 나오신 분 계신가요..?"
3/28/24 9:50,옐로카드 프로도,dropout 시에 probability값은 훈련 도중에 무조건 고정해놓는 것인가요? 가변적으로 사용하는 방법도 있을까요?
3/28/24 9:53,초롱초롱 어피치,https://proceedings.neurips.cc/paper_files/paper/2013/file/7b5b23f4aadf9513306bcd59afb6e4c9-Paper.pdf
3/28/24 9:54,초롱초롱 어피치,찾아보니 이런 paper가 있는데 자주 본적은 없는거 같습니다
3/28/24 9:58,하트뿅뿅 라이언,"기술적으로 가능하긴 하겠지만, dropout의 probability가 달라지면 다음 레이어 인풋의 스케일이 달라져버려서.. 큰 이유가 있는게 아니면 쓰기 힘들 것 같습니다"
3/28/24 10:01,손을 번쩍 든 무지,"보통은 0.5 정도로 쓴다고는 한다고 어디서 들은 것 같은데, 그 이유는 모르겠네요"
3/28/24 10:22,옐로카드 프로도,"제가 ppt에서 본 것으로는 확률에 맞추어 스케일을 조정하는 걸로 아는데, 인풋의 스케일이 달라진다는 것이 무슨 뜻인지 알 수 있을까요..??"
3/28/24 10:24,옐로카드 프로도,"뭔가 그냥 추측하기로는, 확률이 커질 수록 정보가 단편적으로 추출되니, 레이어 상황에 따라 그 정도를 조절하면 좋지 않나 싶어서 여쭤봤습니다"
3/28/24 14:23,멋쩍은 튜브,헉 저도 그래서 고치는 중이에여...
3/28/24 15:54,멋쩍은 튜브,혹시 5번에서
3/28/24 15:54,멋쩍은 튜브,사진
3/28/24 15:55,멋쩍은 튜브,mse의 경우 정답 여부를 이렇게 확인해 주어도 되나요?
3/28/24 15:55,멋쩍은 튜브,제가 손으로 식을 써보니 그랬는데 결과가 자꾸 44%가 나와서... 여기서 틀린 건지 궁금해서요
3/28/24 16:32,치맥하는 제이지,5번 혹시 다들 iterations 몇으로 잡고 하셨나요? 전 1000 3000 5000 10000이렇게 해봤는데 sum_squared_loss로 했을 떄 iteration 별로 정확도 차이가 많이 나길래 맞게 되고 있는건지 약간 불안하네요
3/28/24 16:32,클라인,클라인님이 나갔습니다.
3/28/24 16:36,얼굴마사지하는 제이지,저도 정확도 50퍼 근처로 나오네요 ㅠㅠ Linear layer 1개로 SGD 돌렸는데 혹시 정확도 높게 나오신 분들도 같은 모델이신가요?
3/28/24 16:39,치맥하는 제이지,"저도 iteration 1000, twolayer sgd로 했는데 50퍼 정도 나와요"
3/28/24 16:39,애교뿜뿜 무지,"혹시 계속해서 돌려보셨나요? 저는 같은 모델로 50퍼 대가 나올 때가 있고 10번 중 2~3번은 8~90퍼가 나올 때도 있는데, 초기값에 영향을 많이 받더라구요. 이렇게 해서 높게 나온 퍼포먼스도 높다고 볼 수 있을까요?"
3/28/24 16:41,치맥하는 제이지,앗 그렇네요 저도 방금 5번정도 돌려봤는데 한번은 49% 나머지는 89~90%로 나오네요
3/28/24 16:44,멋쩍은 튜브,"저는 계속 돌려도 50%가 나오네요,,,"
3/28/24 16:44,멋쩍은 튜브,흠
3/28/24 16:46,멋쩍은 튜브,앗!! lr 조정하니 나아졌어요 신기하네요 ㅋㅋㅋ
3/28/24 16:46,멋쩍은 튜브,10000번에 lr = 1e-5 해보세요 50% 나오시는 분들
3/28/24 17:15,멋쩍은 튜브,저희 문제 4번에서 혹시 저 maxpool의 stride나 그런 것을 알아야 서술이 명확하지 않나요?
3/28/24 17:20,청소하는 튜브,맞는 말이긴 한데 그럼에도 일반적인 경우에 대해 서술할 수 있어요
3/28/24 17:21,멋쩍은 튜브,음 뭔가 뭉개서 서술하면 되네요 감사합니다!
3/28/24 17:21,손을 번쩍 든 무지,activation function이 non-decreasing이니깐 아마 상관없긴 할것같아요
3/28/24 18:19,손을 번쩍 든 무지,오
3/28/24 18:19,손을 번쩍 든 무지,저도 조정하니 나아지네요 ㅋㅋㅋㅋ
3/28/24 21:05,Apeach enjoys music,혹시 과제4 문제1번에서 w를 수식으로 표현하라는건가요..? 아니면 수업 피피티 나온것처럼 Y를X에 대한 식으로 쓰라는 말일까요..?
3/28/24 21:06,Apeach enjoys music,사진
3/28/24 21:06,Apeach enjoys music,이런식으로요..?
3/28/24 21:09,손을 번쩍 든 무지,저는 후자로 이해하고 썼습니다
3/28/24 21:19,Apeach enjoys music,감사합니당!
3/29/24 8:53,TA Gyeongmin,HW2 redo is graded.
3/29/24 10:51,부끄러워하는 라이언,"카카오톡을 지웠다 깔아 답장 이동이 안되는데, 혹시 같은 내용을 다시 보내주실 수 있을까요?"
3/29/24 10:55,기타치는 튜브,Etl 공지에 제출방법 다 있습니다
3/29/24 10:58,애교뿜뿜 무지,네 맞습니다 공지 보시면 되는데 각 문항별로 새로 나온 과제에 redo 제출하시면 됩니다
3/29/24 11:32,부끄러워하는 라이언,감사합니다
3/29/24 14:24,눈물 흘리는 제이지,5*5 matrix에 zero padding을 1 만큼 했다는 것은 6*6 으로 만든건지 7*7 로 만든건지 궁금합니다
3/29/24 14:25,열심히 일하는 네오,7*7입니다
3/29/24 14:25,열심히 일하는 네오,위아래 양옆 1씩 늘어나는 거라 보시면 돼요
3/29/24 14:43,말썽쟁이 네오,3번은 푸는건가요?
3/29/24 16:48,택배 상자를 든 네오,혹시 문제 5번 다들 실행시간 얼마나 걸리셨나요? 저는 epoch 한개당 거의 1초씩 걸리는거 같은데.. 다른분들은 어떻게하셨는지 궁금합니다
3/29/24 17:42,TA Yongin,"네, 모든 문제에 대해 풀이를 제출해주시기 바랍니다. (Yes, please submit solutions for all the problems.)"
3/29/24 18:11,Apeach enjoys music,"저도 궁금한데 위아래양옆이 1씩 늘어나즌거면 X_:,0=0과 같은 조건들이 더 주어져야할거 같은데 없어서 혼란스럽습니다.."
3/29/24 18:36,초롱초롱 어피치,사진
3/29/24 18:36,초롱초롱 어피치,"저 부분에 theta k+1을 대입하면 stepsize랑 gradient term 2개가 big o 안에 남는데
stepsize term이 dominant한 이유가 뭔지 알 수 있을까요?"
3/29/24 19:41,화난 라이언,음 아마 제 생각에는 뒤쪽 슬라이드에서 gradient term이 0로 converge하는 걸 가정한 것 같아요
3/29/24 19:41,초롱초롱 어피치,grad f가 유계라는 가정은 없나요?
3/29/24 19:56,기지개 고양이,"혹시 5번 문제
Lr= 1e-4 부터 1e-7까지,
train 횟수도 10만으로 늘렸는데도

정확도 50%대 나오는 분 계신가요?"
3/29/24 19:57,하트뿅뿅 라이언,아마 라벨 체크하는 코드가 잘못돼있을 가능성이 높을 거 같아요
3/29/24 19:58,기지개 고양이,제가 라벨 분류 프린트까지 해보면서 그부분 체크 해봤는데
3/29/24 19:58,기지개 고양이,그 부분 한정으론 문제 없는것 같습니다
3/29/24 20:04,청소하는 튜브,음 50%대면 loss 식이나 train code에 실수가 있을 가능성이 높은 거 같아요
3/29/24 20:04,청소하는 튜브,라벨 정답 후보 두 개 중 하나를 찍는 거 자체가 기댓값이 50%라...
3/29/24 20:06,멋쟁이 프로도,"저 슬라이드는 GD가 항상 descent direction으로 간다는 걸 설명하고 있는데, ||grad||랑 상관없이 a<<1 regime에서 f(k+1)<f(k)를 설명하는 데는 문제없지 않나요?
물론 그런 a를 잡는 것은 grad에 의존하긴 하겠지만요.
이를테면 C*a²*grad²으로 bound시키는 C가 존재한다고 했을 때 a<1/C이면 descent는 보장되지 않나 싶습니다"
3/29/24 20:07,기지개 고양이,다시 살펴보겠습니다. 감사합니다
3/29/24 20:11,기지개 고양이,감사합니다. 해결했습니다.
3/29/24 20:11,기지개 고양이,이모티콘
3/29/24 20:44,손을 번쩍 든 무지,혹시 5번 문제에서 training time 얼마나 나오시나요? 20초 이상 걸린다면 코드가 뭔가 비효율적인 걸까요?
3/29/24 20:55,기지개 고양이,"저는 코랩에서 gpu 가속 없이 1만번은 6초, 10만번은 60초 가량 걸렸어요"
3/29/24 21:28,손을 번쩍 든 무지,삭제된 메시지입니다.
3/29/24 21:28,손을 번쩍 든 무지,삭제된 메시지입니다.
3/29/24 21:34,손을 번쩍 든 무지,2단원 코드 참조하긴 했는데 모델을 같은 걸 써봐도 loss function을 잘못 짠 건지 training time이 꽤 걸리네요..
3/29/24 21:38,청소하는 튜브,혹시 if문이 있으시면 Hint처럼 loss 짜는게 좋을거예요
3/29/24 22:11,손을 번쩍 든 무지,"if문은 따로 없긴 한데, epoch를 10000에서 1000으로 줄이니까 해결되네요.. ㅋㅋㅋㅋ"
3/29/24 22:11,손을 번쩍 든 무지,이게 맞는건진 모르겠지만요 ㅠ
3/29/24 22:13,손을 번쩍 든 무지,"2단원 코드에서도 똑같이 epoch 갯수를 10000으로 늘리면 10후~20초 정도 나오긴 하는데, 이러니깐 코드 문제인지 노트북 문제인지 더 헷갈리네요"
3/29/24 22:17,택배 상자를 든 네오,혹시 training data를 dataloader를 이용해서 다루셨나요? 저도 같은 문제로 고민하다 2단원 코드 앞부분에 dataloader 없이 training data 를 불러오는 코드로 작성했더니 시간이 많이 줄었습니다.
3/29/24 22:20,손을 번쩍 든 무지,앗 넵 dataloader로 다루었어요. 코드를 조금 더 다듬어 봐야겠네요.. 세분 모두 감사합니다!
3/30/24 12:24,멋쩍은 튜브,저희 과제 6번에서 part b에 use the chain rule to first compute~ 는 증명하라는 건가요 아니면 그냥 쓰라는 의미인가요?
3/30/24 12:40,Joshua,"I am getting an accuracy of around 11.6% with my implementation for problem 7 which feels like its wrong. Is the expected accuracy in that region or is my code wrong?
Thank you"
3/30/24 12:42,신난 어피치,"I got same value, so I tried to explain the reason..."
3/30/24 12:44,멋쟁이 프로도,One possibie reason might be that you have processed the shapes of tensors in a wrong way. I recommend looking at the reshape() method again.
3/30/24 12:59,Joshua,The dimension of the tensor concatenation was wrong.
3/30/24 14:17,N이 되고픈 엡실론,"While working on problem 5, the code seems to work well only when I put a - sign in front of what I thought should be the loss function. This also seems to be the case in the Ch 2. code; is there a reason for this?"
3/30/24 14:28,초롱초롱 튜브,"I think you should check your gradient descent step. If you are adding gradient to your parameter, - sign in front of loss funciton may be necessary."
3/30/24 14:29,N이 되고픈 엡실론,Thank you!
3/30/24 14:44,건방진 제이지,혹시 5번 문제 정확도 다들 얼마쯤 나오시나요...?
3/30/24 14:45,말썽쟁이 네오,8~90 나와요
3/30/24 14:50,손을 번쩍 든 무지,저도 대충 80 ~ 90 정도 나옵니다
3/30/24 15:09,N이 되고픈 엡실론,저도 그 정도 나옵니다
3/30/24 15:26,권투하는 무지,5번 힌트처럼 loss를 직접 정의하면 pytorch의 backward를 쓸 수 없는 게 맞나요?
3/30/24 15:29,치즈케이크,"In Problem 2 of HW4, it seems that there are several valid choices of dimensions for the filters used in the convolution operation. (For example, the filter is supposed to be a C×C×k×k-dimensional tensor according to slide 7 of the lecture notes from Chapter 3, while it is possible to reduce the filter to a k×k-tensor due to repeated numbers.)
Is there a convention we should follow, or are we allowed to freely decide the filter dimensions on our own?"
3/30/24 15:35,손을 번쩍 든 무지,삭제된 메시지입니다.
3/30/24 16:01,건배하는 프로도,혹시 중간고사 범위는 사전에 공지되나요?
3/30/24 16:06,벌 서는 라이언,제 기억이 정확할지는 모르겠지만 Chap 1~3이라고 들었던 것 같습니다.
3/30/24 16:32,TA Gyeongmin,일반적으로 torch는 loss function을 customize해도 back propagation을 잘 수행합니다
3/30/24 16:34,권투하는 무지, customize라는 게 제공되는 loss function을 inherit하여 연산을 추가한다는 말씀이신가요?
3/30/24 16:48,멋쟁이 프로도,"In my opinion, kxk dimension would be more appropriate. According to the lecture note, pooling operates over each channel, and the channel size is preserved after the operation, nevertheless this view is somewhat different from the original convolution we've learned."
3/30/24 16:50,멋쟁이 프로도,저는 CE랑 squares 둘 다 95%대가 나오던데...
3/30/24 16:53,TA Gyeongmin,torch에서 제공하는 loss fnc뿐만아니라 torch의 autograd가 gradient를 계산할 수 있는 함수이면 가능합니다. 이와 관련해서 Ch3 후반부(ppt 85p 이후)에 관련 내용이 있으니 참고하시면 좋을 것 같습니다.
3/30/24 16:57,권투하는 무지,감사합니다!
3/30/24 17:02,말썽쟁이 네오,저도 만들어서 썼어요
3/30/24 17:05,치즈케이크,That makes sense. Appreciate the opinion!
3/30/24 17:30,청소하는 튜브,저도요 94-95퍼 나옵니다
3/30/24 17:42,건방진 제이지,다들 파라미터 갯수 얼마 나오시나요? 인터넷에 검색했을때랑 다르게 나오네요 ㅠㅠ
3/30/24 17:49,멋쟁이 프로도,저도 계산한것보다 더 많이 감소해서 고민중이네요...
3/30/24 18:14,멋쩍은 튜브,이게 달라야되는 거 아니에요?
3/30/24 18:14,멋쩍은 튜브,문제보니까 다르냐고 물어보길래
3/30/24 18:14,멋쩍은 튜브,같으면 굳이 안물어볼거같아서
3/30/24 18:14,멋쩍은 튜브,아닌가
3/30/24 18:18,멋쟁이 프로도,답은 아직 모르겠지만 다른 이유가 잘 짐작이 안 돼서요
3/30/24 18:20,건방진 제이지,구글에 찾아보니깐 6만개가 나와야된다고 그러는데 제건 그거보다 많이 나와서...
3/30/24 18:34,기타치는 튜브,직접 Layer 두께 가지고 파라미터 개수 계산해서 비교했어요
3/30/24 18:35,청소하는 튜브,저는 계산이랑 같게 나왓어요 (Conv2d로 했을때 - Original LeNet으로 했을때)
3/30/24 18:36,Tube cleaning,저도 같게 나왔어요 6만개 좀 넘게 나오더라고요
3/30/24 18:42,이승환,이승환님이 나갔습니다.
3/30/24 20:41,화나서 방방 뛰는 튜브,혹시 layer하나를 여러번 쓰고 계신가요?
3/30/24 21:16,멋쟁이 프로도,네 그런 문제였습니다. 지금은 해결되었습니다.
3/30/24 23:22,심수기모띠,심수기모띠님이 나갔습니다.
3/31/24 12:02,열심히 일하는 네오,삭제된 메시지입니다.
3/31/24 14:24,쑥스럽게 인사하는 프로도,혹시 7번문제에서 이론적으로 계산한 파라미터수가 print 결과와 다른게 정상인가요..?
3/31/24 14:24,쑥스럽게 인사하는 프로도,아니면 제가 계산을 잘못한걸까요
3/31/24 14:25,신난 어피치,저는 같게 나왔어요
3/31/24 14:25,귀여운 라이언,저도 같게 나왔어요
3/31/24 14:29,쑥스럽게 인사하는 프로도,아 제가 bias를 빼먹었ㄴ요
3/31/24 14:29,쑥스럽게 인사하는 프로도,네요
3/31/24 14:29,쑥스럽게 인사하는 프로도,감사합니다!
3/31/24 18:28,눈물바다에 빠진 라이언,"혹시 minibatch로 학습시키는게 SGD보다 성능이 오히려 안 나올 수가 있나요? batch 사이즈를 바꿔가면서 여러번 해봤는데, batch가 있는 경우엔 학습이 아예 안 돼고, SGD로 하니 성능이 높게 나옵니다"
3/31/24 18:32,눈물바다에 빠진 라이언,"코드가 안 돌아가는 건 아니고, 학습의 진행은 되는데 accuracy가 50% 언저리에서만 움직입니다"
3/31/24 18:34,치즈케이크,7번인가요?
3/31/24 18:34,눈물바다에 빠진 라이언,5번입니다
3/31/24 18:34,눈물바다에 빠진 라이언,Loss에 따라서 성능을 평가하려면 Hyperparameter가 같아야할 것 같아서
3/31/24 18:35,눈물바다에 빠진 라이언,Hyperparameter를 같게 두고 minibatch로 학습을 진행했는데 이래서 질문드립니다
3/31/24 18:35,치즈케이크,Implementation이 잘못된 거 아닌가 싶네요
3/31/24 18:43,멋쟁이 프로도,저같은 경우는 reshape() 빠트렸을 때 그러긴 했어요
3/31/24 18:44,눈물바다에 빠진 라이언,아 저도 방금 그걸 확인했네요
3/31/24 18:44,눈물바다에 빠진 라이언,다들 감사합니다
3/31/24 19:05,음료 마시는 어피치,혹시 어떤 부분을 reshape하셨는지 질문해도 가능할까요? ㅠㅠ
3/31/24 19:06,눈물바다에 빠진 라이언,"loss function의 input으로 target과 model output를 받는데, 이 때 두 텐서에다가 reshape(-1)를 해줬습니다"
3/31/24 19:07,음료 마시는 어피치,감사합니다!!
3/31/24 19:08,권투하는 무지,loss func을 뭘로 쓰느냐에 따라 input의 dim을 조정해 주어야 합니다. 전 pytorch 매뉴얼 참고하니 도움 많이 되었어요
3/31/24 19:20,초롱초롱 네오,7번 스켈레톤 코드 안 고치고 그대로 런하면 원래 에러가 나오나요?
3/31/24 19:21,초롱초롱 네오,사진
3/31/24 19:25,기타치는 튜브,당연하죠 …
3/31/24 19:35,치즈케이크,model의 생성자와 forward method가 미구현 상태이기 때문에 에러가 날 수밖에 없습니다(pass라고 되어 있는 부분)
3/31/24 19:40,N이 되고픈 엡실론,삭제된 메시지입니다.
3/31/24 20:25,초롱초롱 네오,아 맞네요 감사합니다
3/31/24 21:48,음료 마시는 어피치,혹시 7번 구현하였을 때 시간이 원래 오래걸리나요..! 10epoch 일때 10분 걸렸습니다.
3/31/24 21:54,Tube cleaning,저도 10분정도 걸렸어요!
3/31/24 22:14,손을 번쩍 든 무지,저도 10 epoch 기준으로 500초 걸리네요..
3/31/24 22:17,건방진 제이지,코랩 t4 gpu로 돌리면 1분정도 걸립니다
3/31/24 23:25,치맥하는 제이지,"혹시 layer를 이렇게 선언해도 서로 다른 layer 16개를 생성한 것과 같은 효과가 있는게 맞나요?
self.layer = [nn.Conv2d(6, 1, kernel_size=5) for _ in range(16)]"
3/31/24 23:30,하트뿅뿅 라이언,nn.ModuleList를 활용해보세요
3/31/24 23:30,힙합맨 제이지,ModuleList로 묶으면 맞을겁니다
3/31/24 23:31,치맥하는 제이지,아 넵 감사합니다
3/31/24 23:33,멋쩍은 튜브,근데 nn.conv3d 써야 하는 거인가요...?
3/31/24 23:33,멋쩍은 튜브,3차원 convolution이라 뭔가 그런 거 같아서요...
3/31/24 23:38,기지개 고양이,아뇨 2d로 돼요
3/31/24 23:49,멋쩍은 튜브,앗 하나만 더 여쭈고 싶습니다!
3/31/24 23:49,멋쩍은 튜브,사진
3/31/24 23:49,멋쩍은 튜브,이 상황에서 저 색깔별로 필터를 하나씩 쓰셧나요?
3/31/24 23:50,멋쩍은 튜브,사진
3/31/24 23:50,멋쩍은 튜브,"이렇게 해서 합치려고 하는데, 혹시 저 연속 4장용 conv2d랑 불연속 4장요 ㅇconv2d를 따로 만들어야 하나"
3/31/24 23:50,멋쩍은 튜브,궁금해서요!
3/31/24 23:51,Tube cleaning,저는 4장용은 한번에 했어요! 따로 안만들어도 되더라고요
3/31/24 23:51,멋쩍은 튜브,감사합니다
3/31/24 23:54,열심히 일하는 네오,다들 5번 문제 epoch 얼마나 돌리셨나요?
4/1/24 0:03,건방진 제이지,전 단순 SGD 써서 iter기준인데 10000정도부터는 성능이 고만고만했던거 같아요
4/1/24 0:05,건방진 제이지,아 근데 lr이나 forward에 따라서 영향을 좀 크게 받는 것 같으니 참고만..하십쇼
4/1/24 0:05,열심히 일하는 네오,넵 감사합니다 생각보다 train이 너무 오래걸려서.. 당황했네요
4/1/24 0:06,치즈케이크,따로 만들지 않더라도 forward()에서 list를 잘 넘겨주면 되더라구요
4/1/24 0:13,멋쩍은 튜브,혹시 feature map을 channel별로 분리하는 법 아시는 분 있나요?
4/1/24 0:14,멋쩍은 튜브,"지금 x[0], 이런 식으로 하는데 잘 안 되고 있고 구글에 feature map seperate by channel 쳐도 잘 안 나와서 검색어 힌트라도 주시면 감사하겠습니다!"
4/1/24 0:18,말썽쟁이 네오,Input이 텐서니까 잘라보세요
4/1/24 0:23,멋쩍은 튜브,문제 hint를 참고하시면 될 듯 합니다.
4/1/24 0:34,멋쩍은 튜브,아 element 별로 했는데
4/1/24 0:34,멋쩍은 튜브,보니까 그냥 잘 인덱싱 하는 게 나앗겟네요
4/1/24 0:34,멋쩍은 튜브,다들 정말 감사ㅏㅎㅂ니다 제가 뻘짓한거였어요
4/1/24 0:55,멋쩍은 튜브,앗 근데 이러면
4/1/24 0:55,멋쩍은 튜브,60000 안 나오지 않나요?
4/1/24 1:17,치즈케이크,잘 나오죠?
4/1/24 1:17,치즈케이크,저는 나오던데..
4/1/24 1:36,멋쩍은 튜브,아 저는
4/1/24 1:36,멋쩍은 튜브,60000으로 계산 안 하고
4/1/24 1:36,멋쩍은 튜브,문제가 그 차이만 물어보길래 그냥
4/1/24 1:36,멋쩍은 튜브,차만 계산했습니다
4/1/24 1:36,멋쩍은 튜브,차는 제대로 나와요
4/1/24 1:37,치즈케이크,아하
4/1/24 2:26,부탁하는 네오,"5번문제에서 챕터2 코드 그대로 가져다가 loss function만 바꾸면 정확도가 50%밖에 안 나옵니다
다른 부분 열심히 찾아봤지만 기존 CE Loss function 가지고는 90% 이상의 정확도가 나오는지라 코드의 다른 부분이 문제라고 말하지를 못하겠는데 바꿔줘야 할 다른 부분이 있을까요?"
4/1/24 2:31,멋쩍은 튜브,저도 이렇게 했는데
4/1/24 2:31,멋쩍은 튜브,Lr 1/10 으로 줄이고 epoch 10배 해보세용
4/1/24 2:31,멋쩍은 튜브,저도 처음에는 50% 나왔다가 바꾸니까 85퍼 나오네요
4/1/24 2:34,부탁하는 네오,그 loss function에 마이너스 붙이는 건 해줘야 하는거 맞죠? 챕터2 코드에도 그렇게 나와있고 여기톡방에서도 그렇게 본 것 같습니다
4/1/24 2:35,부탁하는 네오,파이토치 4줄로 트레이닝할때 그래디언트를 더하는 작업이 있는 거라고 보면 될까요
4/1/24 2:35,멋쩍은 튜브,앗...?
4/1/24 2:35,멋쩍은 튜브,전 그대로 썼는디
4/1/24 2:35,멋쩍은 튜브,마이너스는 쟤가 알아서 해주지않을까요
4/1/24 2:35,멋쩍은 튜브,하는 마음
4/1/24 2:36,부탁하는 네오,앗 그렇군요 ㅎㅎ 일단 해보겠습니다 답변 감사해요
4/1/24 2:36,손을 번쩍 든 무지,sigmoid function 썼더니 다른거 안바꿔도 90퍼이상 나오던데요
4/1/24 2:36,부탁하는 네오,제가 그냥 sgd를 해서 그런걸까요
4/1/24 2:36,멋쟁이 프로도,마이너스가 붙은 건 원래 로스의 정의가 그렇기 때문일텐데요. logistic regression 강의 슬라이드 참고하시는게 좋겠습니다.
4/1/24 2:37,멋쩍은 튜브,앗 혹시 저한테 하시는 말씀이신가요!!
4/1/24 2:37,멋쩍은 튜브,제가 모르는거면 빨리 수정하게요...
4/1/24 2:37,손을 번쩍 든 무지,저도 그냥 sgd썼어요 loss ftn 제대로 구하셨는지 확인해보시는것도 좋겠네요
4/1/24 2:37,멋쩍은 튜브,근데 loss는 그
4/1/24 2:38,멋쩍은 튜브,힌트 부분에 주어져 있어서
4/1/24 2:38,멋쟁이 프로도,"네 그니까 코드에 있는 로스펑션이 마이너스를 붙인 게 아니라, 마이너스 붙은 함수가 원래 원래 로스입니다"
4/1/24 2:38,부탁하는 네오,아 그 파이토치 logsigmoid가 강의자료에 나온거랑 마이너스 붙인 관계네요
4/1/24 2:38,멋쩍은 튜브,앗 감사합니다!!
4/1/24 2:38,부탁하는 네오,이해했습니다
4/1/24 2:38,멋쩍은 튜브,그런데 제가 궁금한 게있는데요...
4/1/24 2:39,멋쩍은 튜브,Mseloss는 양수인데 왜 코드의 loss는 음수인가요...? 뭔가 어떤 loss를 쓰던 음수거나 양수거나 하나만 하는 게 일관성도 있고 맞을 것 같은데
4/1/24 2:39,멋쟁이 프로도,정확도 50%는 tensor shape를 잘못 다뤘을 가능성도 있습니다
4/1/24 2:41,멋쟁이 프로도,LR에서 정의된 CE-loss는 log(1+exp(-z))입니다. 그런데 코드에서 logsigmoid 함수를 갖다쓰다보니 -log(1/1+exp(-z))가 된 것입니다. 차이는 없습니다.
4/1/24 2:42,멋쩍은 튜브,아!!! 제가 logsigmoid를 잘 몰랐던 것 같습니다 정말 감사해요
4/1/24 2:42,멋쩍은 튜브,저는 뭔가 양수인 값에 -를 붙여야 loss다라고 하시는 줄 알고 엥 왜지 했는데...
4/1/24 2:42,멋쩍은 튜브,제가 이해를 잘못했네요 감사해요
4/1/24 2:47,부탁하는 네오,"흠 reshape(-1) 해도 결과가 별로 달라지지 않는데..
output이랑 target에 각각 .reshape(-1) 해주고 lossfunction 계산하면 되는거 맞지 않나요?"
4/1/24 2:48,멋쩍은 튜브,이거 효과 없나요?
4/1/24 2:51,부탁하는 네오,batch size 얼마로 하셨어요?
4/1/24 2:51,부탁하는 네오,톡방 훑어보니까 dataloader 안 쓰는 게 좋다길래 그냥 무지성 sgd로 일단 해보고있는데 잘 안 돼서
4/1/24 2:54,부탁하는 네오,효과 있네요 감사합니다
4/1/24 2:54,부탁하는 네오,시간이 좀 많이 걸리긴 하는데
4/1/24 2:58,멋쩍은 튜브,저도 sgd 한거같아요
4/1/24 2:58,부탁하는 네오,근데 그  궁금한 게
4/1/24 2:58,부탁하는 네오,챕터2 코드 보면 torch.nn.functional.logsigmoid에 넣을 때 어디는 reshape 해서 넣고 어디는 그냥 넣던데
4/1/24 2:59,부탁하는 네오,초반 코드는 reshape 안 하고 돌려도 정확도 높게 나오거든요
4/1/24 2:59,부탁하는 네오,이유가 뭘까요..
4/1/24 3:02,아침햇살,"혹시 5번에 저희 모델은 a가 -1, b가 0인 건가요?"
4/1/24 3:11,TA chaeju,"chap2코드에서 reshape을 처음으로 사용하는 부분인, 아래 사진 설명을 읽어보시면 해결될 것 같습니다.

핵심은 SGD / minibatch의 차이라고 할 수 있겠네요"
4/1/24 3:11,TA chaeju,사진
4/1/24 3:20,부탁하는 네오,감사합니다
4/1/24 4:34,머리 빗는 네오,혹시 5번 몇 epoch정도 돌려야 하나요...? 그리고 둘다 90퍼 이상 정확도 보이는데 이러면 문제가 될까요?
4/1/24 5:17,손을 번쩍 든 무지,저는 10000번했어요
4/1/24 5:17,손을 번쩍 든 무지,둘이 별차이 없었네요
4/1/24 6:33,전커서교수님이될래요,문제 5번에서 KL-Divergence 식에만 1/N가 붙어있지 않는 이유가 있나요?
4/1/24 8:42,애교뿜뿜 무지,굉장히 뜬금없는 질문이지만 Chap. 3 ResNext forward에서 self.layers는 오타인가요? 앞서 layers를 정의한 적이 없는데 기본 메소드인가요
4/1/24 9:01,멋쩍은 튜브,조교님! 죄송한데 아직 청강전환 받으시나요?
4/1/24 9:02,멋쩍은 튜브,드랍하고 청강 신청하면 되는 것인지도 궁금합니다
4/1/24 9:04,멋쩍은 튜브,그리고 기억이 안 나는데 추후 수강 시 청강생에 대한 불이익이 있었나요...? 뭔ㄱㅏ 그런 말을 들은 것 같기도 해서요... 만약 있다면 드랍하려고요
4/1/24 11:01,TA chaeju,"1/N의 포함 여부는 minimizing problem을 푸는 데 영향을 미치지 않으니, 크게 중요한 부분은 아닙니다"
4/1/24 11:08,TA chaeju,"네, self.layer의 오타로 보이네요"
4/1/24 11:12,애교뿜뿜 무지,감사합니다!
4/1/24 11:34,.,"안녕하세요, HW3 P2 redo에 관한 질문입니다. 코드/수식 중 하나에만 오류가 있는 경우, 둘 중 하나만 제출해도 괜찮을까요, 혹은 두 개를 모두 제출해야 하나요??"
4/1/24 11:36,ㅇㅇ,두 개 다 제출하면 안전하지 않을까요
4/1/24 13:54,애교뿜뿜 무지,"혹시 이번 과제 5번 코드에서 seed 설정을 어떻게 해야 같은 결과가 나오나요? optimizer, LR 앞 그리고 SGD 각 step에서 seed 설정해주었는데 결과가 일정하지 않네요ㅠ"
4/1/24 13:56,.,"Overfitting을 피하기 위한 방법을 
Regularization 이라 했었는데
왜 Regularization이라는 표현을 사용하는 건지 궁금합니다!"
4/1/24 14:29,TA chaeju,"traning data를 과하게 학습하여, test data에서는 제 성능을 내지 못하는(Overfitting)상황을 방지하고, 모델을 조금 더 범용적인 모델로 만든다는 의미에서 나온 용어라고 보시면 됩니다"
4/1/24 15:02,.,네 이해했습니다 감사합니다
4/1/24 15:20,.,"RMS prop과 Adam 에서 모두
m2 는 동일한 식인데
왜 Adam에서만 biased correct(m2~)를 하는지 궁금합니다"
4/1/24 15:49,TA Jisun,이것에 대해서는 제가 아는 바가 없네요. 청강 신청 바랍니다.
4/1/24 15:57,TA Jisun,https://pytorch.org/docs/stable/notes/randomness.html 이 문서가 도움이 되실지 모르겠습니다
4/1/24 16:01,Ernest Lee,seed 고정을 해야 하나요?
4/1/24 16:16,TA Jisun,전반적으로 비슷한 결과가 나온다면 굳이 고정하실 필요 없을 것 같습니다.
4/1/24 18:17,손을 번쩍 든 무지,이번 과제 4번 문제는 handwriting 문제인가요?
4/1/24 18:19,TA chaeju,"네, handwriting 문제입니다."
4/1/24 22:29,눈물바다에 빠진 라이언,"hw5의 problem4의 indices(c,m,n,k,i,j)는 1부터라고 생각하고 서술하면 될까요?"
4/2/24 0:38,생각하는 라이언,"HW4 Problem2에서 solution의 의도를 이해했습니다만, 강의자료에 따르면 w는 엄밀하게는 R^(C*C*k*k)에 속하는 tensor이고 w_i,i = k*k square matrix with every component 1/k^2 , w_i,j = k*k zero matrix (if j != i) 이어야 되는게 아닌지 질문드립니다."
4/2/24 0:52,멋쟁이 프로도,"제 생각인데 애초에 풀링은 정확히 컨볼루션은 아니라서요, 풀링은 인풋 채널별로 작용하는데 컨볼루션은 인풋 채널 전체에 한번에 작용하기 때문에..
다만 필터에 의한 연산 자체는 컨볼루션과 동일한 방식이기 때문에 이 문제처럼 필터를 정의할 수 있겠는데,
컨볼루션의 연산에 주목한다면 필터의 두께(?)는 1이라고 보는 게 적절하지 않을까요"
4/2/24 0:55,엄지척 프로도,엄지척 프로도님이 나갔습니다.
4/2/24 1:11,생각하는 라이언,"네 그런 센스에서 문제의 풀이를 이해하였는데, represent the AvgPool2d operation ""as a convolution""에 더 부합하는 답변은 filter를 C*C*k*k tensor로 답을 하는 것이 더 정확하지 않을까하는 궁금증이었습니다. 자세한 답변 감사드립니다ㅎㅎ"
4/2/24 10:33,화난 라이언,저도 그렇게 생각합니다~
4/2/24 12:25,하트뿅뿅 라이언,"hw5 prob1에서 A에 대한 gradient는 같게 출력되는데, bias에 대한 gradient가 다른 이유가 있을까요? option 1,2로 구하면 dy_L/db_L = 1.33으로 나오는 것과 같이, dy/db가 다릅니다"
4/2/24 16:08,으쓱으쓱 어피치,"전 bias에 대한 gradient도 같게 나오긴 합니다. notebook에서 cell을 실행할때마다 값이 달라져서, 한번에 쭉 실행시켜야 같게 나오더라구요"
4/2/24 16:09,으쓱으쓱 어피치,Hw5 P5에서 3x3 maxpool (p=1)을 결과에 어떻게 concatenate하는지 알 수 있을까요? 아니면 maxpool이 stride=1인 overlapping maxpool인가요??
4/2/24 16:13,손을 번쩍 든 무지,저는 overlapping mazpool로 이해했습니다. 그래야 각 채널이 32×32를 유지할 수 있지 않나요?
4/2/24 16:18,손을 번쩍 든 무지,"혹시 5번 문제 코드 짜서 푸셨나요, 아님 그냥 풀어도 되는건가요?"
4/2/24 18:50,TA Hyojun,주어진 model의 parameter와 연산 횟수를 계산하는 식을 적어주시고 값까지 함께 적어주시면 됩니다.
4/2/24 19:41,손을 번쩍 든 무지,옵션3으로 해도 똑같이 나오는것같아요. Chain rule에 따라 (y - Y_data)까지 같이 곱해져서 그런거 아닐까요?
4/2/24 22:53,열심히 일하는 네오,HW5 P2의 lower cased l 의 정의가 정확히 어떻게 되나요? lower case j에 의해 정의되어지는 값인가요?
4/2/24 23:20,TA Hyojun,"l이 주어져 있다고 생각하시고 A_j가 작은 j>l이 존재한다면 i=1, …, l에 대해 dy_L/db_i 등이 작아진다는 것을 설명하시면 됩니다."
4/3/24 0:34,열심히 일하는 네오,감사합니다!
4/3/24 0:39,하트뿅뿅 라이언,감사합니다 ! 간단한걸 놓쳤네요
4/3/24 12:23,애교뿜뿜 무지,사진
4/3/24 12:24,애교뿜뿜 무지,"안녕하세요. 행렬 연산에서 미분의 결과가 몇x몇이 되야하는지 헷갈리는데, 마지막에 왜 X^T( )가 되는지에 관해 참고할만한 자료가 있을까요"
4/3/24 12:25,TA chaeju,homework 1의 solution을 참조하시면 될 것 같습니다
4/3/24 12:26,애교뿜뿜 무지,감사합니다!
4/3/24 13:07,화난 라이언,"Do we have classes on April 10th, election day? I can't find any announcement on class website."
4/3/24 14:59,비옷입은 튜브,are the problems in HW5 are all solvable before taking today's lecture?
4/3/24 15:02,엄지척 튜브,엄지척 튜브님이 나갔습니다.
4/3/24 16:14,경제학도,혹시 HW5의 Problem5에서 tensor(ex. wX+b)에 activation function을 적용(ex. σ(wX+b))한 경우 activation function evaluations 한번은 tensor 당 한번으로 봐야하나요? 아니면 내부 element 한개 당 한번으로 봐야하나요?
4/3/24 18:08,.,"hw4의 1번에서 padding 때문에 필터가 top-left corner에 centered되지 않는다고 하셨는데, padding이 있다면 아래 그림의 오른쪽처럼, padding이 없으면 왼쪽처럼 filter가 centered된다고 보면 되나요?

그리고 filter size를 (padding*2+1) 보다 크게 잡게되면 오른쪽 그림이 성립할 수가 없을 것 같은데, 애초에 filter size를 padding 영역을 넘치게 못 잡는다고 보면 되나요?"
4/3/24 18:08,.,사진
4/3/24 18:41,.,"Padding의 유무와 관계없이 filter는 original image에 있는 i,j 부터 잡기 때문에 padding이 없는 경우에는 왼쪽처럼 잡게되고, padding이 있다면 padding된 셀(?) 까지 모두 연산에 포함시키기 위해 오른쪽처럼 잡는 것으로 이해하면 될까요"
4/3/24 18:43,옐로카드 프로도,"혹시 오늘 autodiff 관련 질문 중에 chain rule 적용시 df/df 같은 요소가 왜 있는지에 물어봐주셨는데, 제가 잘 못들어서요.. 혹시 설명해주실 수 있으신가요??"
4/3/24 18:56,벌 서는 라이언,"제가 제대로 이해한건지는 잘 모르겠는데, 반복되는 연산을 끝내는 기준이 된다고 하셨던 것 같습니다. Step 0에서 df/df=1이 적혀있었던 것 같은데, 그 이후에는 df/df가 나올 때까지 계속 chain rule을 적용하고, df/df라는 곳에 도달하면 그때 비로소 멈춘다고 하셨던 것 같습니다."
4/3/24 18:58,TA Jihoon,"top-left corner에 centered된 filter가 정답이 아니라는 것은 많이 제출하신오답의 형태를 말씀드리기 위한 표현이었습니다. 
padding(size=1)된 data의 (i,j)번째 index에는 padding 이전 input data의 (i-1,j-1)번째 index의 데이터가 있게 되기 때문에, filter와의 연산이 오른쪽 그림처럼 보이실 수 있는데, centering의 개념으로 이해하시기보다는 챕터3 강의노트 7페이지와 같이 연산이 이루어지고, padding이 있는 경우 input data의 index가 shift된다고 보시면 좋을 것 같습니다.

filter size > (padding*2+1)인 경우 input보다 output의 dimension이 작아질 뿐 불가능한 것은 아닙니다"
4/3/24 18:58,TA Youngmin,Every activation function evaluation per element should be counted as a single evaluation in problem5.
4/3/24 19:02,화나서 방방 뛰는 튜브,저도 궁금합니다
4/3/24 19:04,화난 라이언,제가 이해하기로는 연산이 recursive한 걸 더 명확히 보여주려고 하신 것 같습니다.
4/3/24 19:04,.,아하 이해했습니다. 감사합니다!
4/3/24 19:37,눈빛 애교 어피치,재제출이 2번 7번만 열려있는데 원래 전부 열리는데 조금 걸리나요?
4/3/24 19:39,열심히 일하는 네오,Hw4 submission에서 제가 받은 성적말고 가능한 성적(total)이 14가 아니라 0으로 돼있는데 오류죠?
4/3/24 19:41,기지개 고양이,저도 그래요. 오류인거 같네요
4/3/24 19:43,TA Jihoon,해당 부분 오류가 있던 것 같아 업데이트 예정입니다 감사합니다!
4/3/24 19:56,TA Yongin,HW4 redo submission 가능하도록 업데이트 되었습니다. 
4/3/24 19:57,TA Yongin,"말씀하신 ‘가능한 성적’의 경우 14점이 맞아 원인을 찾고 있습니다만, 우선 채점 결과에는 영향이 없는 것으로 보입니다."
4/3/24 20:13,TA Jihoon,hw4 총점 etl 상에서 0->14점 만점으로 변경 완료되었습니다
4/3/24 20:33,열심히 일하는 네오,감사합니다!
4/3/24 20:40,눈빛 애교 어피치,감사합니다~
4/3/24 20:46,애교뿜뿜 무지,"제가 chapter2 슬라이드를 2024버전으로 업데이트 되기 전꺼로 받아서 필기를 해뒀는데, 혹시 내용이 업데이트된 부분이 있는지 아시는 분 계신가요? 잘 안 보여서요"
4/3/24 20:58,TA Hyojun,Chapter2 슬라이드는 바뀐 부분이 거의 없는 것 같습니다. 전 2022버전으로 보고 있는데 사소한 오탈자는 모르겠지만 내용은 거의 동일한 것 같네요.
4/3/24 21:19,애교뿜뿜 무지,답변 감사합니다!!
4/4/24 8:58,청소하는 튜브,hw5 마지막 문제에서 test error 0.9로 수렴하시는 분 계신가요?? 뭔가 randomized label이어도 잘 추론하는 것 같은데 혹시 몰라서 CIFAR10으로 하니까 원 논문처럼 0.9로 수렴하네요. 혹시 MNIST dataset에서 test error 0.9 나오신 분 계신가요?
4/4/24 9:49,씩씩거리는 무지,씩씩거리는 무지님이 나갔습니다.
4/4/24 10:07,비옷입은 튜브,what's the difference between kernel and filter?
4/4/24 10:54,애교뿜뿜 무지,사진
4/4/24 10:59,애교뿜뿜 무지,"안녕하세요. 여기서 MLE와 Entropy가 같은 것이 직관적으로는 이해가 되는데, 수식이 헷갈려서 질문드립니다. P(X1,…,Xn)=1/N[#X_i=1… #X_i=k]가 의미하는 것이 혹시 무엇인가요? 그리고 저 수식이 같아지는걸 어떻게 보이면 좋을지 궁금합니다"
4/4/24 11:02,옐로카드 프로도,P는 실제 label값이 형성되어있던 비율을 나타내는 것 같아요
4/4/24 11:03,옐로카드 프로도,"그 비율과 비슷하도록 모델을 짜는 것이 목표이고, 그래서 모델의 분포와 실제분포의 cross entropy(H라고 되어있는부분)를 minimize하려는 것이고"
4/4/24 11:04,옐로카드 프로도,그게 kl divergence를 minimize하는것과 같습니다
4/4/24 12:53,눈물바다에 빠진 라이언,사진
4/4/24 12:54,눈물바다에 빠진 라이언,혹시 problem7에 그래프 이렇게 나오신 분 계신가요..? hint랑 비슷하긴 한데 뭔가 미묘하게 다르네요..
4/4/24 13:01,하트뿅뿅 라이언,똑같은거 아닌가요? scale은 나눠주는 값에 따라 다를거 같아요 len(dataloader) or total # of data
4/4/24 13:08,청소하는 튜브,저정도면 잘 나온거같아요
4/4/24 13:11,눈물바다에 빠진 라이언,넵 다들 감사합니다!
4/4/24 13:35,TA Hyojun,"For CNN, kernel and filter have the same meaning."
4/4/24 13:37,으쓱으쓱 어피치,혹시 4/10일날 수업 있나요?
4/4/24 13:49,애교뿜뿜 무지,사진
4/4/24 13:49,애교뿜뿜 무지,여기서 혹시 첫번째 -> 두번째로 넘어갈 때 p(P(Y_i))= 1/N 인건가요?? 1/N이 어떻게 나온건지 궁금합닏
4/4/24 13:50,.,minimize이기 때문에 n은 있어도 없어도 상관없습니다
4/4/24 13:52,애교뿜뿜 무지,"아하 그렇군요!! 그렇다면 혹시 두번째 식에 log뿐인데, 앞에 붙어야 할 확률에 관한 부분은 어떻게 되는걸까요?"
4/4/24 14:07,멋쟁이 프로도,저게 레이블이 정해져있어서 KL-divergence 계산할 때 P는 one-hot 아니던가요?
4/4/24 14:17,TA chaeju,"맞습니다. 따라서 두 번째 식으로 전개되지요
\mu_{Y_i}(f_{A,b}(X_i))가 어떤 식인지 정의를 찾아보시면 될 것 같습니다"
4/4/24 14:22,애교뿜뿜 무지,"아하 그러면 Y_i에 해당하는 부분만 앞이 1이, 나머지는 0이 붙어서 사라졌다고 생각하면 되겠군요. 감사합니다!!"
4/4/24 14:37,Apeach in love,"hw5-4애서 각 c= 1, 2, 3,
m, n= 1, •••, 224
로 인덱싱하면 되나요?"
4/4/24 15:48,TA Hyojun,네 파이썬처럼 0-based indexing을 하셔도 되고 말씀하신 대로 1~244로 indexing하셔도 됩니다.
4/4/24 23:22,애교뿜뿜 무지,안녕하세요. 여기서 혹시 첫 번째 등호가 왜 성립하는지 궁금합니다.
4/4/24 23:22,애교뿜뿜 무지,사진
4/4/24 23:23,청소하는 튜브,Conditional expectation에서 성질입니다
4/4/24 23:26,애교뿜뿜 무지,"yi^~가 x에 independent 하지 않아 보이는데, 그것과 관계없이 성립하는걸까요??"
4/4/24 23:26,기지개 고양이,네네
4/4/24 23:27,기지개 고양이,사진
4/4/24 23:27,기지개 고양이,https://analysisbugs.tistory.com/m/8
4/4/24 23:28,애교뿜뿜 무지,"헉 이런게 있군요,, 감사합니다!!"
4/4/24 23:29,기지개 고양이,단학기 수리통계 / 수리통계1 에서 다룹니다
4/5/24 10:50,눈물 흘리는 제이지,만약 전체 데이터개수가 많지않다면 batch size를 크게 잡지 않는 것이 좋은것 같은데 이유가 직관적으로 잘 와닿지 않습니다 이부분 설명해주실수 있나요?
4/5/24 12:28,열심히 일하는 네오,삭제된 메시지입니다.
4/5/24 13:14,눈물 흘리는 제이지,"bias correction이 의미하는 바가 무엇인지도 궁금합니다. Adam에서 bias correction이 없다면 m1=g, m2=g*g랑 같아지기에 momentum으로서의 의미가 사라지기에 이를 보정해 주는것인가요?"
4/5/24 14:16,TA Hyojun,batch size가 크면 한 번에 많은 연산을 GPU에 올릴 수 있어 GPU를 더 효율적으로 돌릴 수 있습니다. 다음 블로그 글도 참고하시면 도움이 될 것 같습니다. (https://jimmy-ai.tistory.com/372)
4/5/24 14:17,TA Hyojun,"m1와 m2가 각각 gk의 1st/2nd momentum의 estimate인데, gk = g인 이상적인 경우를 생각하더라도 (1-beta^k) 항이 앞에 붙어 m1, m2가 g, g^2과 (특히 학습 초반에서) 다른 값을 갖게 됩니다. 따라서 1-beta^k로 나누어서 값을 보정해 준다고 생각하시면 됩니다. "
4/5/24 14:28,밥줘,사진
4/5/24 14:28,밥줘,문제가 뭘까요..
4/5/24 14:29,청소하는 튜브,랜덤하게 레이블이 달린 데이터셋이 들어가고 있는지 확인해보셔야할 거 같네요
4/5/24 14:29,청소하는 튜브,저렇게까지 빨리 수렴할 수가 없어서 아마 그냥 원래 레이블 그래도 들어가고 있을 확률이 높아보입니다
4/5/24 14:41,TA Gyeongmin,"덧붙이자면, 일반적으로 메모리가 허용하는 선에서 batch size를 최대로 늘리는 게 convention이긴 합니다. 다만 batch size가 클 경우 local minima에서 잘 벗어나지 못한다는 연구도 있습니다.
효준 조교님께서 올려주신 링크에도 마지막에 관련 설명이 있는데, 이에 대해 더 궁금하시면
https://openreview.net/pdf?id=H1oyRlYgg
논문 참고하셔도 좋을것같습니다."
4/5/24 14:43,옐로카드 프로도,페이지를 찾을 수 없다고 나옵니다 조교님!
4/5/24 14:44,TA Gyeongmin,https://arxiv.org/abs/1609.04836
4/5/24 14:44,TA Gyeongmin,이 링크 쓰시면 될 것 같습니다
4/5/24 15:32,밥줘,사랑합니다.
4/5/24 17:29,손을 번쩍 든 무지,혹시 6번 문제에서 training의 속도가 느리거나 진척 없는 분들 계신가요? train_test_split 기능 써가지고 train set의 크기를 줄여도 이 모양이네요..
4/5/24 17:30,손을 번쩍 든 무지,"일단 epoch는 넘어가지는 상황인데, 한 epoch마다 3~5분 정도 걸립니다."
4/5/24 17:30,눈물 흘리는 제이지,"강의록 70p에서 E(y^2) 유도 과정에서 Exp(A^2*x^2)=Exp(A^2)*Exp(x^2)-cov(A^2,x^2) 이걸 이용해서 유도해도 괜찮을까요"
4/5/24 17:33,눈물 흘리는 제이지,아니면 저 경우에서 A와 x가 correlation이 있어도 성립하는것인가요
4/5/24 18:05,신난 어피치,삭제된 메시지입니다.
4/5/24 18:06,신난 어피치,6번 앞에서부터 6000개 잘라서 모델에 넣었는데 Hint보다 좀 느리게 수렴하네요...
4/5/24 18:12,APT,APT님이 나갔습니다.
4/5/24 19:31,신난 어피치,사진
4/5/24 19:31,신난 어피치,이런 식으로 나왔습니다
4/5/24 19:50,부끄러워하는 라이언,부끄러워하는 라이언님이 나갔습니다.
4/5/24 20:02,.,"in #5, is relu also applied after maxpool?"
4/5/24 20:08,손을 번쩍 든 무지,사진
4/5/24 20:09,손을 번쩍 든 무지,"I don't think so, at least in problem 5 case."
4/5/24 20:17,하트뿅뿅 라이언,"5번 문제에서, 마지막 연산에 대해 naive에서는 max pool만 취해 채널 수가 256인채로 유지되고 bottleneck에서는 max pool 이후 conv로 채널수를 64로 바꾸는데, 이렇게 하면 최종 채널 수가 달라져 정확한 비교가 안되지 않나요?"
4/5/24 22:45,기지개 고양이,사진
4/5/24 22:46,기지개 고양이,"전체적 경향은 비슷한데, 분산과 수렴지점이 다른채로 나와서"
4/5/24 22:46,기지개 고양이,뭐가 문제일까요
4/5/24 22:46,기지개 고양이,솔직히 랜덤 라벨에도 오버피팅 가능한걸 보였으니 과제 문제를 푼거는 맞는데
4/5/24 22:46,화난 라이언,음 아마 에포크마다 찍은게 아니라 배치마다 찍으신 것 같은데요
4/5/24 22:46,기지개 고양이,아 그렇네요 감사합니다.
4/6/24 0:23,부끄러운 어피치,부끄러운 어피치님이 나갔습니다.
4/6/24 5:56,렐?루,사진
4/6/24 5:56,렐?루,혹시 이건 어떤 에런지 아시는 분 있나요
4/6/24 5:56,렐?루,사진
4/6/24 5:57,렐?루,AlexNet().to(device)에서 코드가 뻗어버렸어요..
4/6/24 7:53,마이크를 든 라이언,저는 렌덤라벨링을 1~10으로 하니 저런 에러가 뜨고 0~9로 하니 없어졌어요
4/6/24 9:19,신난 어피치,이번 과제 3번 손풀이인가요!
4/6/24 9:19,신난 어피치,?
4/6/24 9:21,기지개 고양이,저는 그리 했어요
4/6/24 10:13,신난 어피치,감사합니다!
4/6/24 11:02,청소하는 튜브,LeCun Initialization에서 왜 \tilde{y}의 기댓값을 따질 때 굳이 conditional expectation으로 바꾸는 것인가요?
4/6/24 13:29,TA Hyojun,"A, b와 달리 x는 실제 데이터의 분포 혹은 이전 레이어를 거치면서 나오는 값이기 때문에 x에 대한 확률분포를 정확히 알 수 없어 x에 대한 conditional expectation으로 바꿔 식을 전개한 것 같습니다."
4/6/24 13:31,TA Hyojun,다시 보니까 2022 버전에는 LeCun initialization에서 tilde y의 평균과 분산을 증명하는 슬라이드가 없는 것 같습니다. 해당 내용은 2024년 슬라이드를 보고 확인하시면 될 것 같습니다.
4/6/24 14:39,청소하는 튜브,사진
4/6/24 14:40,청소하는 튜브,그렇다면 이 식에서 A_ij와 b_i의 conditional expecation이 0인 이유는 가장 바깥 쪽 기댓값과 합쳐져서 \sum_j E[A_ij]x_j + E[b_i]가 되었다고 이해하면 될까요
4/6/24 15:19,쑥스럽게 인사하는 프로도,혹시 다음주에도 과제가 있나요?
4/6/24 16:37,애교뿜뿜 무지,사진
4/6/24 16:37,애교뿜뿜 무지,"안녕하세요. Hw1 solution 관련 질문이 있습니다. 혹시 마지막에 ""Note that the top eigvenvector equal to an (n-1)-dimensional set"" 이 문구가 적혀 있는 이유가 무엇인지 궁금합니다 (풀이에 저 특성이 사용되는걸까요?)"
4/6/24 16:39,치즈케이크,n-1 dim set에서 initial value가 시작하지 않으면 무조건 알고리즘이 발산한다고 주장하는 것입니다
4/6/24 16:39,치즈케이크,그런데 n차원 공간에서 n-1 dim 공간을 정확히 찍을 확률이 0이므로 알고리즘이 거의 항상 발산하는 거구오
4/6/24 16:45,애교뿜뿜 무지,"감사합니다! 설명해 주셔서 이해는 얼추 된 것 같은데, 방금 하신 말씀이 저 ""Note that the top eigvenvector equal to an (n-1)-dimensional set""안에 내제되어 있어서 저 문구를 보면 말씀하신대로 떠올려지는 자명한(?) 것일까요?"
4/6/24 16:49,치즈케이크,Remark 한번 읽어보세요
4/6/24 17:09,TA Hyojun,"A_ij와 b_i가 각각 mean=0인 gaussian으로 initialize 되어있어 각각의 conditional expectation이 0이 됩니다. 제가 놓친 부분이 있을 수도 있겠지만, 지금은 A, b와 x가 independent하다고 가정하기 때문에 꼭 저렇게 식을 전개할 필요는 없을 것 같습니다. 다만, A_ij와 b_i에 대해서만 먼저 계산한다는 점이 해당 방식으로 식을 전개했을 때 잘 드러나고, Ch 5에서는 저런 형태로 조건부확률이 많이 사용되기 때문에 이와 같은 식 전개에 익숙해지는 것도 좋을 것 같습니다."
4/6/24 17:10,TA Hyojun,정확한 과제 일정은 잘 모르겠네요 ㅠㅠ 다만 학기 중에 과제는 총 12번 정도 나갈 예정입니다.
4/6/24 17:59,기타치는 튜브,2022년 과제 표시된 기일을 보니 매주 나오는 게 맞는 것 같습니다 ..
4/6/24 18:20,기지개 고양이,4월 22일 전까지 Mysnu-수강강좌에 있는 어떤 버튼을 누르면
4/6/24 18:20,기지개 고양이,과제는 물론이고 시험도 안봐도 된다네요
4/6/24 18:20,밥줘,Drop out ㄷㄷ
4/6/24 19:25,기타치는 튜브,사진
4/6/24 19:26,기타치는 튜브,drop_out=131
4/6/24 19:32,열심히 일하는 네오,"hw2 problem 4 solution에 궁금한 게 있어 질문드립니다! 아래 부등식의 우변, 특히 {} 안 부분이 왜 0이 되는지 잘 이해가 되지 않습니다 ㅜㅜ"
4/6/24 19:33,열심히 일하는 네오,사진
4/6/24 19:33,애교뿜뿜 무지,위에 x3 정의를 보시면 될 것 같습니다!
4/6/24 19:35,열심히 일하는 네오,헉 그러네요 감사합니다 !!!
4/6/24 20:41,열심히 일하는 네오,HW5 Submission 제출함은 아직 생성이 안 된거죠?
4/6/24 20:43,General Trash,과제 탭에 들어가시면 있습니다!
4/6/24 20:43,TA chaeju,방금 모듈에도 추가해놓았습니다.
4/6/24 20:43,열심히 일하는 네오,감사합니다!
4/6/24 23:57,하트뿅뿅 라이언,저도 이게 헷갈리네요
4/7/24 0:59,TA Hyojun,"말씀하신 부분도 생각해 볼 수 있겠지만, 해당 부분을 감안하더라도 전체 연산량과 parameter 개수 차이가 크게 날 것입니다. 3x3과 5x5 conv의 연산량과 parameter 개수를 1x1 conv layer가 얼마나 줄일 수 있는지에 주목하시면 좋을 것 같습니다.
(반대로, inception model에서 1x1 conv을 빼면 얼마나 model이 무거워지는지로 생각하셔도 좋을 듯합니다)"
4/7/24 1:12,화난 라이언,데이터 6000개로 줄여도 Epoch 1번에 몇 분 씩 걸리는데 왜 이러는걸까요...
4/7/24 1:12,밥줘,cpu로 돌리시나요??
4/7/24 1:13,신난 어피치,그거 gpu 안쓰면 몇분씩 걸리더라고요
4/7/24 1:13,화난 라이언,Images.to(device) 하는거 아닌가요..?
4/7/24 1:16,밥줘,노트북에 엔비디아 gpu 없으면 그냥 cpu로 계산할걸요
4/7/24 1:17,눈물바다에 빠진 라이언,코랩이시면 런타임에서 런타임 유형 변경하셔야 합니다
4/7/24 1:17,열심히 일하는 네오,코랩으로 해보세용
4/7/24 1:17,화나서 방방 뛰는 튜브,노트북용 4060기준 약 500초 걸려요
4/7/24 1:17,신난 어피치,그거 런타임 변경하셔야 합니다
4/7/24 1:18,신난 어피치,전 코랩으로 했는데 맨처음에 매 epoch마다 6000개 데이터로 평가하는 걸로 짰더니 gpu로 해도 31분걸렸어요
4/7/24 1:18,하트뿅뿅 라이언,그거 배치로 나눠주기만 해도
4/7/24 1:18,하트뿅뿅 라이언,확 줍니다 시간
4/7/24 1:18,눈물바다에 빠진 라이언,Learning rate을 0.1로 했더니 loss가 nan으로 나와서 0.05나 0.01은 잘 되는데 구현이 잘못된 걸까요?
4/7/24 1:47,기지개 고양이,Loss가 float 데이터형 최댓값을 넘어서 그런거 같아요
4/7/24 11:30,기타치는 튜브,"Dataloader 매개변수에서 batch_size=64 설정했는데도 시간이 너무 오래걸리네요 ㅠㅠ epoch 당 40-50초 가까이 걸려서 한 시간씩 걸리는데 혹시 다른 이유가 있을까요?
cuda 사용하는 것도 확인했고 jupyter notebook에서 사용하는데 잘 안되네요.."
4/7/24 11:33,기지개 고양이,그
4/7/24 11:34,기지개 고양이,코랩에서 런타임>런타임 유형 변경>하드웨어 가속기
4/7/24 11:34,기지개 고양이,코랩 사용하시는거 맞나요? 그냥  anaconda의 Jupyter notebook이 아니라
4/7/24 11:35,기타치는 튜브,코랩이 아니라 노트북 자체 anaconda에서 노트북 gpu로 하고 있는데 단순한 성능 문제일까요?
4/7/24 11:37,.,저는 자체 GPU가 구려서 Colab 제공 T4 썼는데 5초에 한 epoch 나왔어요
4/7/24 11:37,기지개 고양이,코랩이 훨씬 빠른거 같아요
4/7/24 11:37,기타치는 튜브,감사합니다
4/7/24 11:38,기지개 고양이,집에 A100 이런게 있지 않는 이상
4/7/24 11:40,마이크를 든 라이언,삭제된 메시지입니다.
4/7/24 11:43,으쓱으쓱 어피치,한 3060부터 t4 이긴다고 알고있긴 합니다
4/7/24 11:53,애교뿜뿜 무지,삭제된 메시지입니다.
4/7/24 11:53,애교뿜뿜 무지,여러분 혹시 6번에서 10%만 데이터 사용할 때 어떻게 하는 것이 권장되나요? 일단 TestLoader를 사용하고 나면 인덱싱을 어떻게 할지 곤란해서 random.sample 방식으로 해봤는데 images.to & labels.to 할 때 images & labels가 그냥 int형으로 불러와져서 안 되고 이 부분 빼도 shape 에러가 나네요 (mat1 and mat2 shapes cannot be multiplied (256x25 and 6400x800)) 어떻게 해결을 모색하는 것이 좋을까요?
4/7/24 11:54,화난 라이언,Dataloader 객체를 만들기 전에 dataset에서 처리하는게 좋을 것 같아요
4/7/24 11:55,애교뿜뿜 무지,sample한 뒤에 Dataloader를 만들어 기존처럼 하는게 확실히 고민을 줄이겠군요 감사합니다
4/7/24 12:32,애교뿜뿜 무지,"RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

이런 에러가 uniform한 index를 넣어 리스트를 다시 만들고 하니 나는데 혹시 왜 일어나는 것인가요? floating-point error라고 colab AI가 추정하며 labels.long() 형태로 바꾸라고 해서 해봤는데 잘 안되고 있습니다

참고로 testloader 넣기 전 데이터를 고쳤으며 기존에 되던 데이터로 바꾸어도 같은 에러로 작동이 되지 않습니다"
4/7/24 12:39,말썽쟁이 네오,인덱스가 안 넘어가서 그런가
4/7/24 15:11,벌 서는 라이언,"HW5 5번 문제에서 Multiplicational operation incurred due to zero-padding도 포함해서 세나요, 아니면 이건 Clarification에 안 적혀있으니 제외하고 세나요?"
4/7/24 15:59,애교뿜뿜 무지,사진
4/7/24 16:00,애교뿜뿜 무지,"안녕하세요. Hw4 답안 관련 질문드립니다.  y_l/b_l의 경우엔 j=i가 아닌 경우 전부 0인 반면 y_l/y_l-1의 경우에는 j=i가 아니더라도 항상 위와 같은 결과가 나오는 것으로 이해했는데, 차이가 왜 생기는지 이해가 잘 되지 않습니다"
4/7/24 16:01,애교뿜뿜 무지,아래의 경우에도 j=i가 아닌 경우엔 결국 0이 되지 않나요?
4/7/24 16:06,치즈케이크,"bias의 경우는 덧셈만 이루어지므로 같은 row component에만 의존하지만, y_l-1의 경우 A_l과 곱해지면서 y_l의 각 component마다 y_l-1의 모든 component에 의한 dependency가 생기는 것으로 받아들였습니다."
4/7/24 17:12,애교뿜뿜 무지,최종 결과식에서 i=j가 아닌 곳이 0이 된다고 생각해서 헷갈렸었늗네 계산해 보니 0이 되지 않는군요..! 감사합니다
4/7/24 18:27,TA Jongchan,HW3 redo has been graded.
4/7/24 19:06,권투하는 무지,"hw5 1번 질문드립니다
option3의 for문 내 주석에서의 ell은 L-1부터 0까지 돌아가니 실제 index에서는 ell+1이 되어야 하는 것 맞나요?"
4/7/24 19:35,초롱초롱 튜브,"hw5 4번 질문드립니다.
receptive field를 설명할 때 인덱스를 명시해서 답을 기재해야 하나요? (ex_ i-2:i+2…) 아니면 전반적인 설명만 적어도 될까요?"
4/7/24 19:40,애교뿜뿜 무지,맞는것 같습니다
4/7/24 19:41,손을 번쩍 든 무지,저는 전자로 했던것같습니다.
4/7/24 21:20,부탁하는 네오,"HW4 prob6에서 제가 맞게 이해한 건지 확인하고 싶습니다
스칼라를 행렬로 미분한 거를 정의할 때 1처럼 정의하는 것이고, 문제에서 A_L은 1 x n_{L-1}이므로 i=1, j=1,2,...,n_{L-1}이어서 2는 1 x n_{L-1}, 즉 row vector가 되는 것인가요?
y=MX+N에서 y를 X로 미분하면 M, M으로 미분하면 X^T가 된다 이런 식으로 이해해도 되는 것인지요?
column이랑 row랑 자꾸 헷갈립니다..ㅜㅜ"
4/7/24 21:20,부탁하는 네오,사진
4/7/24 21:21,부탁하는 네오,"그리고 야코비안은 R^m->R^n 같은 경우에 정의하는 걸로 아는데, b 문제의 첫 문장처럼 행렬에서 벡터로 가는 경우는 일반적으로 미분을 정의할 수 없는 것인가요?"
4/7/24 21:26,화난 라이언,"보통 벡터는 열벡터 기반이라고 기억하시는게 좋을 것 같고, 하나의 팁은 미분하려는 벡터를 적으면 어떤 요소가 미분되는지 눈에 잘 들어옵니다. 예를 들면 y_l(m*1 vector)를 열벡터로 적고 해당 위치의 요소를 미분하려는 벡터 순서대로 오른쪽으로 적으면 됩니다"
4/7/24 21:27,화난 라이언,스칼라의 경우 1*1이니까 그냥 오른쪽으로 순서대로 적으면 행벡터의 결과가 나옵니다
4/7/24 21:28,손을 번쩍 든 무지,삭제된 메시지입니다.
4/7/24 21:28,화난 라이언,이것도 3차원 행렬로 확장해서 계산할 수는 있을 것 같습니다
4/7/24 21:28,손을 번쩍 든 무지,이 경우는 2d 텐서에서 1d 텐서(=벡터)로 가는 경우라 미분을 정의하는 것이 안될거에요 아마
4/7/24 21:29,손을 번쩍 든 무지,아 아닌가
4/7/24 21:29,치즈케이크,"reshape 잘 해주면 행렬로 쓸 순 있겠죠.
그런데 (익숙함 문제일지는 몰라도) 행렬과 벡터의 곱처럼 명확히 보이지는 않을 것 같습니다"
4/7/24 21:29,손을 번쩍 든 무지,저까지 헷갈리네요 ㅠ
4/7/24 21:30,화난 라이언,네 저도 그렇게 생각합니다
4/7/24 21:31,부탁하는 네오,넵 다들 감사합니다
4/7/24 21:42,부탁하는 네오,"같은문제에서 추가질문이 있습니다
그림 1번은 R^{n_l}->R^{n_L} 변환이므로 야코비안이 잘 정의되는 것으로 보이는데, 2번에서 y_l은 n_l x 1 vector이고 A_l은 n_l x n_{l-1} matrix인데 미분이 어떻게 정의되나요?
또한 스칼라를 행렬로 미분한 것을 문제에서 정의한 대로 한다는 것은 알겠는데, 그걸 3번처럼 인덱스를 저렇게 쪼개서 쓰는 게 잘 이해가 안 됩니다.."
4/7/24 21:42,부탁하는 네오,사진
4/7/24 21:56,기타치는 튜브,ij 인덱스가 생략되어 있는 것 아닐까요
4/7/24 22:00,General Trash,'3차원 행렬' 형태로 정의된다고 생각하시면 될것같습니다.
4/7/24 22:01,말썽쟁이 네오,A_l 미분은 문제에 나와있어요
4/7/24 22:17,애교뿜뿜 무지,애교뿜뿜 무지님이 나갔습니다.
4/7/24 23:36,권투하는 무지,사진
4/7/24 23:36,권투하는 무지,이거 어떻게 해결하면 좋을까요... 그냥 import torch 지난번까진 잘 됐었는데 이러네요...
4/7/24 23:38,Piepie,"pip install 다시 해보시거나, 챗지피티에 검색하는 것을 추천드립니다."
4/7/24 23:39,권투하는 무지,둘다 해봤는데 해결이 안됩니다... uninstall 후 재설치도 해봤는데 uninstall이 아예 안되네요
4/7/24 23:41,권투하는 무지,사진
4/7/24 23:41,권투하는 무지,설치는 되어있는듯 합니다
4/7/24 23:44,기지개 고양이,한번 주피터 노트북 다 껐다가 다시 켜보시는건 어떨까요
4/7/24 23:44,기지개 고양이,저도 똑같이 그랬고 삭제 재설치해도 안됐는데 껐다 키니 돼서
4/7/24 23:50,권투하는 무지,노트북 자체를 껐다 켜봐도 안되네요...
4/7/24 23:54,부끄러워하는 라이언,6번 문제 training 시간 어느정도 걸리나요? 문제에서 시키는대로 10%인 6000개까지만 사용하였는데 한시간이 넘도록 결과가 안나오면 잘못된 코드일까요?
4/7/24 23:55,건방진 제이지,저는 다섯 시간 걸려서 잘 나왔습니다
4/7/24 23:57,치맥하는 제이지,저도 한 3시간 걸려서 제대로 나왔어요...
4/7/24 23:58,.,저도 4시간정도 걸렸습니다...
4/8/24 0:00,DL하는 블루,저는 코랩 T4 GPU 썼더니 8분 정도 걸렸습니다
4/8/24 0:00,DL하는 블루,로컬에 그래픽카드 스펙이 낮으면 코랩 써보시는 거 추천드려요
4/8/24 0:00,부끄러워하는 라이언,아.. 10%로 조정 안하면 90분 걸린다고 써있길래 그보다 빨리 되길 기대했는데 일단 더 기다려봐야겠네요
4/8/24 0:01,부끄러워하는 라이언,참고해보겠습니다!
4/8/24 0:09,눈물바다에 빠진 라이언,저는 10%만 사용하고 코랩 V100으로 하니 3분 정도에 끝났습니다
4/8/24 0:41,nnd,nnd님이 나갔습니다.
4/8/24 1:05,열심히 일하는 네오,사진
4/8/24 1:06,열심히 일하는 네오,"plot이 이런 식으로 전혀 training이 안 되는데, 혹시 어떤 문제가 있는 걸까요.."
4/8/24 1:14,밥줘,"loss랑 accuracy를 잘못 구한거 아니에요??
training은 코드에 다 구현 돼있어서.."
4/8/24 1:15,눈물바다에 빠진 라이언,아니면 혹시 label을 epoch마다 랜덤하게 바꾸신건 아닌가요?
4/8/24 1:15,열심히 일하는 네오,헉 맞는 거 같습니다..
4/8/24 9:33,애교뿜뿜 무지,"안녕하세요. Weight w를 명시해야 하는 문제 관련 질문이 있습니다. 만약 output shape이 예를 들어 Cout*m*n이라면 filter가 총 Cout개 있는데, 이를 각각 적게 되면 w의 shape이 Cout*Cin*m*n 이런식으로 되어야 할 것 같은데 과제의 솔루션에서는 Cout이 1이거나, 혹은 Cout개의 filter가 전부 같은 모양이라 3차원으로 명시되고 있는 것일까요??"
4/8/24 9:33,애교뿜뿜 무지,사진
4/8/24 9:59,멋쟁이 프로도,풀링의 연산이 컨볼루션과 완전히 똑같지는 않아서 형식적인 차이는 있을수 있는거같습니다
4/8/24 11:41,치맥하는 제이지,HW5 4번 혹시 general 한 경우에 대해서 구하면 되는건가요 아니면 layer 모서리에서 원본 이미지랑 padding의 영향을 모두 받는 경우를 따로 나눠서 계산해야 되는건가요?
4/8/24 11:43,화나서 방방 뛰는 튜브,"저는 padding 생각해서 했는데 경우 안 나누고 max, min으로 쉽게 할 수 있습니다"
4/8/24 11:46,치맥하는 제이지,아 그런 방법도 있군요.. 참고하겟습니다
4/8/24 13:41,부탁하는 네오,패딩이 예시코드에서 명시되어있는거 같은데 그거대로 하셨나ㅓ요?
4/8/24 13:48,부탁하는 네오,아 그 얘기가 아니었군요
4/8/24 14:14,애교뿜뿜 무지,5번에서 concatenate 결과가 두 모듈에 대해 다른 것 맞나요? size가
4/8/24 14:15,화나서 방방 뛰는 튜브,저는 그렇게 생각하고 했습니다
4/8/24 15:09,JX,Do anyone know the grading system for S/U? I emailed our professor but he doesn’t reply. 
4/8/24 16:29,Apeach enjoys music,"Unfortunately, the deadline for changing to S/U grading is over and I also missed the deadline...😭"
4/8/24 16:54,애교뿜뿜 무지,"Convolution의 경우 stride, filter size, padding의 pair가 valid하지 않은 경우 성립하지 않는 것으로 이해했는데, MaxPooling의 경우 filter, stride 등에 의해 딱 맞아 떨어지지 않더라도 마지막에 남아있는 칸들을 활용해 연산을 하는 것 같던데 아시는 분 있으실까요? 

(Ex: kernel size=3이고 stride=2인데 input size는 3*64*64, 2칸씩 밀고 가다 보면 마지막에 3*3이 온전히 남지 않지만, 있는 것만으로 계산)"
4/8/24 16:54,신난 어피치,혹시 청강생은 시험을 응시할 수 없나요..?
4/8/24 16:56,피자 먹다 자는 무지,교수님께서 따로 허용하시지 않는 이상 보통은 그렇습니다
4/8/24 16:56,신난 어피치,앗 그렇군요
4/8/24 17:04,건배하는 프로도,HW6은 언제 올라오나요?
4/8/24 17:05,권투하는 무지,제발요...
4/8/24 17:05,인사하는 프로도,과제 레이트 미제출은 그냥 0점처리 되나요?
4/8/24 17:07,TA chaeju,"네, eTL의 late submission 관련 공지를 참고해주세요."
4/8/24 17:31,JX,Actually I changed before the deadline but I dont know the grading way...
4/8/24 17:59,Ernest Lee,저희 시험 이번 주 토요일 맞죠 ..?
4/8/24 17:59,화나서 방방 뛰는 튜브,네
4/8/24 18:01,Ernest Lee,ㅜ
4/8/24 18:19,.,시험은 그동안이랑 동일하게 오픈 북 형식인가요?
4/8/24 18:19,하트뽀뽀 어피치,Ch1~3 다 아직 안 나갔는데 시험범위 오늘 한데까진가요?
4/8/24 18:20,엄지척 프로도,과제6은 22일까지인가요?
4/8/24 18:26,Apeach enjoys music,I've just asked and the prof said there won't be any hw due next Mon but he's not sure about the deadline of the hw6
4/8/24 18:45,TA Jaewook,"Yes, the professor said at the beginning of today's class that the exam scope will be things covered up to today"
4/8/24 18:48,화난 라이언,"In this exam, do we have the problems about python code e.g. filling the blanks in given code?"
4/8/24 18:49,TA Jaewook,"yes, it is open-book in the sense announced on the class website"
4/8/24 18:58,Luca [루카],Past exams have questions in this format so seems reasonable to expect
4/8/24 20:01,멋쟁이 프로도,멋쟁이 프로도님이 나갔습니다.
4/8/24 21:22,건배하는 프로도,오늘 강의 영상은 언제쯤 업로드 되나요?
4/8/24 21:43,Apeach enjoys music,"In hw4 pb7 lenet c3 layer implementation, I was wondering the reason why torch.cat is done along dim=1. Could anybody please explain this?"
4/8/24 21:47,치즈케이크,"one should concatenate along the features,"
4/8/24 21:47,치즈케이크,which in default is stored in the second dimension(dim =1)
4/9/24 0:54,Apeach enjoys music,Thanks a lot!
4/9/24 11:51,눈빛 애교 어피치,"안녕하세요, 혹시 HW5 답안은 언제 올라올까요?"
4/9/24 12:02,TA Jisun,파일이 비공개로 올라갔었네요. 답안 공개하였습니다.
4/9/24 12:07,화난 라이언,Where does the exam take place?
4/9/24 14:00,좌절하는 제이지,"안녕하세요, 혹시 어제 강의 영상은 언제 올라올까요?"
4/9/24 14:59,TA Jaewook,"It will be one of the classrooms on the first or third floor in the same building, and the room numbers for each student are planned to be announced"
4/9/24 15:01,TA Jaewook,It is uploaded in the eTL now.
4/9/24 15:08,.,"안녕하세요, CPU와 GPU 사이의 데이터 이동에 대한 질문이 있습니다.
아래 Chapter 2 코드에서 test_loader for loop를 돌 때 test_loss의 계산이나 correct의 계산의 경우 .item() 메소드를 통해 자동으로 GPU로부터 CPU로 데이터가 이동된다고 하셨는데요,
total의 경우 labels가 GPU에 있는 텐서이지만 .item() 없이도 CPU에서 출력이 가능한 이유는 마찬가지로 .size(0)이 같은 역할을 해주어서 그런건가요?"
4/9/24 15:08,.,"[Chapter 2 코드]
for images, labels in test_loader :
    images, labels = images.to(device), labels.to(device)

    output = model(images)
    test_loss += loss_function(output, labels).item() 
    pred = output.max(1, keepdim=True)[1]
    correct += pred.eq(labels.view_as(pred)).sum().item()
    
    total += labels.size(0)
            
print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss /total, correct, total,
        100. * correct / total))"
4/9/24 15:16,TA chaeju,"저도 방금 검색으로 알아낸 것이긴 한데, .item()이나 .size(0)처럼, plain python number(int, float 등)을 출력하는 함수들은 자동으로 데이터를 CPU로 이동시켜 준다고 합니다."
4/9/24 15:16,TA chaeju,https://pytorch.org/docs/stable/generated/torch.Tensor.item.html
4/9/24 15:16,TA chaeju,사진
4/9/24 15:17,TA chaeju,".size() method 또한, .size(0)처럼 dimension을 specify했을 때 int를 반환하므로, 같은 역할을 하고 있으리라 예상됩니다"
4/9/24 15:25,.,답변 감사합니다!
4/9/24 16:02,.,"Data augmentation에 대한 질문입니다.
Train_loader에서 데이터를 불러올 때 random data augmentation is applied라고 교수님께서 설명을 해주셨는데요, 
그러면 epoch가 변할 때 DA 역시 랜덤하게 다시 적용되는 걸까요, 
아니면 맨 처음에 dataset 전체에 대해서 랜덤 DA를 적용한 후, epoch가 변할 땐 DA 없이 train_loader의 shuffle만 적용하는 걸까요?
직관적으로 생각해보면 epoch마다 데이터셋이 변화하면 안될 것 같아 후자 같은데, random DA의 계산이 정확히 언제 발생하는지 헷갈려서 질문 드렸습니다."
4/9/24 16:08,치즈케이크,Label만 정확히 보존되면 같은 image에 최대한 다양한 변형을 가하는 게 generalization에 도움이 될 것 같은데 그러면 epoch 단위로 같은 set이 아니어도 상관없지 않을까요?
4/9/24 16:08,치즈케이크,python - Data Augmentation in PyTorch - Stack Overflow - https://stackoverflow.com/questions/51677788/data-augmentation-in-pytorch
4/9/24 16:08,치즈케이크,첫 번째 답변의 댓글이 관련있는 것 같습니다
4/9/24 16:11,.,"아 그렇네요
챕터3 ppt 23p에서 neural network never encounters the exact same data again이라는 문장도, worsens the training error라는 표현만 봐도 epoch와 관계 없이 train_loader에서 데이터를 불러올 때마다 transform이 일어나야겠네요
감사합니다!"
4/9/24 16:15,하트뿅뿅 라이언,하트뿅뿅 라이언님이 나갔습니다.
4/9/24 18:56,TA Juhyeok,"이번 주 목요일 (4.11) 기존에 제가 맡는 Office Hour 시간인 19:00 ~ 20:00이 시험 시간과 겹쳐서 이번 주 해당 Office Hour은 금요일 (4.12) 동시간대인 19:00 ~ 20:00에 진행하겠습니다. 다음 주부터는 다시 정상적으로 목요일에 진행될 예정입니다. 감사합니다.

My TA Office Hour session this week, originally planned to be held at 19:00 ~ 20:00 this Thursday (4.11), will be moved to the same time (19:00 ~ 20:00) on Friday (4.12) due to my exam schedule. From next week it will again be held on Thursday as usual. Thank you."
4/9/24 19:06,N이 되고픈 엡실론,N이 되고픈 엡실론님이 나갔습니다.
4/9/24 21:21,.,"what does ""batch direction"" (axis=0) mean when concatenating branches in the googLenet code?"
4/10/24 1:42,음료수 마시는 어피치,사진
4/10/24 1:42,음료수 마시는 어피치,"안녕하세요, 2021 midterm problem 6번과 관련된 문제입니다. 이미지에서 보듯 마지막 줄까지 갔는데 어떻게 진행해야 할지 모르겠습니다. 여러 가지 방법을 시도해 봤지만 계속 아무 소용이 없어요 ㅠㅠ 어떻게 해야 푸는지 아시는 분 있을까요? 감사합니다!"
4/10/24 1:44,애교뿜뿜 무지,strong induction으로 시도해보세요
4/10/24 1:46,애교뿜뿜 무지,그후 theta_k끼리 같다는 식을 전개해서 theta_k 외에 다른 인자들간의 관계식을 찾아야 합니다
4/10/24 1:56,아이스크림 든 네오,"식 정리한 것에서 볼 수 있듯이 psi^{k+1}-psi{k} = - alpha*m^{k+1} 임을 보이면 충분합니다. 
강귀납을 사용하신 후 좌변에 psi 정의식을 k+1, k에 대해 사용하면 다음 식이 나옵니다. 이 식을 v와 m의 정의식을 사용하여 식변환하면 됩니다.
 theta^{k+1}-theta^{k} - alpha*(g^{k}-g^{k-1})=  -alpha*v^{k+1} - alpha*(g^{k}-g^{k-1}) = -alpha*beta*m^{k} - alpha*g^{k-1} - alpha*(g^{k}- g^{k-1} = -alpha*(beta*m^{k} + g^{k}) = -alpha*m^{k+1} 로 식을 바꾸면 풀 수 있습니다."
4/10/24 1:56,제이지,제이지님이 나갔습니다.
4/10/24 2:38,음료수 마시는 어피치,늦은 시간에 도와주신 두 분께 감사합니다!
4/10/24 13:53,눈물 흘리는 제이지,"minibatch SGD w/o replacement에서

for _ in range(K*N//B):

    indices = np.random.choice(np.arange(N), size = B, replace = True)

    theta -= alpha/B*np.sum([X[indices[i],:]@theta - Y[indices[i]]*X[indices[i],:] for i in range(B)])

라는 코드는 Chapter 1 코드에 적힌

for _ in range(K*N//B):

    grad = np.zeros(p)

    for _ in range(B):

        ind = np.random.randint(N)

        grad += (X[ind,:]@theta-Y[ind])*X[ind,:]

    theta -= alpha*grad

와는 다르게 왜 이상하게 작동하는지 궁금합니다"
4/10/24 14:02,TA chaeju,"[X[indices[i],:]@theta - Y[indices[i]]*X[indices[i],:] 에서 괄호 실수가 있는 것 같네요.
([X[indices[i],:]@theta - Y[indices[i]])*X[indices[i],:] 가 되어야겠습니다."
4/10/24 14:51,눈물 흘리는 제이지,"말씀하신 대로(X[indices[i],:]@theta - Y[indices[i]]) 부분에 괄호를 붙여서

for _ in range(K*N//B):

    indices = np.random.choice(np.arange(N), size = B, replace = True)

    theta -= alpha/B*np.sum(
[(X[indices[i],:]@theta - Y[indices[i]])*X[indices[i],:] for i in range(B)]
)
와 같이 하니까 덜 이상해지긴 했는데 그래도 여전히 이상하네요.."
4/10/24 14:52,눈물 흘리는 제이지,사진
4/10/24 14:54,TA chaeju,/B도 지우셔야 원래 코드와 동일해질 것 같습니다.
4/10/24 14:58,눈물 흘리는 제이지,"for _ in range(K*N//B):

    indices = np.random.choice(np.arange(N), size = B, replace = True)

    theta -= alpha*np.sum(
[(X[indices[i],:]@theta - Y[indices[i]])*X[indices[i],:] for i in range(B)]
)"
4/10/24 14:58,눈물 흘리는 제이지,사진
4/10/24 14:58,눈물 흘리는 제이지,이렇게 나옵니다..
4/10/24 15:01,TA chaeju,np.sum의 axis를 지정해주지 않았네요
4/10/24 15:02,TA chaeju,axis를 지정해주지 않으면 모든 element를 다 더한 값을 return해줍니다.
4/10/24 15:03,TA chaeju,"np.sum([…], axis=0)으로 돌려보세요"
4/10/24 15:04,눈물 흘리는 제이지,ㅠㅠ정말감사합니다
4/10/24 15:04,눈물 흘리는 제이지,됐습니다
4/10/24 15:27,귀여운 라이언,시험 볼 때 ppt 인쇄물 가져가도 되는 건가요?
4/10/24 15:28,General Trash,전자기기를 사용하지 않는 선에서 오픈북이라고 하니 가능할 듯합니다.
4/10/24 15:32,라면먹는 제이지,혹시 시험 장소는 공지 됐나요?
4/10/24 15:41,TA Jisun,"1. Yes, you may bring printed ppt slides.
2. Exam location will be announced in eTL before the exam. "
4/10/24 17:23,열심히 일하는 네오,"Hw4 problem3에 관한 질문입니다! 밑줄 친 X_i,j가 arbitrary하다는 조건이 problem2,3 풀이에 전부 사용되었는데 혹시 특별한 의미가 있는 걸까요?"
4/10/24 17:23,열심히 일하는 네오,사진
4/10/24 17:24,General Trash,계수비교를 통해 w의 값을 정하려면 주어진 등식이 X에 대한 항등식이어야 하기 때문에 주어진 조건인 것 같습니다.
4/10/24 18:46,음료 마시는 어피치,"for k in range(K) :
    i_k = np.random.randint(N)
    theta = theta - alpha * (f_th(theta,X[i_k]) - Y[i_k]) * diff_f_th(theta, X[i_k])
    if (k+1)%2000 == 0 :
        plt.plot(xx,f_th(theta, xx),label=f'Learned Fn after {k+1} iterations' HW2-7 sol에서 i_k를 랜덤으로해서 사용한 이유가 무엇인가요? ㅠㅠ"
4/10/24 19:06,벌 서는 라이언,Gradient Descent가 아니라 Stochastic Gradient Descent를 이용하니까 모든 경우에 대해 다 계산해 평균을 내는 것이 아니라 하나를 랜덤하게 뽑아 gradient를 계산했던 것 같습니다.
4/10/24 19:09,기지개 고양이,"성적 부여 방식이 절대평가라고 알고 있는데, 절대평가 점수 커트라인 등을 알 수 있을까요?"
4/10/24 19:10,기지개 고양이,너무나 중요한 정보인데 이 부분이 공시됐으면 좋겠습니다.
4/10/24 19:15,인사하는 제이지,삭제된 메시지입니다.
4/10/24 19:15,아침햇살,수리과학부 수업이 보통 절대평가가 많은데 대부분 결과적으로 상대평가처럼 되긴 합니다 사전에 공지한 수업은 거의 보지 못한 것 같습니다
4/10/24 19:16,기지개 고양이,"아하 그렇군요, 제가 해당 부분을 잘 몰랐네요. 유연한 상대평가라고 받아들이면 되겠군요"
4/10/24 20:37,택배 상자를 든 네오,"안녕하세요! HW5 P1의 솔루션에서 질문이 있습니다.
솔루션을 보면 함수 S안에 A@y+b 로 표현이 되어있는데,
HW4P4에서의 dy_l / db_l 을 보면 sigma' 함수 안에 A_l@y_{l-1} + b_l 로 표현이 되어있습니다. 코드에서 따로 인덱스를 수정하지 않는 이유가 무엇인지 궁금합니다.

y의 리스트 첫번째 원소가 X_data이고 그 이후로 y값들이 append되었기 때문이라고 봐도 되는걸까요?"
4/10/24 20:47,청소하는 튜브,네 코드에서 y가 y_{l-1}에 대응되도록 만들어졋어요
4/10/24 20:48,택배 상자를 든 네오,아하 감사합니다~!
4/10/24 21:31,옐로카드 프로도,"안녕하세요! HW5 P6의 sol code를 돌려보는 중인데, 코랩 기준으로 얼마나 소요되는지 아시는 분이 계실까요?"
4/10/24 21:47,열심히 일하는 네오,"혹시 밑줄 친 부분에서 왜 i_2, j_2 -> 2i_2, 2j_2로 두배가 되는 걸까요?"
4/10/24 21:47,열심히 일하는 네오,사진
4/10/24 21:48,옐로카드 프로도,"maxpool이 2X2 인 4칸을 하나로 압축시키는 형태이니 반대방향으로 생각하면 행,열 각각 두개씩 잡아야 합니다!"
4/10/24 21:49,열심히 일하는 네오,아! 이해됐습니다 감사합니다!!!
4/10/24 22:15,베개를 부비적대는 라이언,시험이 4시간인데 혹시 먹을 거 가져가서 먹어도 되나요
4/10/24 22:22,엄지척 튜브,학생증을 분실했는데 혹시 민증으로 갈음할 수 있을까요?
4/10/24 22:25,TA Jaewook,"네, 모든 형태의 가진을 포함한 신분증(민증, 운전면허증, 여권 등)이 가능합니다. 학생증이 신분증으로서 기능할 수 있는지 질문이 들어올까봐 참고사항으로 학생증 역시 가능하다는 맥락으로 적은것입니다."
4/10/24 22:42,엄지척 튜브,감사합니다!
4/11/24 9:15,권투하는 무지,저희 시험에서 계산기는 필요 없나요?
4/11/24 11:20,택배 상자를 든 네오,안녕하세요! 학점교류로 수업듣고 있는 타대생인데 시험 어느 강의실로 가면 될까요?
4/11/24 11:46,TA Jisun,"네, 소리가 크게 나지 않고 냄새가 많이 나지 않는 (다른 분들께 방해가 되지 않는) 선에서 음식 섭취하셔도 괜찮습니다.
Yes, you may bring food that does not interrupt other students in terms of sound and smell."
4/11/24 12:04,옐로카드 프로도,"batch normalization 관련 질문이 있습니다. beta gamma 또한 learned parameter로 사용한다고 하셨는데, 굳이 고정시키지 않고 따로 학습시키는 이유가 있을까요?"
4/11/24 12:07,치즈케이크,문제에 맞는 데이터 정규화를 찾으려는 노력이라고 생각했습니다
4/11/24 12:09,옐로카드 프로도,뭔가 hyper parameter로 따로 빼서 학습시킬 수도 있지 않나 싶어서요.. 너무 오래걸리려나요 그러면
4/11/24 12:26,TA Jisun,"네 계산기는 시험장에 들고 오실 수 없습니다.
Calculators are not allowed in the exam."
4/11/24 13:46,부탁하는 무지,사진
4/11/24 13:46,부탁하는 무지,ㅗㅎㄱ시 이거 왜 b_i가 아니라 b인가요?
4/11/24 14:16,TA Jaewook,I think it is a typo.
4/11/24 15:00,권투하는 무지,청강생도 중간고사 응시 가능한가요?
4/11/24 15:41,TA Jisun,삭제된 메시지입니다.
4/11/24 15:41,TA Jisun,"청강생은 중간고사 응시 불가합니다.
Auditors are not allowed to take exams."
4/11/24 15:42,TA Jisun,타대생 분들도 소속 학과에 맞추어 가시면 됩니다.
4/11/24 15:44,택배 상자를 든 네오,네 감사합니다
4/11/24 16:10,애교뿜뿜 무지,"안녕하세요. NiN 슬라이드 마지막에 why is this equivalent to 3x3 - 1x1 - 1x1 conv라고 나와있는 것이 무엇을 의미하는지 헷갈려 질문드립니다. 

fXf conv - linear - linear의 구조가 3x3-1x1-1x1와 동일, 즉 1x1 conv가 linear와 동일함을 묻는 것인지 

아니면 f를 무엇으로 설정하던 3x3으로 결국 표현 가능한지를 묻는 것인지 this가 무엇이며 뭐가 equivalent 한지 헷갈려서 마지막 문장을 어떻게 해석하면 좋을까요?"
4/11/24 16:10,애교뿜뿜 무지,사진
4/11/24 16:41,부끄러워하는 라이언,시험 때 답안을 영어로만 작성해야 한다는 조건이 있을까요?
4/11/24 16:59,부탁하는 네오,과제5가 12점 만점 맞나요? 이티엘 상에서 만점이 0점으로 표시되는 것 같습니다(점수는 제대로 나오는데 ‘총 몇점’이 0점으로 나와요)
4/11/24 16:59,애교뿜뿜 무지,6문항이라서 12점 만점인 것 같습니다
4/11/24 17:01,하트뿅뿅 라이언,삭제된 메시지입니다.
4/11/24 17:07,말썽쟁이 네오,첫번째가 맞는 것 같아요
4/11/24 20:22,경제학도,"HW5의 4번에서 X, y의 indexing을 padding 영역까지 포함해서 padding 영역 내에 (0, 0)이 위치하는 것이 아닌, padding 전 본래의 layer에서(0, 0)을 시작해야 하나요? 전자는 완전히 틀렸다고 봐야 하나요?"
4/11/24 21:48,음료수 마시는 어피치,"혹시 2022, 2021 midterm 답안지 받을 수 있을까요?"
4/11/24 22:41,TA Hyojun,기출 문제에 대한 솔루션은 따로 제공되지 않았던 것 같습니다
4/11/24 22:42,TA Hyojun,해당 문제는 output의 특정 성분을 결정하는 input(X)의 성분들을 찾는 문제이므로 결과를 나타낼 때 padding은 제외하는 것이 자연스러울 것 같습니다.
4/11/24 23:28,건배하는 프로도,23년 기출은 못올려주시나요?
4/11/24 23:28,벌 서는 라이언,23년에 교수님 연구년이셨던 걸로 알고 있습니다.
4/11/24 23:31,건배하는 프로도,아 그렇군요 감사합니다
4/12/24 9:14,경제학도,"답변 감사합니다! 혹시 그렇다면 X channel의 index를 설정할 때 c = 0, 1, 2가 아닌 solution처럼 c = 1, 2, 3인 이유를 여쭈어봐도 될까요? 다른 w, h 차원의 경우 0부터 index가 시작된 것 같아 여쭈어 보고자 합니다."
4/12/24 9:32,ㅈㅅㅇ,"안녕하세요, 기출문제를 풀어보니까 파라미터 개수를 계산하는 문제가 있더라고요. 손으로 계산 못 할 정도는 아니긴 한데... 계산기를 쓸 필요가 없는 문제만 출제될 예정인가요?"
4/12/24 9:42,ㅈㅅㅇ,계산실수로 감점 안 한다는 문구가 아래에 있었네요...죄송합니다
4/12/24 9:53,양손 엄지척 무지,혹시 손으로 코딩하는 문제도 시험에 나오나요? 나온다면 python 문법적인 부분도 엄격하게 채점하는지 궁금합니다.
4/12/24 10:20,애교뿜뿜 무지,"BN for linear Layers와 BN for convolutional layers 질문드립니다. 각각 independent over neurons / channels라 적혀있는데, 

Linear layer 에서 mu[:]는 각 원소별로, Conv layer에서 mu[:]는 각 input 채널별로 계산되는게 맞게 이해한 것일까요?"
4/12/24 10:22,애교뿜뿜 무지,사진
4/12/24 10:22,애교뿜뿜 무지,BN for conv에서 mu의 shape이 Cin인지를 여쭤본 것입니다!
4/12/24 10:24,빈털터리 제이지,"RMSProp과 Adam 관련 질문 드립니다. (m_1)^0 과 (m_2)^0 (m1, m2의 초기값)은 0과 같은 특정값으로 따로 설정하는 것인지 moment의 정의에 따라 g_0, (g_0)^2로 간주하는 것인지 궁금합니다. 관련 문제들에서 초기값에 대한 언급이 없어 어떻게 생각해야할지 모르겠어서 질문드립니다!"
4/12/24 10:27,청소하는 튜브,"X[b,:,i,j]의 길이가 C_in만큼이니까 맞지 않을까요..?"
4/12/24 10:45,TA Youngmin,Yes. This is correct. Additionally it is reasonable to normalize outputs in a channelwise manner since same filter weights are shared.
4/12/24 10:53,눈빛 애교 어피치,Hw4 1번인데 이거 혹시 답변이 있었나요? 왼쪽 padding에 대한 조건은 왜 없는건가요?
4/12/24 11:02,TA Gyeongmin,RMSProp Adam 둘 모두에서 m은 0으로 initialize됩니다.
4/12/24 11:57,으쓱으쓱 어피치,시험을 볼 때 과제에서 이미 증명한 내용을 사용해도 되나요?
4/12/24 11:58,열심히 일하는 네오,저도 이게 궁금합니다!
4/12/24 12:01,하트뿅뿅 라이언,족보 확인하세요
4/12/24 12:02,TA Jaewook,제 생각에 out of bound 의 값을 별도로 채우는 부분은 문제에서 새로 소개하는 finite difference에 해당하는 부분이라 0에 대한 언급이 없는 것 같습니다.
4/12/24 12:05,TA Jaewook,"convolution에 대한 padding은 이미 소개가 됐던 개념이라 추가적인 설명이 없는것 같고요.
그리고 convolution에 대한 padding은 다들 말씀하신것처럼 왼쪽과 위쪽도 0으로 채우는 것이 맞습니다"
4/12/24 12:33,TA Hyojun,문법 실수의 경중에 따라 사소한 감점이 있을 수도 있습니다. 수업 시간과 과제에서 다룬 코드들도 출력하여 답안 작성하실 때 참고하시면 도움이 될 것 같습니다.
4/12/24 12:35,TA Hyojun,제가 생각하기엔 별 다른 이유는 없는 것 같습니다. 풀이를 작성하실 때도 문제에 따로 지시 사항이 없는 경우 각 dimension에 대해 indexing을 일관적으로 사용하기만 한다면 시작점(0-based / 1-based)을 어떻게 잡는지는 크게 상관없습니다.
4/12/24 13:11,빈털터리 제이지,"혹시 시험에서 vector로 미분했을 때 분자중심표현 미분을 쓰나요, 분모중심표현 미분을 쓰나요? (ex: scalar을 nx1 vector로 미분한 결과값은 nx1인가요 1xn 인가요?)
 Hw1 #1은 분모중심 미분을 사용하고, Hw4#6(a)는 분자중심표현 미분을 사용해서 헷갈리는 것 같습니다."
4/12/24 13:11,음료 마시는 어피치,HW5-6 sol에서 step1에 random_label을 지정해서 각 데이터마다 0~9까지 레이블을 지정해주었는데 이 코드의 역할은 무엇인가요..? 각 이미지에 숫자를 배당하는 것인가요?
4/12/24 13:12,애교뿜뿜 무지,문제가 애초에 randomized label을 부여해도 학습이 되는지 확인하는 것이라서 임의로 레이블을 지정한 것입니다
4/12/24 13:13,청소하는 튜브,저도 그게 헷갈렸는데 Clarification이 없다면 분자중심표현으로 쭉 하는게 맞을 거 같습니다
4/12/24 13:13,음료 마시는 어피치,착각했었네요.. 감사합니다!!
4/12/24 13:18,Tube cleaning,분자중심표현이 jacobian matrix와 같은 표기인거죠? 감사합니다ㅎㅎ
4/12/24 13:19,벌 서는 라이언,벌 서는 라이언님이 나갔습니다.
4/12/24 13:46,.,"gradient가 붙으면 분모중심 미분,
그냥 편미분이면 디폴트는 분자중심 미분이라 생각했는데 맞을까요?
Hw1 #1은 gradient가 붙어있고
Hw4 #6(a)는 그냥 벡터 편미분이라 차이가 있지 않나 유추했습니다"
4/12/24 13:52,.,"2022 midterm 7번에 관한 질문입니다.
HW5 #4번처럼 receptive field를 구해 nonzero elements를 추적하고자 했습니다.
이때 HW5 #4의 경우 maxpool2d에서 f=2 s=2라서 2i와 2i+1을 비교했지만, 이 상황에선 f=3, s=2라서 2i, 2i+1, 2i+2를 비교해주었습니다.
또한 ceil_mode=True이고 패딩은 없는 상황입니다. 이때 문서를 찾아보니 MaxPool2d에서 ceil_mode=True면 시작 위치가 left padding이거나 input이면 그냥 가는거고 right padding이면 스탑이라고 하길래, 32x32 이미지에서 풀링하면
0 based이므로 0~31 중에서
0, 2, 4, …, 30 기준으로 찾고
0은 0,1,2를 비교해서 0으로 매핑,
30은 오른쪽에 index 32인 부분은 0이라 생각하고 비교한 뒤에 15로 매핑되어 최종 결과는 0~15인 16x16 이미지가 나온다고 생각했습니다.
이에따라 b c d번 문제에 맞게 계산해주었고, e를 구하기 위해 nonzero elements를 추적하다보니 out2까지는 X와 X tilda에 적용되는 연산이 동일하고 그저 평행이동일 뿐이었지만, X tilda의 경우 마지막 3x3 conv에서 X와는 다르게 맨 아랫줄 conv 적용이 되지 않는 것을 확인했습니다. 따라서 global avg pool에서 약간의 차이가 존재한다고 생각해서 exactly equal은 아니라고 생각했습니다.
혹시 이 과정에서 오류가 있는지 봐주실 수 있으실까요?"
4/12/24 13:52,.,사진
4/12/24 13:52,.,https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html
4/12/24 13:53,.,사진
4/12/24 13:56,.,"그리고 직접 코드로 구현해서 돌려보니 nonzero elements의 index는 값이 작아 가끔 누락되는 부분을 제외하고는 맞게 나왔지만,
out2의 경우 X와 X tilda가 평행이동된 값이 아니라 아예 다른 값이 나와 이 부분에 대해서도 궁금증이 있습니다."
4/12/24 13:57,.,"import torch
import torch.nn as nn
from torch.optim import Optimizer
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import transforms


device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

X = torch.zeros(3, 32, 32)
X_tilda = torch.zeros(3, 32, 32)

# X 텐서에 값 할당
X[0, 18, 18] = 1

# X_tilda 텐서에 값 할당
X_tilda[0, 22, 18] = 1

'''
Step 2
'''
class NiN(nn.Module) :
    def __init__(self) :
        super(NiN, self).__init__()

        self.mlpconv_layer1 = nn.Sequential(
                nn.Conv2d(3, 192, kernel_size=5, padding=2, bias=False),    # 192 * 32 * 32
                nn.ReLU(),
                nn.Conv2d(192, 160, kernel_size=1, bias=False),             # 160 * 32 * 32
                nn.ReLU(),
                nn.Conv2d(160, 96, kernel_size=1, bias=False),              # 96 * 32 * 32
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),# 96 * 16 * 16
                nn.Dropout()
                )
        self.mlpconv_layer2 = nn.Sequential(
                nn.Conv2d(96, 192, kernel_size=5, padding=2, bias=False),   # 192 * 16 * 16
                nn.ReLU(),
                nn.Conv2d(192, 192, kernel_size=1, bias=False),             # 192 * 16 * 16
                nn.ReLU(),
                nn.Conv2d(192, 192, kernel_size=1, bias=False),             # 192 * 16 * 16
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode = True),# 192 * 8 * 8
                nn.Dropout()
                )
        self.mlpconv_layer3 = nn.Sequential(
                nn.Conv2d(192, 192, kernel_size=3, padding=1, bias=False),  # 192 * 8 * 8
                nn.ReLU(),
                nn.Conv2d(192, 192, kernel_size=1, bias=False),             # 192 * 8 * 8
                nn.ReLU(),
                nn.Conv2d(192, 10, kernel_size=1, bias=False),              # 10 * 8 * 8
                nn.ReLU()                
                )


    def forward(self, x) :
        output = self.mlpconv_layer1(x)
        nonzero_indices = torch.nonzero(output[0])
        print(""nonzero indices for out1"")
        for index in nonzero_indices:
            print(index)
        output = self.mlpconv_layer2(output)
        nonzero_indices = torch.nonzero(output[0])
        print(""nonzero indices for out2"")
        for index in nonzero_indices:
            print(index)
        print(""out2[0]: "", output[0])
        output = self.mlpconv_layer3(output)
        nonzero_indices = torch.nonzero(output[0])
        print(""nonzero indices for out3"")
        for index in nonzero_indices:
            print(index)
        output = nn.AvgPool2d(kernel_size = 8)(output)
        output = output.view(-1, 10)
        print(""out4: "", output)
        print(""label: "", torch.argmax(output))
        return output


'''
Step 3
'''
model = NiN().to(device)
X, X_tilda = X.to(device), X_tilda.to(device)
print(""X: "")
outX = model(X)
print(""\n\n\nX_tilda: "")
outX_tilda = model(X_tilda)"
4/12/24 13:58,.,코드 구현
4/12/24 13:58,.,"X: 
nonzero indices for out1
tensor([7, 7])
tensor([7, 8])
tensor([8, 7])
tensor([8, 8])
tensor([8, 9])
tensor([9, 8])
tensor([9, 9])
tensor([ 9, 10])
tensor([10,  9])
tensor([10, 10])
nonzero indices for out2
tensor([2, 4])
tensor([2, 5])
tensor([2, 6])
tensor([3, 2])
tensor([3, 3])
tensor([3, 4])
tensor([3, 5])
tensor([3, 6])
tensor([4, 3])
tensor([4, 4])
tensor([5, 2])
tensor([5, 3])
tensor([5, 5])
tensor([6, 2])
tensor([6, 4])
tensor([6, 6])
out2[0]:  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0010, 0.0002, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0010, 0.0010, 0.0019, 0.0019, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0019, 0.0023, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011, 0.0013, 0.0000, 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002, 0.0000, 0.0006, 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],
       grad_fn=<SelectBackward0>)
nonzero indices for out3
tensor([1, 7])
tensor([4, 5])
tensor([5, 3])
tensor([5, 5])
tensor([6, 1])
tensor([6, 2])
tensor([6, 3])
tensor([6, 6])
tensor([6, 7])
tensor([7, 2])
tensor([7, 3])
tensor([7, 4])
tensor([7, 5])
tensor([7, 6])
out4:  tensor([[5.5411e-06, 6.1209e-06, 3.3443e-06, 6.2764e-06, 6.1866e-05, 0.0000e+00,
         5.8710e-06, 5.2706e-05, 1.2660e-05, 2.4360e-07]],
       grad_fn=<ViewBackward0>)
label:  tensor(4)



X_tilda: 
nonzero indices for out1
tensor([9, 7])
tensor([9, 8])
tensor([9, 9])
tensor([10,  7])
tensor([10,  8])
tensor([10,  9])
tensor([12,  9])
nonzero indices for out2
tensor([3, 3])
tensor([4, 2])
tensor([4, 3])
tensor([6, 3])
tensor([6, 5])
tensor([6, 6])
tensor([7, 3])
tensor([7, 4])
tensor([7, 6])
out2[0]:  tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0002, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016, 0.0030, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0018, 0.0000, 0.0010, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0004, 0.0004, 0.0000, 0.0002, 0.0000]],
       grad_fn=<SelectBackward0>)
nonzero indices for out3
tensor([3, 7])
tensor([4, 1])
tensor([4, 3])
tensor([5, 2])
tensor([5, 3])
tensor([5, 4])
tensor([6, 1])
tensor([6, 4])
tensor([6, 5])
tensor([7, 3])
tensor([7, 4])
out4:  tensor([[8.1067e-06, 4.8756e-06, 1.6780e-06, 1.0662e-05, 5.6174e-05, 0.0000e+00,
         6.5254e-06, 5.5880e-05, 1.1186e-05, 0.0000e+00]],
       grad_fn=<ViewBackward0>)
label:  tensor(4)"
4/12/24 13:58,.,출력 결과
4/12/24 13:58,.,혹시 코드 구현에 문제가 있다면 이 부분도 알려주시면 감사하겠습니다!
4/12/24 14:28,.,"*0 based indexing에 오류가 있었습니다!
X[1, 19, 19]를 1로 고쳐 맞게 설정해주었습니다.
그럼에도 out2까지 X와 X tilda가 평행이동이라는 내용은 유지 되어야할텐데, 여전히 다른 값이 출력되어 이 부분이 아직 궁금합니다."
4/12/24 14:29,소심한 네오,model.eval()해야하지않을까요
4/12/24 14:45,.,Dropout을 놓쳤네요 감사합니다!
4/12/24 14:47,소심한 네오,소심한 네오님이 들어왔습니다.
4/12/24 14:52,.,"다시 해보니 예상한 결과대로 out2까지는 같고 X_tilda의 3x3 conv에서 마지막 줄이 누락되네요 
답변 감사합니다!"
4/12/24 15:43,먹보 네오,사진
4/12/24 15:44,먹보 네오,혹시 이 슬라이드에서 2'에 df/df * df/dc 가 나오는 이유가 뭔가요..?
4/12/24 15:45,춤추는 튜브,마지막 c b의 오타라고 하셨던거 같습니다
4/12/24 15:45,먹보 네오,감사합니다!!
4/12/24 15:53,먹보 네오,"위의 backprop에서 df/dc, dc/db, db/da, da/dx, db/dy 등은 그냥 symbolic differentiation 으로 계산하는건가요? (흑흑)"
4/12/24 15:55,DL하는 블루,DL하는 블루님이 나갔습니다.
4/12/24 16:15,리듬타는 제이지,"In the exam, should we regard the gradient as a (1 by n) vector or a (n by 1) vector?"
4/12/24 16:21,부탁하는 네오,"시험에서 연습지를 제공해주시는지 궁금합니다!
기출을 보니 여백이 있긴 한데 조금 부족할 수도 있겠다는 생각이 듭니다
만약 제공하지 않는다면 개인 연습지를 들고 가도 될까요?(어차피 오픈북이므로 외부 종이 반입은 허용되니 괜찮을지 궁금합니다)"
4/12/24 16:22,음료 마시는 어피치,"22 기출에 통계관련 문제가 있던데, 기본적인 통계 선수지식이 필요할까요?"
4/12/24 16:46,경제학도,답변 감사드립니다!
4/12/24 18:03,부탁하는 네오,여기서 저 편미분한거를 row vector로 보는 건가요? 인덱싱을 저렇게 생각하는 게 맞을까요?
4/12/24 18:03,부탁하는 네오,사진
4/12/24 18:05,Tube cleaning,그런 것 같아요!
4/12/24 18:06,부탁하는 네오,행렬곱이 성립하려면 저렇게 생각할 수밖에 없을 것 같은데 과제에서는 그래디언트를 column vector로 보는 걸 봤던 것 같아서.. 좀 헷갈리네요 ㅜㅜ
4/12/24 18:06,부탁하는 네오,감사합니다
4/12/24 18:13,Ernest Lee,hw5 1번에서 db = dy * S(A@y+b).T에서 마지막에 .T를 하는 이유가 먼가요?
4/12/24 18:56,Tube cleaning,"차원 맞춰주기 위해서인 것 같아요 저는 이렇게 했었는데 이래도 구현상은 상관 없는 것 같더라고요!
db = S(A@y+b)*dy
dA = db@y.T
dy = A.T@db"
4/12/24 19:31,파이팅하는 무지,파이팅하는 무지님이 들어왔습니다.
4/12/24 19:31,파이팅하는 무지,삭제된 메시지입니다.
4/12/24 19:33,파이팅하는 무지,파이팅하는 무지님이 나갔습니다.
4/12/24 20:06,TA Jaewook,한국어로 작성하셔도 됩니다
4/12/24 20:08,TA Jaewook,말씀하신대로 오픈북이니 외부 종이 반입이 허용되어 괜찮을 것 같습니다
4/12/24 20:16,부탁하는 네오,아 넵 그럼 연습지는 제공되지 않고 개인적으로 준비해가면 될까요?
4/12/24 20:43,애교뿜뿜 무지,수업시간/혹은 과제에서 증명한 것들은 From HW assignment ~~ 등으로 서술해도 괜찮을까요?
4/12/24 20:49,TA Jaewook,삭제된 메시지입니다.
4/12/24 20:49,TA Jaewook,"네 제공되지 않고, 그러시면 될것 같습니다. "
4/12/24 21:03,Apeach in love,저도 궁금합니다!
4/12/24 21:05,TA Jaewook,"채점기준과 관련한 질문은 오해의 소지가 있을 수 있어 답하기가 조심스러운데요,
보통 그 자체나 근본적으로 같은 사고과정을  물어보는 것이 아닌 경우 수업때 다룬 것은 가져다 써도 되는 것이 수학과 채점기준의 기조인 것 같습니다.
다만 어떤 것이 ""그 자체나 근본적으로 같은 사고과정""인지에 대해서는 스스로 잘 판단하셔야 할 것 같습니다."
4/12/24 21:08,Apeach in love,넵 감사합니다!
4/12/24 21:47,눈물바다에 빠진 라이언,"Dropout-σ와 σ-Dropout이 수업 내용에서는 σ이 ""nonnegative homogeneous""인 경우 equivalent하다고 했는데, nonnegative가 어디서 필요한 건지 궁금합니다! 실제로 증명할 때는 homogeneity만 있어도 괜찮을 것 같고, Leaky ReLU도 Dropout과 σ가 commutative한 activation function이라고 나와 있지만 nonnegative는 아니지 않나요?"
4/12/24 21:52,화난 라이언,nonnegative homogeneous function space가 더 커서 더 여유있는 조건을 준 것 같습니다
4/12/24 21:52,화나서 방방 뛰는 튜브,역은 성립 안하는거 아닐까요
4/12/24 21:52,화난 라이언,그리고 정의를 다시 참고하시면 ReLU는 homogeneous 하지 않은 것 같습니다
4/12/24 21:53,기지개 고양이,사진
4/12/24 21:54,기지개 고양이,사진
4/12/24 21:54,화난 라이언,아 반대로 말했군요
4/12/24 21:54,기지개 고양이,"2022년 중간고사 6번 (a) 문제 푸는데
E[y|y>0] = 0 이어야 주어진 정답의 결과가 나오는데,"
4/12/24 21:54,기지개 고양이,그렇게 생각할 근거가 없어 보이는데
4/12/24 21:55,기지개 고양이,혹시 제 접근 방식이 잘못된 걸까요?
4/12/24 21:58,빈털터리 제이지,두번째 등호가 틀렸어요
4/12/24 21:58,빈털터리 제이지,y>0일 확률을 곱해줘야죵
4/12/24 22:01,기지개 고양이,어 그부분은 문제 없지 않나요?
4/12/24 22:07,렐?루,VAR(y^2 | y>0) 이 1이 아니라 1-k^2이 되지 않나요
4/12/24 22:08,눈물바다에 빠진 라이언,이 말 듣고 생각났는데 nonnegative homogeneous가 nonnegative & homogeneous가 아니라 한 단어인 거 같네요. (i.e. f(ax)=af(x) where a≥0) 이러면 ReLU와 Leaky ReLU가 모두 nonnegative homogeneous라서 맞는 거 같아요!
4/12/24 22:08,부탁하는 무지,Tanh도 그럼 성립하는 거죠??
4/12/24 22:08,기지개 고양이,앗 그런가요  다시 보겠습니다
4/12/24 22:10,눈물바다에 빠진 라이언,tanh는 아닌 거 같아요.. 애초에 exp가 homogeneous가 아니니까요
4/12/24 22:11,말썽쟁이 네오,말썽쟁이 네오님이 나갔습니다.
4/12/24 22:12,부탁하는 무지,앗 그렇네요 착각했습니다 감사합니다!
4/12/24 22:24,으쓱으쓱 어피치,"inner E는 conditional expectation이고 밖은 그냥 expectation인데, 그걸 밖으로 뺄수 있을까요?? 전 E[x_j^2 | y]에서 y<=0일때 y>0일때 나눠서 안쪽 기댓값을 먼저 구했습니다 "
4/12/24 22:34,기지개 고양이,밖으로 뺀건 아니고 저도 값을 계산하기 위해
4/12/24 22:35,기지개 고양이,y를 y>0 y<0로 교집합이 없으며 합집합이 전체인 걸로 쪼개서
4/12/24 22:35,기지개 고양이,각각 구한 것이었습니다
4/12/24 22:35,기지개 고양이,아마 제가 뭔가 착각을 하고 있는 것 같습니다. 조금더 스스로 고민해보겠습니다. 도와주셔서 감사합니다
4/12/24 22:39,초롱초롱 어피치,사진
4/12/24 22:40,초롱초롱 어피치,삭제된 메시지입니다.
4/12/24 22:40,초롱초롱 어피치,혹시 저 빨간 부분에서 dropout 진행하는 방식이 96*16*16 element 각각에 대해 p_drop 확률로 dropout 시키는 건가요?
4/12/24 22:46,말썽쟁이 네오,Var가 y>0인 경우로 잘라도 1인가요?
4/12/24 22:50,기지개 고양이,아하...  그게 문제네요
4/12/24 22:51,기지개 고양이,감사합니다
4/12/24 22:52,기지개 고양이,E[(y-E[y|y>0])² | y>0] 으로 계산해야 되군요
4/12/24 22:54,말썽쟁이 네오,전 복잡해서 저런거 나오면 그냥 반 사라지니까 평균 반 준다 그렇게 적으려고요
4/12/24 22:56,애교뿜뿜 어피치,확률밀도함수를 이용하는 것도 좋아 보입니다
4/12/24 23:01,기지개 고양이,감사합니다!
4/12/24 23:16,부탁하는 무지,사진
4/12/24 23:17,부탁하는 무지,"늦은 시간에 죄송합니다.
이 문제 감이 안 와서 그런데 어떤 식으로 써내려가야 할까요..?"
4/12/24 23:18,눈물바다에 빠진 라이언,LeCun Initialization 유도하는 파트를 보시면 도움이 될 것 같습니다
4/12/24 23:19,기지개 고양이,사진
4/12/24 23:19,기지개 고양이,전 이렇게 했어요
4/12/24 23:19,부탁하는 무지,늦은 시간에 감사드립니다. 해보겠습니다!!
4/12/24 23:28,음료수 마시는 어피치,사진
4/12/24 23:28,음료수 마시는 어피치,안녕하세요 누군가 이 질문에 대한 solution을 공유해 주실 수 있으면 정말 감사하겠습니다!
4/12/24 23:29,Ernest Lee,"(a) 1/2, 0 (b) 0,0 나오지 않나요?"
4/12/24 23:30,권투하는 무지,varience가 0이 되진 않을 것 같은데요...?
4/12/24 23:31,Ernest Lee,아 잘못 썼네요 ㅋㅋ
4/12/24 23:31,기지개 고양이,Covariance 요
4/12/24 23:31,기지개 고양이,저거 (a)만 푼 페이지예요
4/12/24 23:31,Ernest Lee,분산 몇 나오시나요
4/12/24 23:33,기지개 고양이,(b)는 안풀어서 모르겠네요
4/12/24 23:34,기지개 고양이,(a)는 n_in σ_a² / 16 나와요
4/12/24 23:35,으쓱으쓱 어피치,E(x^2)이 1이 아닌 것 같습니다..!
4/12/24 23:36,힙합맨 제이지,곱해지는 계수가 5/64 되지 않나요?
4/12/24 23:37,기지개 고양이,다시 풀어봐야되겠네요 ㅠㅠ
4/12/24 23:38,애교뿜뿜 무지,5/64라는 숫자가 혹시 어떻게 나오는건가요..?
4/12/24 23:38,애교뿜뿜 무지,1/16아닌가요
4/12/24 23:39,권투하는 무지,사진
4/12/24 23:40,권투하는 무지,맞나요...?
4/12/24 23:40,힙합맨 제이지,저도 이렇게 풀었습니다
4/12/24 23:41,권투하는 무지,아 계수 2는 없어도 되겠네요
4/12/24 23:43,애교뿜뿜 무지,혹시 그러면 (b)는 계수가
4/12/24 23:43,애교뿜뿜 무지,어떻게 되나요..?
4/12/24 23:48,화나서 방방 뛰는 튜브,"저는 0, nσ^2/16로 나옵니다"
4/12/24 23:59,권투하는 무지,저도 그렇게 나오네요
4/13/24 0:21,부탁하는 무지,"22년 3번
""Convolution can represent identity""에 대한 문제는 Resnet 관련된 문제인가요?"
4/13/24 0:32,눈물바다에 빠진 라이언,굉장히 뜬금없지만 갑자기 헷갈려서 그런데 X[a:b]처럼 표현하면 파이썬 슬라이싱처럼 b는 포함 안하는 걸로 생각하면 될까요? 뭔가 예외가 있었던 거 같은데..
4/13/24 0:49,Ernest Lee,5/64 맞습니다
4/13/24 0:52,Ernest Lee,"b는 0, 1/16 * sigma^2 나오네요"
4/13/24 0:54,Ernest Lee,아뇨 그냥 X * w = X를 말합니다
4/13/24 0:54,Ernest Lee,포함 안 돼요
4/13/24 1:48,치맥하는 제이지,"지금까지 그냥 그런갑다 하고 있었는데, ch3 initialization 부분에서 왜 항상 mean 0 var 1로 맞추려고 하는 것인가요?"
4/13/24 1:53,Ernest Lee,자칫하면 variance가 다 감소하거나 폭발해서 그렇습니당
4/13/24 1:56,치맥하는 제이지,아하 그렇군요 늦은시간에 답변 감사합니다!
4/13/24 2:15,애교뿜뿜 무지,"혹시 Xavier initialization에서 forward & backward 결과의 ""harmonic"" mean을 init. 으로 가지고 시작하는 이유가 뭔가요? (혹은 증명)"
4/13/24 2:19,콘이 웃긴 무지,사진
4/13/24 2:20,콘이 웃긴 무지,혹시 여기 2번에서 df/dc인 이유가 무엇인가요?
4/13/24 2:38,인사하는 프로도,오 typo 같습니다
4/13/24 2:38,인사하는 프로도,df/db여야 할 것 같은데요..?
4/13/24 2:41,부탁하는 네오,.
4/13/24 2:44,콘이 웃긴 무지,감사합니다
4/13/24 8:04,옐로카드 프로도,혹시 저 과정에서 5/4가 왜 나오는지 알 수 있을까요??
4/13/24 8:06,화난 라이언,Var(X)=E(X²)-E(X)² 이지 않을까 생각해봅니다
4/13/24 8:07,옐로카드 프로도,그 5/4부분도 이 식을 바탕으로 전개되나요?
4/13/24 8:08,화난 라이언,네 1+(1/2)^2로부터 구해지는 것 같아서요!
4/13/24 8:09,옐로카드 프로도,혹시 저 aij랑 xj랑 독립이라는 것을 전제하고 이루어지는 건가요?
4/13/24 8:09,부탁하는 네오,"강의슬라이드에 있는 Var(XY)=Var(X)Var(Y)는 X,Y가 독립일뿐만 아니라 평균이 0이라는 조건이 필요한데 그래서 여기서는 그 공식을 적용 못하고 제평평제로 풀어야 하는거같아요"
4/13/24 8:12,멋쟁이 프로도,사실 저기에 (X-mu)(Y-mu)를 대입하면 공식이 나오긴 하죠
4/13/24 8:14,음료 마시는 어피치,혹시 제곱평균이 왜이렇게 되는지 간략하게 알려주실 수 있나요?
4/13/24 8:17,옐로카드 프로도,"aij, xj 가 독립이라면 E(aij2)E(xj2) 이므로 각각 구하신 것 같아요"
4/13/24 8:18,옐로카드 프로도,"그런데 제가 놓친 게 있는 것 같지만, aij와 xj가 독립이라고 할 수 있는 근거가 무엇인지 알 수 있을까요??"
4/13/24 8:23,기지개 고양이,x는 input 이고 a는 parameter이니 독립 아닐까요
4/13/24 8:27,옐로카드 프로도,딱히 언급된게 없으니까 그냥 독립이다라고 생각하는게 맞겠군요
4/13/24 8:28,음료 마시는 어피치,"감사합니다,,"
4/13/24 8:29,건배하는 프로도,혹시 학생회관 말고 인쇄 가능한 곳 있을까요…🥺
4/13/24 8:31,Muzi raising hand,중앙도서관 복사실 안 열었나요?
4/13/24 8:32,벌 서는 라이언,사회대 신양에도 프린터기 있었던 것 같습니다
4/13/24 8:35,화나서 방방 뛰는 튜브,학생회관에 있어요
4/13/24 8:35,General Trash,농생대 지하에도 있습니다
4/13/24 8:37,화나서 방방 뛰는 튜브,1층으로 들어가야하나요?
4/13/24 8:37,.,네 1층으로 들어가집니다
4/13/24 8:38,화나서 방방 뛰는 튜브,감사합니다
4/13/24 8:39,머리 빗는 네오,농대 지하 출력되나요?
4/13/24 9:08,쑥스럽게 인사하는 프로도,혹시 지각 입실 가능한가요?ㅠㅜ
4/13/24 9:08,TA Jisun,네 빨리 오세요
4/13/24 9:08,쑥스럽게 인사하는 프로도,네 죄송합니다ㅠㅠ
4/13/24 13:05,TA Jisun,사진
4/13/24 13:06,TA Jisun,혹시 301동에 휴대전화 두고 가신 분 계시면 알려주세요
4/13/24 13:08,TA Jisun,*301호
4/13/24 16:32,애교뿜뿜 무지,사진
4/13/24 16:32,애교뿜뿜 무지,"안녕하세요 메일로 이런 알림이 왔는데, 과제가 없는게 맞겠죠..?"
4/13/24 16:33,TA Jisun,네 무시하시면 됩니다.
4/13/24 16:34,건배하는 프로도,혹시 중간고사 채점시에 부분점수가 있나요?
4/13/24 16:35,엄지척 제이지,엄지척 제이지님이 나갔습니다.
4/13/24 17:14,TA kibeom,삭제된 메시지입니다.
4/13/24 17:15,TA Jisun,추후 채점기준 공개 시 확인하시면 됩니다.
4/13/24 17:24,마이크를 든 라이언,삭제된 메시지입니다.
4/13/24 17:40,건배하는 프로도,네 감사합니다
4/13/24 18:02,TA Jisun,"중간고사 해답 공개되었습니다.
The midterm solution has been uploaded."
4/15/24 11:31,화난 라이언,hw5 p6에서 Large neural networks가 label을 completely memorize한다는 건 이해됐는데 interpolate라는건 어떤 의미인가요? 참조된 논문도 읽어봤는데 과제에서 어떤 의미로 쓰신건지 잘 이해가 안 되네요
4/15/24 11:35,옐로카드 프로도,"함수가 특정 점들을 모두 지나게 하는 것이 interpolate하는 것이니, 모델이 모든 (타겟, 라벨)을 지나게 한다는 뜻으로 사용하신 것 같습니다!"
4/15/24 11:42,화난 라이언,저는 interpolate를 보간의 의미로 이해했는데 interpolate가 함수의 관점에서는 그런 의미를 가지는건가요??
4/15/24 11:56,기지개 고양이,그게 보간 아닌가요?
4/15/24 11:58,화난 라이언,제가 이해한 보간은 이미 알고 있는 데이터가 아닌 모르는 데이터에 대해서 레이블링을 어떻게 하는지에 대한 개념으로 알고 있는데요
4/15/24 12:00,아침햇살,함수에서 interpolate도 마찬가지로 모르는 정의역의 값에 대해 치역을 만들어서 채웁니다
4/15/24 12:01,화난 라이언,그래서 그게 해당 문제에서 large neural network가 어떤 interpolate를 한다는건가요?
4/15/24 12:06,기지개 고양이,"Input이 정의역이고, parameter들이 testdata라는 특정 정의역위 점들에서 label이라는 함수값을 반드시 갖도록 하는 보간이죠"
4/15/24 12:08,화난 라이언,그러면 memorize 랑 정확히 같은 의미로 사용했다고 보면 될까요?
4/15/24 12:09,화난 라이언,어떻게 보면 이미 주어진 함수값에 대해서 보간한다는 말이 이상하게 느껴지네요
4/15/24 12:12,기지개 고양이,보간이란게 특정 함수 (다항함수라던가)를 미리 짜놓고 이의 계수(parameter)를 주어진 데이터를 만족시키도록 맞춘다는 점에서
4/15/24 12:13,기지개 고양이,유사하다고 생각합니다
4/15/24 16:58,부탁하는 네오,강의실에 왜이리 긴 줄이 서있나 봤더니 간식줄이었군요..
4/15/24 21:28,소심한 네오,혹시 HW5 label_memorization 결과 나오는데 다들 얼마나 걸리셨나요?
4/15/24 21:28,소심한 네오,엄청 오래 걸리는게 정상인지...
4/15/24 21:28,부탁하는 네오,그거 채팅방 뒤져보면 나오긴 할텐데 코랩 gpu 쓰시는 게 좋을 거예요
4/15/24 21:28,옐로카드 프로도,코랩 gpu쓰면 별로 안걸립니다!
4/15/24 21:28,권투하는 무지,코랩 T4 써서 12분 결렸습니다
4/15/24 21:28,부탁하는 네오,로컬에서 하면 몇시간 걸려요
4/15/24 21:28,힙합맨 제이지,3060ti 기준 5분 안에 끝났습니다
4/15/24 22:04,소심한 네오,감사합니다ㅠㅠ
4/15/24 22:09,화난 튜브,혹시 시험결과는 언제쯤 공개될까요?
4/15/24 22:10,부탁하는 네오,드랍기간 전에 공개되길 바라는 마음이신가요
4/15/24 22:20,화나서 방방 뛰는 튜브,드랍하지 말아요ㅠㅜ
4/16/24 8:13,열심히 일하는 네오,열심히 일하는 네오님이 나갔습니다.
4/16/24 13:24,마이크를 든 라이언,혹시 HW6은 언제 올라오나요?
4/16/24 13:45,비옷입은 튜브,혹시 etl에서 성적 확인 되고있나요?
4/16/24 13:45,화나서 방방 뛰는 튜브,아뇨
4/16/24 13:45,손을 번쩍 든 무지,이미 올라왔네요. 4/22 5pm까지입니다 ㅠ
4/16/24 13:47,손을 번쩍 든 무지,혹시 시험 평균 같은거 공개해 주시나요? 🥺🥺🥺
4/16/24 13:50,화나서 방방 뛰는 튜브,"평균이랑 min, max는 성적란에서 확인 가능합니다"
4/16/24 13:52,비옷입은 튜브,저만 안 보이는게 아니었군요...
4/16/24 13:53,부탁하는 네오,"채점기준 공지가 올라온 거고, 성적은 조만간 업로드될 예정인 것 같아요"
4/16/24 13:54,콘이 웃긴 무지,hw6가 etl에서 확인이 되시나요?
4/16/24 13:54,손을 번쩍 든 무지,교수님 홈페이지에 올라와있어요!
4/16/24 13:55,콘이 웃긴 무지,아 감사합니다
4/16/24 13:56,화나서 방방 뛰는 튜브,과제가 기출이랑 비슷하네요
4/16/24 13:56,화나서 방방 뛰는 튜브,이모티콘
4/16/24 13:56,베개를 부비적대는 라이언,"평균, min, max가 어디에 있죠?"
4/16/24 13:57,화나서 방방 뛰는 튜브,지금은 없고 나중에 개인 성적 올라오면 확인 가능합니다
4/16/24 13:57,베개를 부비적대는 라이언,아 아직 안 보이군요
4/16/24 13:57,기지개 고양이,기다려야될거 같습니다
4/16/24 13:57,화나서 방방 뛰는 튜브,사진
4/16/24 13:58,화나서 방방 뛰는 튜브,오른쪽 체크박스 누르시면 돼요
4/16/24 13:59,손을 번쩍 든 무지,아직 점수가 공개 안돼서 확인이 안되는 모양이네요
4/16/24 14:00,치즈케이크,공지 맨 앞에 있는데..
4/16/24 19:46,눈물바다에 빠진 라이언,사진
4/16/24 19:46,눈물바다에 빠진 라이언,혹시 중간고사 solution 중에
4/16/24 19:47,눈물바다에 빠진 라이언,dropout이 zero가 아니면 identity라는 말이 있는데
4/16/24 19:47,눈물바다에 빠진 라이언,zero가 아니면 1/(1-p) 가중치가 곱해지는 것이 아닌가요?
4/16/24 19:48,눈물바다에 빠진 라이언,사진
4/17/24 0:02,TA kibeom,"넵 맞습니다. 
오타입니다. 수정하겠습니다"
4/17/24 1:01,.,삭제된 메시지입니다.
4/17/24 2:03,인사하는 프로도,HW6 P5에서 test error는 어떤 식을 통해 구해야하나요?
4/17/24 2:07,으쓱으쓱 어피치,혹시 중간고사 문제6번에서 dropout도 고려해야되는 이유가 무엇인가요? Evaluation때는 비활성화되는 것 아니었나요?
4/17/24 2:10,기타치는 튜브,training 에서도 forward 는 일어나므로 그런 것 아닐까요?
4/17/24 2:11,콘이 웃긴 무지,저도 evaluation에서 같은지 판단하는 문제라고 생각했었어요
4/17/24 2:53,치즈케이크,It seems that the midterm grades are not yet visible on eTL (despite the announcement). Could someone check if it is just my problem?
4/17/24 2:57,엄지척 튜브,저도 공지는 봤는데 아직 성적은 확인 못 하고 있습니다ㅠㅠ
4/17/24 3:01,손을 번쩍 든 무지,Me neither.
4/17/24 3:05,손을 번쩍 든 무지,저도 안 보입니다.
4/17/24 3:22,퇴근하는 프로도,점수 올라오는 중인가요
4/17/24 3:23,손을 번쩍 든 무지,넵 그런것 같네요
4/17/24 3:25,손을 번쩍 든 무지,방금 다 떴네요
4/17/24 7:14,건배하는 프로도,성적 클레임은 어떻게 이루어지나요?
4/17/24 7:16,신난 어피치,예년에 성적컷을 A+ 92.5 A0 85 등으로 잡았다는데 이는 중간고사 30. 과제 30.기말 40을 반영한 비율일까요?
4/17/24 8:19,애교뿜뿜 무지,아무래도 그렇겠죠? 그런데 예전에 수강한 경우가 아니라면 교수님의 학점 부여는 조교님들도 잘 모를겁니다.
4/17/24 8:30,TA Yongin,HW4 redo has been graded.
4/17/24 8:30,Apeach in love,그래도 70-100을 A대로 생각하고 계신다고 하니 90점 이상은 A+ 기대해봐도 좋을 듯 하네요! 물론 최종 성적은 아무도 모르지만
4/17/24 8:38,밥줘,저도 똑같이 생각해서 8점 감점 됐네요 (흑흑)
4/17/24 8:58,Ernest Lee,클레임은 어떻게 할 수 있나요?
4/17/24 9:00,TA Jisun,"클레임 관련 공지가 eTL에 게시될 예정입니다.
The announcement about the claim session for midterm will be posted in eTL shortly."
4/17/24 9:32,기뻐하는 라이언,기뻐하는 라이언님이 나갔습니다.
4/17/24 10:06,엄지척 튜브,전체 평균이나 q1 q2 q3 등도 알려주실 수 있나요??
4/17/24 10:54,화나서 방방 뛰는 튜브,평균은 대충 77~78정도 되는 것 같습니다
4/17/24 11:03,권투하는 무지,pc버전 성적에서 네모체크박스 누르시면 문항별 평균 최고 최저 나옵니다
4/17/24 11:23,비옷입은 튜브,ResNet18에서 layer 개수가 어떻게 18개가 된건가요?
4/17/24 11:25,비옷입은 튜브,사진
4/17/24 11:25,비옷입은 튜브,이 사진에서요
4/17/24 11:27,Tube cleaning,Main branch에 있는 conv layer 17개와 마지막 linear layer 1개입니다!
4/17/24 11:30,비옷입은 튜브,아 그렇군요 stem layer은 어떤 역할을 하는건가요?
4/17/24 14:29,화나서 방방 뛰는 튜브,사진
4/17/24 14:29,화나서 방방 뛰는 튜브,음...다시 할까요?
4/17/24 14:33,TA Jisun,"클레임 세션 관련 공지가 업로드 되었습니다.
The announcement about the midterm claim session is posted."
4/17/24 15:32,파이팅하는 무지,"*If you want to raise claims but do not have a photo of your answer sheet, please contact TAs for further instructions.
이 경우에는 구체적으로 어떤 TA분께 여쭤보면 될까요?"
4/17/24 15:59,TA Jaewook,"각 문제별로 담당하는 TA와 클레임 일정 및 방법을 조율(비대면/대면)하여 클레임을 진행하시면 되고, 따라서 클레임을 희망하는 문항을 담당하는 조교에게 문의하시면 됩니다.

You can proceed the claim by coordinating the schedule and method (online/ in-person) with the TA responsible for each problem. Therefore, please contact the TAs responsible for the problem you want to claim."
4/17/24 17:38,빈털터리 제이지,다음학기나 내년에도 같은 강의가 열리나요?
4/17/24 17:40,경례하는 프로도,저도 궁금 ㅋㅋ
4/17/24 18:02,화난 라이언,"Sorry for asking, I missed a point when professor was answering the question. Is it correct that Batchnorm could be one kind of SENet?"
4/17/24 18:22,Luca [루카],Not quite:
4/17/24 18:23,Luca [루카],Gamma is a trainable parameter in batch norm whereas the scaling factor in SENet (gamma's equivalent) is based entirely on the actual input data. More detail in his answer at around 50 minutes into the recording probably?
4/17/24 18:24,화난 라이언,Thank you for your help:)
4/17/24 20:39,눈물바다에 빠진 라이언,혹시 HW 6의 Problem에서 필요하다면 torch를 사용해도 되는 것일까요?
4/17/24 20:40,화나서 방방 뛰는 튜브,전 torch 안 쓰긴 했어요
4/17/24 20:41,눈물바다에 빠진 라이언,음
4/17/24 20:42,눈물바다에 빠진 라이언,Relu를 직접 만들어야 하나 했는데
4/17/24 20:42,눈물바다에 빠진 라이언,torch 안 쓰는게 의도인 거 같으니까
4/17/24 20:42,눈물바다에 빠진 라이언,그냥 만들어야겠네요
4/17/24 20:46,화나서 방방 뛰는 튜브,이모티콘
4/17/24 20:56,치즈케이크,"I don't think it is significant, but shouldn't the forward method in Problem 1 should return dropout(sigma(linear(x))) instead of dropout(sigma(linear)) to be functional?"
4/17/24 20:58,청소하는 튜브,넵 np.where로 간단히 만들 수 있어요!
4/17/24 21:05,눈물바다에 빠진 라이언,감사합니다! np.maximum으로 만들었는데 이런 것도 있군요 참고하겠습니당
4/17/24 21:07,치맥하는 제이지,"hw6 6번에서 잘 이해가 되지 않는데 validation data로 lambda를 튜닝 하라는 말이, 이미 training data로 학습된 theta는 가만히 놔두고 가능한 lambda들마다 validation loss를 계산해서 그게 최소가 되도록 하는 lambda를 찾으라는 말인가요?"
4/17/24 21:13,화나서 방방 뛰는 튜브,저는 그렇게 이해했습니다
4/17/24 22:09,청소하는 튜브,네 근데 예시랑 똑같이 그림 얻으려면 tuning할 때 lambda가 바뀌어서 data는 그대로 train set으로 하고 theta도 lambda 바꾸면서 튜닝해줘야 돼요
4/18/24 0:05,화나서 방방 뛰는 튜브,아 저도 이렇게 했네요 죄송합니다ㅠㅠ
4/18/24 1:22,TA Gyeongmin,HW5 redo has been graded.
4/18/24 9:59,으쓱으쓱 어피치,"p마다, tuned된 lambda가 다른게 맞는거죠?? (ex. params 100의 tuned lambda와 1000의 tuned lambda는 일반적으로 다름)"
4/18/24 10:02,화나서 방방 뛰는 튜브,네
4/18/24 10:02,화나서 방방 뛰는 튜브,따로 tuning해야합니다
4/18/24 10:19,으쓱으쓱 어피치,Larger모델에 대한 train data interpolate도 되시나요??
4/18/24 11:23,붕붕,조교님 이번주 수요일 강의영상 곧 올라올 예정인가요??
4/18/24 15:11,.,"HW6 5번에서 lambda에 대해 tuning을 할 때 미분식이 항상 양수가 나오는 것 같은데, 그럼 lambda가 0.01에서 시작해버리면 항상 감소하는 방향으로 tuning 되는 건가요?"
4/18/24 15:11,.,Lambda tuning을 어떻게 진행해야할지 헷갈리네요
4/18/24 15:19,.,"제가 잘못 이해했던 것 같습니다!
lambda List 원소들마다 모델을 훈련하고, 각 모델에서 나온 validation loss를 비교해 최소가 되도록 하는 lambda랑 theta를 고르면 되는 걸까요?"
4/18/24 15:28,기타치는 튜브,"훈련을 하는 것이 아니라, 선형대수학 계산으로 직접 최적 theta값을 exact하게 찾아 그 loss function의 최솟값을 플롯하면 되는 것 같습니다"
4/18/24 15:52,.,이해했습니다 감사합니다!
4/18/24 16:38,.,이거 어떻게 해결하셨나요?
4/18/24 16:38,Tychonoff,Tychonoff님이 나갔습니다.
4/18/24 16:58,화나서 방방 뛰는 튜브,에러를 바꿨습니다. 어디까지 말씀드려도 되는지 모르겠네요ㅠㅠ
4/18/24 17:07,.,아하 알겠습니다!
4/18/24 17:07,.,좀 더 고민해보겠습니다
4/18/24 20:14,하트뿅뿅 라이언,하트뿅뿅 라이언님이 나갔습니다.
4/18/24 22:48,치맥하는 제이지,혹시 hw6 4번에 손코딩 대신 따로 코드를 컴퓨터로 작성하고 사진으로 첨부해도 될까요?
4/18/24 22:49,하트뿅뿅 라이언,가능할거 같습니다 !
4/18/24 22:53,멋쟁이 프로도,이게 잘못된 결과인가요?
4/18/24 22:53,화나서 방방 뛰는 튜브,네
4/18/24 22:54,화나서 방방 뛰는 튜브,더블 디센트가 살아있잖아요
4/18/24 23:59,택배 상자를 든 네오,"yes, I think you are right"
4/19/24 0:30,밥줘,"Even though I've solved the problem 6, I keep getting an error in the ""plt.rc"" part. Has anyone experienced something similar? I keep getting 'latex could not be found' even after trying pip install latex."
4/19/24 0:30,화나서 방방 뛰는 튜브,근데 5번 아니에요?
4/19/24 0:31,눈물바다에 빠진 라이언,!sudo apt-get install texlive-latex-extra texlive-fonts-recommended dvipng cm-super
4/19/24 0:31,눈물바다에 빠진 라이언,"if you use colab, then this might work"
4/19/24 0:32,화나서 방방 뛰는 튜브,colab 안 쓰시면 miktex 깔고 PC 재시작하시면 돼요
4/19/24 0:43,밥줘,"두 분 다 감사합니다 ㅎㅎ
둘 다 되네요"
4/19/24 1:13,하트뿅뿅 라이언,이번 과제 prob2 y_L variance 깔끔하게 정리되나요..?
4/19/24 1:14,신난 어피치,"과제 2번에서 제가 찾아본 바로는 A는 variance가 1/n_in 이고, b는 variance가 1/(3*n_in)인데, 제가 이해한게 맞을까요?"
4/19/24 1:15,화나서 방방 뛰는 튜브,전 정답인지는 모르겠지만 아니에요ㅠㅠ
4/19/24 1:15,힙합맨 제이지,5번 실행 시간 얼마나 걸리시나요?
4/19/24 1:16,화나서 방방 뛰는 튜브,1분 내로 끝나는것 같습니다
4/19/24 1:16,하트뿅뿅 라이언,저는 그 document uniform distribution의 variance로 계산했습니다..!
4/19/24 1:16,하트뿅뿅 라이언,혹시 recursive하게 나오시나요?
4/19/24 1:18,화나서 방방 뛰는 튜브,아무래도 구조가 recursive하다보니 그렇게 될 것 같습니다
4/19/24 1:19,화나서 방방 뛰는 튜브,var(y_L-1)같은거는 안 나오게 조금 정리하긴 했는데 깔끔하지는 않아요
4/19/24 1:21,신난 어피치,"그 bound에 대해서 variance가 bound^2 / 3으로 나오는데, 소스코드 보니까 weight 부분은 init.kaiming_uniform_ 메소드를 사용하고 bias 부분은 직접 초기화하더라고요"
4/19/24 1:21,신난 어피치,그래서 메소드 소스코드를 찾아보니까... 여기에는 sqrt(3)을 이미 곱해서 variance를 1/n_in으로 만든 것 같아서요
4/19/24 1:21,신난 어피치,https://github.com/pytorch/pytorch/blob/77721ee318d6785010144aa4569efb98199e7162/torch/nn/init.py#L390-L395
4/19/24 1:31,하트뿅뿅 라이언,감사합니다 kaiming_uniform_으로 initialize하는지는 몰랐네요
4/19/24 1:32,신난 어피치,https://discuss.pytorch.org/t/clarity-on-default-initialization-in-pytorch/84696/2
4/19/24 1:32,신난 어피치,"음... 근데 작동 방식을 찾아보니, 실제로는 bound쪽에 직접 구현된 것과 같은 효과를 준다고 하네요"
4/19/24 1:32,신난 어피치,*bias
4/19/24 1:44,눈물바다에 빠진 라이언,음..그런데 kaiming_uniform_의 nonlinearity='leaky_relu' 때문에 gain=1/sqrt(3)으로 계산되지 않나요?
4/19/24 1:45,눈물바다에 빠진 라이언,https://github.com/pytorch/pytorch/issues/57109
4/19/24 1:45,신난 어피치,넵 그래서 상쇄되는 것 같습니다.. 그게 a=sqrt(5) factor이 있는 이유네요
4/19/24 2:07,힙합맨 제이지,Can I solve the least-squares problem in problem 5 without using inverse matrices? I think calculating inverse matrices for large values of p takes too long time.
4/19/24 8:56,deee,ch3 슬라이드 124쪽에 1x1 conv가 75% 뉴런들을 무시한다고 나와있는데 혹시 이유를 알 수 있을까요? 50%가 아닌 75%인 이유를 잘 모르겠습니다..
4/19/24 9:19,파이팅하는 무지,"OX
XX

라서 75퍼무시입니닷"
4/19/24 9:31,deee,아 제가 세로 방향을 생각안했군요.. 감사합니다!!
4/19/24 9:36,택배 상자를 든 네오,택배 상자를 든 네오님이 나갔습니다.
4/19/24 17:58,건방진 제이지,저도 궁금하네요
4/19/24 17:58,건방진 제이지,최소한 문제에 나와있는 식은 아닌것 같습니다
4/19/24 17:59,청소하는 튜브,네 error 정의랑 비슷하게 생각하면 y끼리의 차이의 L2 norm이에요
4/19/24 18:00,건방진 제이지,그렇군요
4/19/24 18:01,청소하는 튜브,np.solve로 조금 더 빠르게 할 수 있습니다
4/19/24 20:07,기지개 고양이,"Hw6 p3 (iii)에서 ""need not vanish"" 가 무슨 뜻인가요? 무엇을 보여야 하는건지 잘 이해가 안되네요 ㅜㅜ"
4/19/24 20:07,기지개 고양이,사진
4/19/24 20:07,초롱초롱 어피치,꼭 0이 될 필요는 없다 이런뜻입니다
4/19/24 20:08,권투하는 무지,주어진 조건에서 항상 vanish하는 것은 아니다
4/19/24 20:08,기지개 고양이,아하... 감사합니다! 
4/19/24 20:09,경례하는 프로도,5번 문제에서 fixed_lambda에 대해서 plot 해봐도 double descent 현상이 안일어나는데 error 계산하는 과정이 잘못된 걸까요...?
4/19/24 20:11,경례하는 프로도,그냥 p가 커짐에 따라서 L2 norm 값이 줄어드는 양상으로만 나오네요 ㅠ
4/19/24 20:34,마이크를 든 라이언,저는 처음에 theta값을 sgd로 구했을때 그렇게 나왔었는데 선형대수로 least square를 풀어서 구하니깐 double descent가 나왔어요
4/19/24 21:49,경례하는 프로도,확인해보니 W initialize 할 때 variance를 잘못입력한게 문제였네요 친절하게 답변해주셔서 고맙습니다!
4/19/24 22:29,열심히 일하는 네오,혹시 이 수업 다음 학기나 내년에도 열리나요? 피치 못할 사정으로 휴학하게 될 수도 있는데.. 꼭 듣고 싶어서 여쭤봅니다..
4/19/24 22:33,초롱초롱 어피치,확실한건 아니지만 내년 1학기에 열리지 않을까요
4/19/24 22:37,부끄러워하는 라이언,"제가 알기로는 2학기에 열리는 수업인데, 작년 2학기에 연구년이라 안열리셔서 올해 1학기에 여신거로 알고 있어요!"
4/19/24 22:37,부끄러워하는 라이언,내년은 어떻게 될지 모르겟네요
4/19/24 22:44,애교뿜뿜 무지,아예 1학기로 개편되었을걸요
4/20/24 0:09,하트뽀뽀 어피치,"Failed to process string with tex because latex could not be found
이 에러 어떻게 해결하는지 아시는 분 계신가요?"
4/20/24 0:10,.,.
4/20/24 0:10,밥줘,여기 보면 돼요
4/20/24 0:12,하트뽀뽀 어피치,빠른 답변 감사합니다. 시도해볼께요!
4/20/24 14:10,열심히 일하는 네오,중간고사 문제3번 솔루션 중 궁금한 게 있어서 여쭤봅니다. 혹시 여기서 상수 C를 굳이 사용하는 이유가 있나요?
4/20/24 14:10,열심히 일하는 네오,사진
4/20/24 14:30,멋쟁이 프로도,"(a_true,b_true)의 존재성만 아는 상황에서 에러를 임의의 양수보다 작게 만들기 위해서입니다"
4/20/24 14:36,열심히 일하는 네오,감사합니다!
4/20/24 14:38,기지개 고양이,"HW6 P5 linear algebra로 풀라고 하는데
Parameter 1400개 되는걸 역행렬 구해가면서 exact한 θ 구해야하는건가요?"
4/20/24 15:43,기지개 고양이,아 요즘 컴퓨터는 1500x1500 역행력 계산도 해내는군요
4/20/24 15:44,인사하는 제이지,중간고사 클레임 답변은 언제쯤 오나요?
4/20/24 15:45,인사하는 제이지,어제 저녁전에 보냈는데 아직까지 답변이 없네요
4/20/24 16:00,기뻐하는 라이언,주말인데 좀 기다려 보시지요
4/20/24 16:01,Ernest Lee,조교님들도 쉬셔야 ..
4/20/24 16:02,Ernest Lee,점수만 좀 주시고 푹 쉬..
4/20/24 16:46,TA Jaewook,"저의 경우 학회에 참석하느라 답변이 늦어지고 있는 분들이 있습니다, 죄송합니다. 늦어도 내일까지는 다 답 드리려고 합니다."
4/20/24 18:11,.,"혹시 HW6 #5 에서 이 에러 나오시는 분 있나요? plt에서 생기는 문제같은데 인터넷 나오는 것들 몇 개 시도해도 계속 떠서요.

Failed to process string with tex because latex could not be found"
4/20/24 18:12,기지개 고양이,.
4/20/24 18:14,.,헉 감사합니다
4/20/24 22:46,양손 엄지척 무지,5번에서 relu 함수를 쓰기 위해 pytoch를 import 해도 되나요?
4/20/24 22:47,신난 어피치,"저는 float을 받아 float을 출력하는 ReLU를 정의하고, np.vectorize()를 사용했습니다"
4/20/24 22:51,기지개 고양이,"np.maximum(0,X) 도 좋아요"
4/20/24 22:51,양손 엄지척 무지,다들 감사합니다
4/21/24 15:54,머리 빗는 네오,결과적으로 둘다 1/n_in이 되나요 그럼?
4/21/24 15:55,머리 빗는 네오,제가 코드를 읽어봤을 때도 default initialization에서 gain이 1/sqrt(3)이 나와서 나눠지는 거 같아서요 
4/21/24 16:10,.,5번 다들 몇 분 정도 걸리셨나요? — 컴퓨터가 구려서 Google CoLab으로 하는데 fixed lambda 갖고만 계산해도 너무 오래 걸리네요.
4/21/24 16:11,기지개 고양이,한 5분 이하로 걸린거 같아요
4/21/24 16:11,기지개 고양이,노트북 cpu가 intel 10세대 i3으로도
4/21/24 16:12,기지개 고양이,생각보다 금방 되는듯요?
4/21/24 16:12,하트뿅뿅 라이언,90초 정도 걸렸습니다
4/21/24 16:13,.,아
4/21/24 16:13,손을 번쩍 든 무지,저는 40초 정도 걸렸습니다
4/21/24 16:13,.,제가 바보였네요.
4/21/24 16:14,.,W를 multivariate normal distribution으로 정의한 부분이 시간을 맛있게 잡아쳐드시고 계셨습니다.
4/21/24 16:15,.,그냥 normal distribution으로 고치니까 15초 걸리네요...
4/21/24 16:31,권투하는 무지,lambda에 대해 roughly tuning을 진행하라는 것이 계산한 optimal theta에 대하여 주어진 lambda 중 test loss가 가장 적어지는 lambda를 직접 계산하여 찾으라는 의미인가요...?
4/21/24 16:40,인사하는 프로도,"lambda_list에 있는 값들로 각 lambda에 대한 theta를 구하고, 그 중에서 가장 loss 작은 theta 고르면 될 것 같습니다!"
4/21/24 16:40,권투하는 무지,아 lambda가 먼저군요...! 감사합니다
4/21/24 21:30,쑥스럽게 인사하는 프로도,HW6 Problem 5 스켈레톤코드에서 validation data와 test data에 대해서는 Y에 noise term이 더해지지 않은 것 같은데 의도하신 부분이라면 이유가 무엇인지 여쭤봐도 될까요?
4/22/24 4:45,부탁하는 네오,코랩 쓰시는 분들 중 이거 하고도 해결 안 된 경우 있나요?
4/22/24 4:45,부탁하는 네오,저거 설치했는데도 같은 에러가 뜹니다
4/22/24 4:45,부탁하는 네오,사진
4/22/24 4:45,부탁하는 네오,설치된거 확인까지 했구요
4/22/24 4:50,부탁하는 네오,아 죄송합니다;; 로컬에 설치하는건 의미없고 코랩 코드 첫줄에 저거 써서 실행하는 거였네요
4/22/24 5:47,부탁하는 네오,"혹시 그림이 이렇게 나오면 뭐가 문제일까요??
lambda를 튜닝해줬을 때는 잘 되는 것 같은데 fix했을 때도 그림이 유사하게 나옵니다
각 p마다 lambda=0.01로 두었을 때 exact한 theta를 구하고 테스트 데이터 가지고 에러 구해서 errors_fixed_lambda에 넣어주었습니다"
4/22/24 5:47,부탁하는 네오,사진
4/22/24 8:12,벌 서는 라이언,저도 이렇게 나오는데 혹시 해결하셨나요…?
4/22/24 8:13,부탁하는 무지,W initialization에서 scale을.잘못 입력한 경우인 것 같습니다
4/22/24 8:15,벌 서는 라이언,감사합니다!
4/22/24 8:40,TA Jisun,"As the professor's request, here is an announcement:

Today, SNU math department will host a talk entitled “Tikhonov regularization as a nonparametric method for uncertainty quantification in aggregate data problems” by Professor Carlos Sing-Long of Pontifical Catholic University of Chile. Those of you who are interested are encouraged to attend in person (상산수리과학관 1층 대강당). Details are posted on the course website."
4/22/24 12:17,부탁하는 네오,"아하 감사합니다
파라미터가 표준편차인데 분산으로 생각하고 하고있었네요;;ㅋㅋㅋㅋ"
4/22/24 12:18,부탁하는 네오,아 그리고 혹시 5번 문제에서 plt.savefig 이 부분 지우고 제출해도 되나요? 실행결과 그림이 보이기만 하면 되는거고 따로 저장되어야 할 필요는 없는 것인지 궁금합니다
4/22/24 15:57,TA chaeju,"실행결과가 보이기만 한다면, 지우고 제출하셔도 됩니다."
4/22/24 17:40,음료수 마시는 어피치,과제 제출 데드라인을 자정으로 착각하여 제출을 하지 못했는데 혹시 지연 제출이라도 할 수 있는 방법이 있을까요? ㅠㅠ
4/22/24 18:12,TA chaeju,"지연 제출에 관해서는 이미 공지가 올라간 상황이라, 받아 드리기 어려울 것 같습니다."
4/22/24 20:03,초롱초롱 튜브,초롱초롱 튜브님이 나갔습니다.
4/22/24 21:54,눈물바다 라이언,눈물바다 라이언님이 나갔습니다.
4/23/24 14:02,인사하는 프로도,"중요한 건 아닌 것 같지만(?) HW7 P2관련 문의드릴 게 있습니다. 시험삼아 torch.randn(3, 227, 227) tensor를 만들어서 AlexNet model에 forward propagation 했는데, 중간에

mat1 and mat2 shapes cannot be multiplied (256x36 and 9216x4096)

이런 에러가 뜹니다.

AlexNet 모델 선언할 때 forward에 있는 torch.flatten(x, 1)이 torch.flatten(x, 0)으로 바뀌어야할 것 같은데 확인해주시면 감사하겠습니다!"
4/23/24 17:09,빈털터리 제이지,혹시 어제 강의영상은 언제쯤 올라오나요??
4/23/24 17:29,선풍기 바람 쐬는 어피치,어제 강의영상이 기계학습의수학적이론 수업 ETL에 잘못 올라온거 같은데 확인 부탁드립니다..!
4/23/24 21:55,눈물바다에 빠진 라이언,혹시 HW7의 Problem 2는 .py/.ipynb 파일을 제출하지 않고 풀이만 적어서 내면 되는 것일까요?
4/23/24 22:14,TA Jisun,네 2번은 풀이만 적어서 제출하시면 되겠습니다.
4/23/24 22:15,눈물바다에 빠진 라이언,넵 감사합니다!
4/23/24 22:15,인사하는 프로도,혹시 코드로 계산해도 괜찮나요?
4/23/24 22:16,TA Jisun,코드로 제출하시려는 걸까요?
4/23/24 22:19,TA Jisun,"For the problem 2 of hw7, please submit the written calculation as your answer. (No need to submit .py / .ipynb codes.)"
4/23/24 22:26,인사하는 프로도,"네 코드로 계산하긴 했는데, 손으로 다시 풀어서 제출하겠습니다!"
4/24/24 11:58,마이크를 든 라이언,HW7에 3번에서 smallNetSaved 파일을 ipnyb파일과 같은 폴더에 두었더니 No such file or directory: './smallNetSaved'라고 뜨는데 어디에 두어야하는 걸까요?
4/24/24 11:58,마이크를 든 라이언,사진
4/24/24 12:27,Luca [루카],""".htm""을 제거해야 돼요. 새로 명한 파일이 그냥 ""smallNetSaved""라고 불려요."
4/24/24 13:17,마이크를 든 라이언,경로를 './smallNetSaved.htm'로 바꿨는데 되네요. 조언감사합니다!!
4/24/24 19:11,힙합맨 제이지,아직 etl 과제 탭에서 보이는 중간고사 점수는 클레임 반영이 안 된 것이 맞나요?
4/25/24 4:32,손을 번쩍 든 무지,혹시 이번 과제 2번 문제도 HW 5 problem #5처럼 zero padding으로 인한 addition까지 고려해야 하나요?
4/25/24 13:01,아이스크림 든 네오,HW7 problem3에서 최종 max(diff) 값이 ~1e-07 정도로 나오신 분 있을까요?? 식에서는 틀린 부분을 아직 찾지 못해서 혹시 다른 분들은 < 1e-08이 잘 나오는지 궁금합니다. 
4/25/24 13:06,기타치는 튜브,7e-09로 나왔습니다
4/25/24 13:07,아이스크림 든 네오,감사합니다! 좀 더 생각해봐야겠네요
4/25/24 13:35,손을 번쩍 든 무지,저는 왜 2.7e+03으로 터질까요...
4/25/24 15:20,옐로카드 프로도,problem3 관련해서 질문한번 드리고 싶어요.. 계속 생각해봤는데도 batch normalization을 이전 linear layer에 결합시켜서 computational cost를 왜 줄일 수 있다는지 감이 안잡힙니다.. 혹시 추천해주실 자료가 있으실까요..?
4/25/24 15:28,TA chaeju,"BN layer의 역할은 이전 layer의 출력값의 평균과 분산을 beta, gamma로 맞추어 주는 것입니다.

beta와 gamma의 값은 학습하는 동안 변화하지만, 학습이 끝난 이후에는 고정됩니다.

따라서, 직전 linear layer의 weight들에 ""적절한"" 상수배를 해줌으로서, BN layer의 영향을(실제로 BN layer를 만들지 않고도) 줄 수 있다는게 문제의 포인트입니다."
4/25/24 15:30,손을 번쩍 든 무지,사진
4/25/24 15:32,손을 번쩍 든 무지,이거 때문이 아닐까요..?
4/25/24 15:46,옐로카드 프로도,"말씀해주셔서 감사합니다! 저도 그 적절한 계수를 찾으려고 시도를 해봤었는데...
그냥 단순히 notation하여
(W는 선형 레이어 weight)
Y = BN W x 로 진행되는 것을
Y = W' x 로 바꾸고 싶고, W' = BN W 이어야 할 것 같은데, BN W를 cW (c는 elementwise하게 곱해질 상수들)로 표현 가능하다는 말씀이신가요?

제가 이해한 바로는, 저 그림에 따르면
BN은 WX에 대해 이루어지는 작업이고,
test 과정에서는 X에 대해 작업해야 하는데

W에 상수배를 적용하면 분산이 그에 따라 바뀔 수 있다는 것은 이해가 가지만, 평균을 어떻게 움직일 수 있나요?"
4/25/24 15:47,TA chaeju,bias를 조정하시면 됩니다.
4/25/24 15:51,옐로카드 프로도,"제가 잘못 생각한 것일수도 있지만,
BN(W X)과정에서는 bias를 조정하는 것이 WX을 전체적으로 움직이는 것 과 같은데

W'X에서 W'의 bias를 조정하는 것은 X를 움직이는 것이라고 생각했습니다. 저는 이 둘이 같지 않다라고 생각했었는데, 같은 것인걸까요? 
일단 좀 더 생각해보겠습니다"
4/25/24 15:53,TA chaeju,"fully linear layer와 convolutional layer 모두 bias term이 존재합니다.

fl layer는 Wx가 아니라 Wx+b 형식입니다."
4/25/24 16:04,옐로카드 프로도,"아 제가 너무 간추려 작성해서 용어표시를 잘 못 했던 것 같습니다...
하지만, 그렇다하더라도 test과정에서 W의 bias를 d만큼 조정한다면 그것은 결과적으로 전체적인 값이 d가 이동하는 것이 아니라 d*X만큼 이동하는 것 아닌가요? "
4/25/24 16:06,기타치는 튜브,직접 대입해서 계산식 써보시면 이해가 빠를 것 같습니다
4/25/24 16:07,화난 라이언,conv1만 했을 때 저도 1e-07 정도로만 나와서 고민했는데 전부 다 짜서 돌리니 -09까지 나오네요
4/25/24 16:07,옐로카드 프로도,제가 잘못 생각한 부분이 있나 보네요.. 일단 다시 생각해보겠습니다!
4/25/24 16:08,화난 라이언,혹시나 시간 버리고 계실까..해서 버려지는 시간은 제 것만으로 충분한 것 같습니ㅏ...
4/26/24 0:43,Tube cleaning,저도 1e-7 정도 나오네요ㅜㅜ 저도 제 식에서 틀린 부분을 못 찾겠네요
4/26/24 12:59,화난 라이언,p5 (a)에 diff 얼마 정도 나오시나요? 저는 0.0033 정도 나오는데 괜찮은 값인지 모르겠네요
4/26/24 13:07,으쓱으쓱 어피치,전 2e-17정도 나옵니다
4/26/24 13:08,신난 어피치,전 bias를 copy 안했을 때 그렇게 나왔어요
4/26/24 13:09,화난 라이언,그렇군요 감사합니다!!
4/26/24 13:37,청소하는 튜브,hw7 문제 3번에서 아무리 해도 conv1에서 에러가 1e-6 이하로 안떨어지는데 모든 자료형을 double로 바꾸니 1e-25로 떨어지네요. 혹시 1e-8이하로 떨어트리는 데에 floating point number limit까지 고려해야하나요?
4/26/24 14:01,비옷입은 튜브,혹시 resnet에서 stem layer의 역할이 어떤거였나요?
4/26/24 14:18,옐로카드 프로도,https://stackoverflow.com/questions/68258188/why-does-cnns-usually-have-a-stem
4/26/24 14:19,옐로카드 프로도,도움이 될 것 같습니다!
4/26/24 14:21,비옷입은 튜브,감사합니다!
4/26/24 17:02,Luca [루카],이번 주나 다음 주에 과제6의 성적들이 나오나요?
4/26/24 17:20,애교뿜뿜 어피치,감사합니다 저도 첫 단계에서 계속 고민했는데.. 전부 구현하니 나오네요
4/26/24 17:54,Ernest Lee,HW6이랑 클레임 적용한 성적은 언제쯤 나올까요?
4/26/24 18:08,TA chaeju,Hw6 grade will be completed by the end of the week.
4/26/24 18:42,렐?루,"아마 batch size 를 고려해서 그런 것 같습니다. torch.randn([1,3,227,227])로 하시만 잘 작동할것 같아요"
4/26/24 20:55,시무룩한 튜브,혹시 savedNetSaved 파일은 그자체로 열어볼 수는 없나요?
4/26/24 20:55,시무룩한 튜브,처음에 다운로드 했을 때 .html 이었는데 다른 확장자명 .ipynb 이런거로 바꿔봐도 안열리더라구요..
4/26/24 21:29,Luca [루카],^
4/27/24 20:35,시무룩한 튜브,알려주신 방법을 써봤는데도 계속 그 경로에 파일이 없다고만 뜨네요.. smallNetSaved 파일이 utf-8로 인코딩되지 않아서 생기는 문제 같은데 혹시 파일 인코딩 방법을 변경하는 법 아시는 분 계신가용
4/27/24 20:36,애교뿜뿜 무지,"저 같은 경우 colab에서 drive mount를 한 뒤 경로 복사하여 "".htm"" 방식으로 하니까 되었습니다"
4/27/24 20:42,초롱초롱 네오,저도 그냥 코드 경로에 .html추가하니 됐어요
4/27/24 20:45,시무룩한 튜브,혹시 smallNetSaved 파일이 어디에 저장되어 있으셨나요?
4/27/24 20:54,손을 번쩍 든 무지,혹시 이거 시도해보셨나요? 저도 그렇게 하다가 안돼서 이렇게 해봤는데 되더라고요
4/27/24 20:55,시무룩한 튜브,.htm .html .pt 다 해보고 확장자명 아예 지우고도 해봤는데 다 안되네요..ㅠ
4/27/24 20:56,손을 번쩍 든 무지,저는 매 과제마다 과제별 폴더를 생성해서 거기에 과제 파일 포함 관련 파일 다 저장하거든요. 한번 그렇게 해보세요. 제가 컴알못이라 여기까지밖에 답을 못 드리겠네요 ㅠ
4/27/24 20:58,손을 번쩍 든 무지,이런식으로요
4/27/24 21:02,밥줘,현재 작업환경이 어떻게 되시나요?
4/27/24 21:03,시무룩한 튜브,저도 그런 식으로 정리해놨는데 안되네요..
4/27/24 21:04,시무룩한 튜브,사진
4/27/24 21:04,시무룩한 튜브,이런식으로 오류가 뜨네요
4/27/24 21:13,TA Jisun,혹시 어디서 작업하고 계실까요?
4/27/24 21:14,시무룩한 튜브,"저 사진은 주피터 노트북이고, 작업 위치는 그냥 데스크탑 안에 있는 심수기 파일에서 하고 있습니다. 근데 작업환경이 정확히 어떤 뜻일까요?"
4/27/24 21:14,TA Jisun,"노트북과 os, python pytorch 버전 등을 여쭤봤습니다."
4/27/24 21:15,TA Jisun,제가 구글링 해본 결과로는 pytorch 버전 문제라는 답변이 많은데요
4/27/24 21:18,시무룩한 튜브,맥북 최신 os 로 하고 있는데 최근에 맥북 sonoma 로 os 업데이트 한 게 문제이려나요..?
4/27/24 21:21,TA Jisun,대부분 지원이 되긴 할텐데 다른 패키지 버전을 모르면 저도 대답해드리기 어렵습니다
4/27/24 21:21,TA Jisun,pytorch 버전이 몇일까요?
4/27/24 21:25,시무룩한 튜브,"print(torch.__version__) 실행했더니

2.2.1+cu121 라고 나왔습니다"
4/27/24 21:28,TA Jisun,"import torch
torch.cuda.is_available()"
4/27/24 21:28,TA Jisun,한번 돌려보시겠어요?
4/27/24 21:41,시무룩한 튜브,false 로 나왔습니다
4/27/24 22:48,소심한 네오,"혹시 HW7 3번 진행할 때 BN 처럼 맞춰주기 위해 기존 model convolution 가중치에서 주어진 mean, variance, gamma, beta 차원 맞춰서 연산해서 model_test에 넣어주는 것이 아닐까요….? 아무리 해도 차이가 줄어들지 않네요..."
4/27/24 22:49,초롱초롱 네오,저도 그렇게 이해하고 맞게한거 같은데 1.1727e-07 밑으로 안내려가요…
4/27/24 22:50,소심한 네오,헉 저는 e+08이 나와요…어디서부터 연산이 잘못 된건지 도통 모르겠네요...
4/27/24 22:50,Tube cleaning,저도 계속 그러네요 ㅜㅜ
4/27/24 22:50,소심한 네오,주어진 bn 파라미터들 unsqueeze() 이용해서 차원 맞춰주는 것이 잘못된 방법인가요...?
4/27/24 22:52,초롱초롱 네오,"저는 .view(~.shape[0],1,1,1) 이런식으로 맞춰줬어요!"
4/27/24 22:53,소심한 네오,".view(-1, 1, 1, 1) 이런식으로 맞춰도 봤었는데 이러면 차원이 16,1,1,1 로 바뀌어서 연산이 제대로 되는 것일까요??"
4/27/24 23:05,권투하는 무지,"저는 singleton 써서 [:, None, None, None]로 인덱싱했더니 잘 됐습니다!"
4/28/24 10:02,TA Jisun,cuda 버전과 pytorch 버전 호환이 안되는 게 아닌가 싶은데 이전에 과제 잘 해오셨다면 OS 업데이트 문제가 원인이 될 수 있을 것 같네요
4/28/24 13:33,시무룩한 튜브,우선 오류난 상태로 제출은 하겠습니다..
4/28/24 14:19,TA chaeju,HW6 is graded. the due for re-do is 5/5 5:00 pm.
4/28/24 14:21,화난 라이언,Thank you! May I also ask about the revised midterm grade announcement date?
4/28/24 14:22,TA Jisun,They will be announced no later than tomorrow.
4/28/24 14:24,화난 라이언,Thanks a lot:)
4/28/24 14:54,하트뿅뿅 라이언,혹시 eps 넣으셨나요?
4/28/24 15:20,퇴근하는 프로도,저 혹시 클레임을 메일로 한걸 조교님이 아직 못보신 것 같은데 어떻게 해야 할까요...
4/28/24 15:55,TA Jaewook,혹시 예전에 메일 보내셨던 것을 첨부해서 다시 보내보실 수 있을까요?
4/28/24 15:59,퇴근하는 프로도,네 다시 한번 보내보겠습니다
4/28/24 16:03,건방진 제이지,hw 7 문제 5번에서 어떻게 convolution layer가 linear layer와 같은 output을 낼 수 있는지 잘 이해가 가지 않네요. 1x1 convolution 말고는 방법이 없는 것 같은데 이 경우에도 convolution은 channel 별로 bias를 더하고 linear layer는 각 항마다 bias가 따로 있지 않나요?
4/28/24 16:06,옐로카드 프로도,저는 말씀하신 내용을 그대로 차용하여 풀었습니다!
4/28/24 16:57,일하기 싫은 네오,Hw7 3번에서 smallnetsaved 파일을 불러오는 중 unexpected eof라는 에러가 자꾸 뜨네요…혹시 해결책을 아시는 분이 있나 해서 여쭤봅니다 ㅠㅠ
4/28/24 16:59,밥줘,저도 그랬는데 파일 새로 다운 받으니까 해결됐어요
4/28/24 17:08,.,"안녕하세요,
HW6 성적이 이상해서 제출했던 파일을 확인해보니 파일 선책 착오로 손으로 풀었던 부분이 아예 제출이 되지 않은 것 같은데, 혹시 파일 생성 날짜를 증명해서 다시 채점받을 방법이 있을까요 ㅠㅠ"
4/28/24 17:08,.,*선택
4/28/24 17:19,힙합맨 제이지,근데 이런 건 이 톡방에 올리는 것보단 개인적으로 메일 보내는 게 낫지 않나요
4/28/24 17:35,TA chaeju,"네, etl 공지사항 참조하여 메일보내주세요."
4/28/24 17:37,.,메일 드렸습니다.
4/28/24 17:38,.,"공지사항에 1, 2, 3번 담당 조교님 메일이 잘못 기재되어 있는 것 같습니다."
4/28/24 17:39,TA chaeju,"아 그렇네요, 수정했습니다. 감사합니다.
이미 보내주신 메일은 제가 포워드해놓을게요"
4/28/24 18:28,소심한 네오,네…. 넣었는데 안되네요. 혹시 제가 작성한 코드의 문제점 확인해 주실 수 있으실까요???
4/28/24 18:28,소심한 네오,사진
4/28/24 18:28,소심한 네오,사진과 같이 코드를 작성했는데 잘못된 부분을 잘 모르겠습니다
4/28/24 18:29,.,Bias가 weight에 잘못 들어간 것 같습니다
4/28/24 18:32,소심한 네오,bias는 beta로 설정하는 것이 아닌가요…??
4/28/24 18:49,밥줘,"y= Ax+b -> BN(y) = gamma × (y-mu)/var + beta 인 BN layer를
y=A* x + b* 로 완전히 표현하는 A*와 b*를 구하시면 되는데 b*를 잘못 생각하신것 같아요"
4/28/24 18:50,밥줘,상수항만 따로 정리해서 비교해보세요
4/28/24 18:50,소심한 네오,어떤 느낌인지 감이 온 것 같습니다. 감사합니다!
4/28/24 19:05,머리 빗는 네오,과제 5번 관련하여 질문드립니다! 혹시 정확히 0이 나와야 하나요?
4/28/24 19:08,기타치는 튜브,Machine epsilon 수준으로 작게 나오면 되는 것 같습니다
4/28/24 19:08,머리 빗는 네오,10^-8정도면 해당사항일까요
4/28/24 19:20,손을 번쩍 든 무지,저는 3번처럼 1e-08로 생각하고 하고있습니다
4/28/24 19:36,멋쟁이 프로도,저는 e-17 스케일 나와요
4/28/24 19:37,화나서 방방 뛰는 튜브,저도용
4/28/24 22:15,머리 빗는 네오,저도 하나는 이렇게 나오는데 아래꺼가 -8이라
4/28/24 23:48,화나서 방방 뛰는 튜브,Hw7 3번 3.0715e-6 나오는데 이거 해결하신 분 계신가요? 아무리 봐도 모르겠네요..
4/28/24 23:53,잠오는 라이언,표준편차 구할 때 epsilon을 안 더했다에 걸게요
4/29/24 11:53,베개를 부비적대는 라이언,이 오류 해결하신 분 계실까요? 파일을 새로 다운 받아도 같은 오류가 뜨네요
4/29/24 11:54,기지개 고양이,전 colab에서 하니 잘 됐어요
4/29/24 11:54,베개를 부비적대는 라이언,colab에서 하는 중인데 안 되네요
4/29/24 11:54,손을 번쩍 든 무지,이것도 해보세요
4/29/24 11:54,베개를 부비적대는 라이언,사진
4/29/24 11:55,베개를 부비적대는 라이언,그것들도 해봤는데 안 되네용..
4/29/24 11:55,기지개 고양이,주소를
4/29/24 11:55,기지개 고양이,바꿔보셨나요
4/29/24 11:55,기지개 고양이,구글 드라이브 연결하고
4/29/24 11:55,베개를 부비적대는 라이언,어떤 주소를 말씀하시는 걸까요?
4/29/24 11:56,기지개 고양이,https://opac.tistory.com/m/3
4/29/24 11:56,기지개 고양이,"여기서 연결하고 파일 들어가서 로케일 복사해서 절대주소 입력하니 되더라고요
(만약 파일 경로 문제라면....)"
4/29/24 11:57,베개를 부비적대는 라이언,한번 시도해보겠습니다 감사합니다
4/29/24 11:57,기지개 고양이,그 파일을 구글드라이브에 업로드하셔야돼요
4/29/24 12:04,베개를 부비적대는 라이언,하니까 전같은 오류는 안 나는데
4/29/24 12:04,베개를 부비적대는 라이언,사용 가능한 RAM을 모두 사용한  후 세션이 다운되었다고 나오는데
4/29/24 12:04,베개를 부비적대는 라이언,이건 제 코드 문제겠죠?
4/29/24 12:05,일하기 싫은 네오,이것도 경험해봤는데 그냥 다시다운받기 nmk번하다보니 어느순간 계속 잘 되더라고요…
4/29/24 12:06,베개를 부비적대는 라이언,nmk번이요…?
4/29/24 12:06,일하기 싫은 네오,사실 n번이에요
4/29/24 12:06,애교뿜뿜 무지,Dimension 오류 있을때도 발생한적 있어요
4/29/24 12:10,베개를 부비적대는 라이언,코드 문제네요 해결했습니다 감사합니다!!
4/29/24 12:10,애교뿜뿜 무지,다행이네요 ㅎㅎ
4/29/24 12:17,손을 번쩍 든 무지,"4번 질문인데요, Cv_lT의 정의가 v_lT를 filter로 하는 convolutional operator인데 그건 그냥 v_lT 아닌가요? 제가 잘못 이해하고있는 부분이 있나싶네요.."
4/29/24 12:22,기지개 고양이,"1차원 convolution에서, v는 1차원 벡터이고,"
4/29/24 12:22,기지개 고양이,C_v는 행렬입니다
4/29/24 12:22,기지개 고양이,HW1의 마지막 문제 생각해보면
4/29/24 12:24,손을 번쩍 든 무지,제가 이해하기로는 C_v가 1×n_l 행렬이라고 생각했는데 그게 아닌건가요??
4/29/24 12:24,기지개 고양이,아닌거 같습니다
4/29/24 12:25,기지개 고양이,글케되면 C_v에 벡터 작용시키면 결과물이 스칼라가 될테니
4/29/24 12:25,하트뿅뿅 라이언,html아니고 htm입니다
4/29/24 12:26,손을 번쩍 든 무지,아 n_l×n_l-1이 되겠군요
4/29/24 12:27,손을 번쩍 든 무지,f_l×n_l-1인가
4/29/24 12:27,손을 번쩍 든 무지,하여튼 감사합니다
4/29/24 12:27,기지개 고양이,이모티콘
4/29/24 13:07,애교뿜뿜 어피치,애교뿜뿜 어피치님이 나갔습니다.
4/29/24 13:41,초롱초롱 네오,html로 저장되는 경우도 있더라고요
4/29/24 13:54,Ernest Lee,혹시 마지막 문제 결과 0.0 나오나요..?
4/29/24 13:54,치즈케이크,굉장히 작게 나오긴 하조
4/29/24 13:54,Ernest Lee,1e-8 로 나올 줄 알앗는데
4/29/24 13:54,Ernest Lee,"아예 차이가 없어서 ,, 좀 궁금하네요"
4/29/24 13:57,밥줘,저는 10e-17정도
4/29/24 13:58,Ernest Lee,맨 마지막 문제요
4/29/24 13:58,Ernest Lee,??
4/29/24 13:58,Ernest Lee,problem 5입니다
4/29/24 14:00,밥줘,네
4/29/24 14:01,Ernest Lee,음.. 제가 뭘 잘못한 걸까요 ㅠ
4/29/24 14:02,치즈케이크,큰 상관없지 않나요
4/29/24 14:02,Ernest Lee,그런가요? 근데 보통 e-8 정도 나오는데 0.0은 그냥 implementation이 틀린 거 같아서
4/29/24 14:02,Ernest Lee,0.0 나오신 분 있으신가요?
4/29/24 14:03,기지개 고양이,혹시 다시 돌리셔도 0.0 나오시나요?
4/29/24 14:03,Ernest Lee,네네
4/29/24 14:04,Ernest Lee,mac cpu로 돌렸습니ㅏㄷ
4/29/24 15:16,손을 번쩍 든 무지,아예 0은 상관없을것 같은데요
4/29/24 15:50,손을 번쩍 든 무지,삭제된 메시지입니다.
4/29/24 15:51,손을 번쩍 든 무지,4번 고민중인데 아직도 잘 모르겠어서요.. 크기 n_l짜리 벡터 v_l로 어떻게 f_l×n_l-1 크기의 filter를 만드는거죠?
4/29/24 15:52,손을 번쩍 든 무지,각행에 n_l-1개의 nonzero 원소가 있는 f_l개의 행이라 보면될까요
4/29/24 15:52,밥줘,과제 1의 마지막 문제의 행렬과 똑같은거에요
4/29/24 15:58,손을 번쩍 든 무지,아 그러면 A는 그냥 임의의 nl×nl-1-개의 원소가 행렬인가요?
4/29/24 16:00,손을 번쩍 든 무지,원소가 있는
4/29/24 16:08,밥줘,"이 문제에서 k1, k2, ...kr 대신
(wl)1 (wl)2... (wl)fl 이 있다 생각하시면 될고에요"
4/29/24 16:11,밥줘,체인룰이랑 Ay+b를 w로 미분한거 생각해서 잘 엮으면 C_v y 형태 찾으실거에요..!
4/29/24 16:59,손을 번쩍 든 무지,넵 감사합니다..!
4/29/24 17:01,궁시렁 프로도,이번 과제는 matplotlib 그래프 같은 출력이 없어서 코드 실행결과를 pdf로 바꾼거는 안 내도 괜찮나요...??
4/29/24 17:20,TA Hyojun,이번 과제가 HW8 말씀하시는 건가요?
4/29/24 17:56,궁시렁 프로도,앗 아니요 hw7이요...!!
4/29/24 18:07,TA Hyojun,음 과제 제출 기한이 지났는데 혹시 코드는 다 제출하셨나요?
4/29/24 18:11,궁시렁 프로도,넵 코드 파일은 제출했습니다
4/29/24 18:13,TA Hyojun,코드 마지막 줄에 print문이 있긴 한데 코드를 제대로 제출하셨다면 큰 문제는 없을 것 같습니다.
4/29/24 18:15,궁시렁 프로도,넵넵 감사합니다!
4/29/24 21:12,양손 엄지척 무지,hw8의 2번 문제는 코드로 제출해야 하나요? 아니면 . . . 에 해당하는 부분을 손으로 써서 제출해야 하나요?
4/29/24 21:15,.,hw6 p4(b)의 경우 코드를 손으로 작성하지 았았다면 감점이 있었나요?
4/29/24 21:19,벌 서는 라이언,저는 손으로 안 적었는데 감점 없었습니다.
4/29/24 22:19,TA Hyojun,손으로 써서 제출해 주시면 됩니다
5/1/24 14:20,비옷입은 튜브,HW6 보충 제출이 오늘까지가 아닌가요?
5/1/24 14:20,기지개 고양이,5/5로 알고있습니다
5/1/24 14:21,소심한 네오,채점이 늦게 올라와서 기간도 넉넉하게 주신 것 같아요!
5/1/24 14:21,비옷입은 튜브,아 그렇군요 답변 감사합니다 
5/1/24 14:26,화난 라이언,Do we have lecture on 05.06.? Was there an announcement on this?
5/1/24 14:27,.,also for 05.15 please!
5/1/24 14:27,벌 서는 라이언,삭제된 메시지입니다.
5/1/24 14:27,벌 서는 라이언,삭제된 메시지입니다.
5/1/24 14:30,화나서 방방 뛰는 튜브,화나서 방방 뛰는 튜브님이 나갔습니다.
5/1/24 20:51,TA kibeom,"📢 Class Cancellation Alert 📢

Hi everyone, just a heads-up: our classes on May 6th, May 8th, and May 15th are canceled as our professor will be attending an academic conference. 

Make-up lectures will be scheduled later and available on eTL. 
Thanks for your attention!"
5/2/24 11:32,건방진 제이지,제가 handwritten file을 재제출하면서 본래 제출했던 코딩파일을 다시 제출하지 않아 3번문제와 5번문제 점수를 못받은것 같은데 어떠한 방법이 없을까요...?
5/2/24 11:34,TA Jihoon,HW7 is graded. the due for re-do is 5/9 5:00 pm.
5/2/24 11:34,TA Jihoon,성함과 함께 이메일 한번 주시겠어요?
5/2/24 11:35,건방진 제이지,넵
5/2/24 18:16,시무룩한 튜브,혹시 저희 5월13일에 정상수업하나요?
5/2/24 18:18,손을 번쩍 든 무지,지난번 수업 끝날때 교수님이 다담주 월요일에 보자고 하셨습니다.
5/2/24 20:33,시무룩한 튜브,감사합니다
5/3/24 9:04,일하기 싫은 네오,지금 진도에 관련된 내용은 아니지만… 흥미로워서 공유하고 싶었습니다 https://arxiv.org/pdf/2303.14151
5/3/24 9:18,Apeach in love,오오… 감사합니다
5/4/24 23:15,비옷입은 튜브,사소한 질문인데 2장에 supervised learning setup 설명에서 image는 vector type이고 text는 non-vector type인 이유가 궁금합니다. image도 string도 엄밀하게는 vector type이 아닌데 vector로 변환할 수 있다고 생각해야 되지 않을까 하는 궁금증이 있습니다.
5/5/24 9:20,붕붕,저희 hw6의 P4에서 STM Conv Layer 에서 32번 평행하게 나온 컨벌루션 결과를 더하기 위해 모을 때 그냥 out.append(tmp)를 하지 않고 out = out.append(tmp)를 하는 이유가 있나요?
5/5/24 16:35,손을 번쩍 든 무지,저도 그게 궁금합니다.
5/5/24 17:16,TA chaeju,"그냥 out.append(tmp)가 맞습니다.
솔루션 오류인 것 같네요."
5/5/24 17:44,엄지척 제이지,Hw7 3번 colab에서 해봐도 ram 용량이 부족한데 혹시 같은 증상 있으셨던 분들 어떻게 해결하셨나요?
5/5/24 18:28,애교뿜뿜 어피치,전 kernel size 오류였습니다.
5/5/24 18:41,생각하는 라이언,저는 (a) (b) 코드 나눠서 실행했는데 됐습니다
5/5/24 18:42,임원섭,임원섭님이 나갔습니다.
5/5/24 19:11,귀여운 라이언,"hw8은 코딩 문제가 없는 것 같은데, 손풀이 파일만 제출해도 되는 건가요?"
5/5/24 20:27,손을 번쩍 든 무지,저는 그렇게 했습니다
5/5/24 21:01,Tube als Cheerleader,"In HW8 problem 3, is the integral supposed to be unbounded? Because then the result would be G(x) + C where C is some constant which would mean it is not always bigger then 0."
5/5/24 21:02,치즈케이크,I assume that the integral taken over the real line
5/5/24 21:02,치즈케이크,*is
5/5/24 21:04,Tube als Cheerleader,What is the real line?
5/5/24 21:04,치즈케이크,From -∞ to ∞?
5/5/24 21:08,화난 라이언,"The integral of definition of f-divergence may be indefinite integral over real except p_Y(x)!=0, i think."
5/5/24 21:09,Tube als Cheerleader,What is an indefinite integral over real? 😭
5/5/24 21:09,Tube als Cheerleader,Ohhhh but they could have wrote that 😭
5/6/24 16:46,TA chaeju,hw6 redo is graded.
5/6/24 19:27,하트뽀뽀 어피치,"과제7 문제 2번에 대해 질문이 있습니다. Convolution, Linear layer 각각에 대해 연산 수를 세는 문제인데,
Convolution layer의 경우에 c_out x h_out^2 x c_in x k^2로 하면 제로패딩도 0의 곱셈과 덧셈을 한다고 보는 것인데, 실제로는 제로패딩에 커널이 겹칠 때에는 해당부분만큼 연산을 안 해도 되므로 빼야하는 것 아닌가요?
올려주신 솔루션에서는 이에 대한 고려가 안 되어 있어 질문드립니다."
5/6/24 20:07,TA Hyojun,HW5 5번 문제와 비슷한 세팅으로 보시면 될 것 같습니다.
5/6/24 20:08,TA Hyojun,사진
5/6/24 20:25,쑥스럽게 인사하는 프로도,"안녕하세요 조교님, hw7 redo가 가능한지 확인하고 가능하다면 제출하고 싶은데 혹시 시간 되실 때 etl 과제란에 게시해주실 수 있으신가요?"
5/6/24 20:36,TA Hyojun,사진
5/6/24 20:37,TA Hyojun,여기에 5월 9일 오후 5시까지 제출해 주시면 됩니다!
5/6/24 21:03,쑥스럽게 인사하는 프로도,감사합니다!!!
5/7/24 16:03,택배 상자를 든 네오,"안녕하세요 이번 HW7 2번문제의 솔루션에서 질문이 있습니다.
(operation per output position)에서의 연산 횟수가 c_in k^2 으로 표현이 되어있는데 제가 생각했을 때는 곱셈의 횟수만 그렇고 각각각의 term들을 더해줘야 하나의 ouput position에서의 값이 나오므로 덧셈 연산의 횟수를 더해줘야 할 것 같은데요 (position 별로 k^2-1 번의 덧셈이 필요하다고 생각합니다), 제가 놓친 부분이 있을까요?"
5/7/24 16:03,택배 상자를 든 네오,사진
5/7/24 16:04,General Trash,bias=True가 default라서 1이 다시 더해집니다
5/7/24 16:04,치즈케이크,bias 덧셈도 있죠
5/7/24 16:05,택배 상자를 든 네오,앗 네네 그렇다면 각각의 output position에서 (c_in + 1) k^2 만큼의 연산이 필요한 것 아닌가요?
5/7/24 16:06,택배 상자를 든 네오,"정확히 말하면
c_in k^2 만큼의 곱셈연산하고
k^2 -1 + 1 만큼의 덧셈 연산을 합친것을 말씀드리는 것입니다"
5/7/24 16:07,General Trash,"bias가 없는 경우 각 output position마다 (c_in * k^2 - 1) 번의 덧셈이 이루어지는데, bias가 있는 경우에는 여기에 1이 다시 더해져서 output position마다 (c_in * k^2) 번의 덧셈이 이루어지게 됩니다."
5/7/24 16:08,치즈케이크,3장 슬라이드의 convolution definition 다시 확인해보시는 게 좋을 듯합니다
5/7/24 16:14,택배 상자를 든 네오,정의를 보고 다시한번 생각해 보겠습니다. 답변주신 두분 감사합니다
5/7/24 16:41,택배 상자를 든 네오,"음.. 죄송하지만 솔직히 말하면 아직 잘 모르겠습니다..
제 질문의 요점은 결국 (c_in * k^2) 번의 곱셈과 덧셈이 있으니 총 2* c_in * k^2 만큼의 연산이 필요한 것 아니냐는 질문이었습니다.."
5/7/24 16:43,벌 서는 라이언,저도 이 부분이 궁금합니다. 혹시 각각의 연산이 저 정도 필요한 것이라고 풀이를 이해해야 하나요?
5/7/24 16:45,Tube cleaning,"지난번 과제에도 저도 헷갈렸었는데 memory가 없는 상황이라고 생각하면 저 값이 나오는 것 같아요! 말씀하신 대로 각 position에 대해 계산하고 다시 더하는 것은 각 position에 대해 값을 저장할 메모리가 필요한 상황이라서, 메모리 없이 하나씩 모두 더하기 위해서는 이 계산법이 나오는 것 같습니다!"
5/7/24 16:51,택배 상자를 든 네오,그 말은 filter의 weight와 input을 곱하고 그것을 cumulative하게 output position에 더해준 연산을 하나로 봤다고 보면 될까요?
5/7/24 16:51,화난 라이언,"질문을 제가 잘 이해한진 모르겠는데 solution의 공식은 각 operation의 개수에 대한 공식입니다. 그러니까 addition도 저만큼, multiplication도 저만큼 필요합니다"
5/7/24 16:52,택배 상자를 든 네오,앗
5/7/24 16:54,택배 상자를 든 네오,655 566 528 multiplications and 655 566 528 additions 라는 말인가요? 그 말이면 좋겠네요
5/7/24 17:02,화난 라이언,네 맞습니다
5/8/24 14:40,비옷입은 튜브,방금 제출하다가 깨달았는데 이번 redo는 왜 9일까지인걸까요...?
5/8/24 14:40,화나서 방방 뛰는 튜브,성적 공개일로부터 1주일 아닌가요
5/8/24 14:40,비옷입은 튜브,아
5/8/24 14:41,비옷입은 튜브,Hw7도 성적이 하루 늦게 나왔나보근요
5/8/24 14:41,비옷입은 튜브,감사합니다
5/8/24 15:41,쑥스럽게 인사하는 프로도,혹시 Hw7 Cifar10 에 url 열때 에러 뜨신분 계신가요?? 저만 이러는건가..
5/8/24 15:44,쑥스럽게 인사하는 프로도,"<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)>
이렇게 에러가 뜨네요.."
5/8/24 16:16,밥줘,전 데탑에서 똑같은 에러 떴다가 노트북으로 하니까 잘 되긴 했는데요
5/8/24 16:16,밥줘,"구글링 해보니까
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
추가 하면 해결 된다고 하는데 한번 해보세요"
5/8/24 16:20,쑥스럽게 인사하는 프로도,아 넵 한번 해보겠습니다! 감사합니다
5/8/24 21:52,음료 마시는 어피치,삭제된 메시지입니다.
5/9/24 9:45,붕붕,저희 이번주 수업 없어서 다음주 월요일 제출 과제 없는 거 맞나요? >.<
5/9/24 10:00,손을 번쩍 든 무지,그렇지 않을까요? (제발)
5/9/24 10:01,부탁하는 네오,홈페이지상에는 hw9가 5/20까지라고 되어있네요
5/9/24 10:31,붕붕,넹 혹시나 오류일까봐 했습니다!
5/9/24 14:07,치맥하는 제이지,과제 redo 하다가 헷갈려서 질문 드립니다.. 이게 왜 multiplication+addition인지 잘 이해가 되지 않습니다. 두 경우 모두 그냥 multiplication의 개수만 계산한것 아닌가요?
5/9/24 14:07,치맥하는 제이지,사진
5/9/24 14:08,TA chaeju,"multiplication과 addition 둘을 더한게 아닌, 각각 저 갯수만큼이라고 해석하시면 됩니다."
5/9/24 14:10,치맥하는 제이지,아 그렇군요 감사합니다!
5/9/24 15:05,부탁하는 네오,"HW7 1번 모범답안 질문드립니다
로피탈을 적용하려면 분모랑 분자가 둘다 무한으로(양이든 음이든) 가야 하는 걸로 아는데, 분모가 무한으로 가는 건 자명하지만 분자는 모든 상황에 무한으로 발산한다고 볼 수 있는 건가요? 예를 들어 x_i들이 모두 0이면 logn/beta의 식인데 이건 물론 0(x_i들 중 최댓값)으로 수렴하긴 하지만 로피탈이 아니라 따로 빼서 설명을 해야 할 것 같습니다"
5/9/24 15:12,화난 라이언,"분모가 무한대로 발산하는 경우, 분모 분자를 미분한 꼴의 극한이 수렴한다면 분자는 관계 없는 걸로 알고 있습니다."
5/9/24 15:13,화난 라이언,사진
5/9/24 15:14,화난 라이언,Manfd Stoll - Introduction to Real Analysis 에서 첨부합니다
5/9/24 15:39,부탁하는 네오,감사합니다
5/9/24 16:03,비옷입은 튜브,같은 문제에서 처음 메모리에 들어오는 데이터도 addition에 포함이 되는건가요? 그러니까 0+(x*weight)로 생각하면 될까요?
5/9/24 16:36,부탁하는 네오,저는 그것보다는 컨볼루션에 바이어스가 있는 걸로 생각했습니다
5/9/24 16:44,비옷입은 튜브,아 그렇군요 감사합니다
5/9/24 19:36,신난 어피치,HW 7 1번 문제 재채점된건가요?
5/9/24 19:44,TA Jihoon,"4,5번 재채점 완료되었고, 1~3번은 아직 재채점 진행 중입니다!"
5/9/24 19:56,신난 어피치,감사합니다!
5/11/24 1:46,기지개 고양이,"다들 HW9 P1 type I, II error rate 어느정도 나오셨나요?"
5/11/24 1:47,기지개 고양이,전 각각 0.3085 0.0017 나왔습니다
5/11/24 9:33,비옷입은 튜브,혹시 과제7 re 채점은 전부 된건가요?
5/11/24 11:09,옐로카드 프로도,"혹시 이것관련해서 좀 여쭤볼 수 있을까요..? type1,2를 구하려면 실제로 anomality인것을 가려내야 하는 것 같은데, 그것은 직접 이미지를 확인하고 판단해야하는 걸까요..?"
5/11/24 11:11,기지개 고양이,저는 mean+3std=threshold로 주어져서
5/11/24 11:12,기지개 고양이,데이터의 Score이 threshold 보다 높으면 anomlity로 판정했어요
5/11/24 11:14,옐로카드 프로도,"제가 이해를 잘 못한 것 같은데, score로 판단하는 방법과, 실제로 anomality인지를 확인하는 방법 두 개가 사용되야하는 것 아닌가요..?"
5/11/24 11:15,기지개 고양이,애초에 test data anomality data로
5/11/24 11:15,기지개 고양이,두개가 분리돼있어서
5/11/24 11:16,기지개 고양이,"스켈레톤 코드 맨 위에 데이터 담을때 test data는 mnist에서, anomality data는 kmnis에서 담아온거라서"
5/11/24 11:18,옐로카드 프로도,아 문제를 제가 잘못 이해했나봐요.. 감사합니다~
5/11/24 11:35,TA Hyojun,HW7 redo is graded.
5/11/24 12:01,화난 라이언,저는 82/10000 245/10000 나왔습니다
5/11/24 12:01,화난 라이언,0.82% 2.45%요
5/11/24 12:02,화난 라이언,threshold는 23.55쯤 나왔구요
5/11/24 12:49,경례하는 프로도,삭제된 메시지입니다.
5/11/24 13:09,기지개 고양이,아 제가 threshold 계산을 잘못했군요
5/11/24 13:09,기지개 고양이,감사합니다
5/11/24 13:10,화난 라이언,제가 틀렸을 수도 있는데 다른 분들은 어떻게 나오셨나요
5/11/24 13:10,화난 라이언,저도 확신은 없네요
5/11/24 13:18,기지개 고양이,문제 해결하니
5/11/24 13:18,기지개 고양이,"Threshold는 0.0371
Type I II error 0.0092 0.0386"
5/11/24 13:18,기지개 고양이,나왔어요
5/11/24 17:41,Saluting Frodo,Saluting Frodo님이 나갔습니다.
5/11/24 20:02,콘이 웃긴 무지,혹시 HW9을 훈련시켜야 하는  Dataset은 무엇인가요?
5/11/24 21:38,TA Hyojun,MNIST 데이터셋을 훈련시키시면 됩니다.
5/11/24 21:49,콘이 웃긴 무지,아 죄송합니다 HW9 Pr2입니다
5/11/24 22:00,TA Hyojun,코드에서 주어진 mixture_of_gaussians를 이용하시면 됩니다
5/11/24 22:16,기지개 고양이,수업 웹사이트에 있는
5/11/24 22:16,기지개 고양이,Chapter 5 code 보시면 큰 도움이 될 수도
5/11/24 22:49,부탁하는 네오,혹시 저희 기말 날짜는 홈페이지에 나온 6/23으로 fix된 건가요?
5/11/24 22:49,부탁하는 네오,아 6/21이요
5/12/24 11:21,궁시렁 프로도,근데 21일에 기말고사면 19일 수욜까지 수업하는건가요…?
5/12/24 18:51,눈물바다에 빠진 라이언,사진
5/12/24 18:51,눈물바다에 빠진 라이언,Chapter 5 Code에서 구하는 loss_mean은 std가 아니라 mean 아닌가요? 맨 위에 Calculate standard deviation이라고 적힌 이유를 모르겠습니다
5/12/24 21:42,TA Yongin,"The make-up lecture recording for May 13 has been uploaded on etl. If you encounter any issues with downloading the recording, please let us know."
5/12/24 22:13,TA Yongin,"Y = f(x) + epsilon,
epsilon ~ N(0, std**2)
인 맥락에서 reconstruction error 의 std 인 것 같습니다.
(y = images, f() = dec(), x = enc(image))"
5/12/24 23:02,눈물바다에 빠진 라이언,답변 감사합니다! 그러면 epsilon = y - f(x)이 되어서 std**2 = E(epsilon**2) - (E(epsilon))**2의 맥락에서 위 코드와 같이 계산한다고 보면 되는 것일까요? (E(epsilon)=0이므로) 완전히 이해한 게 맞는지 궁금해서 질문드립니다.
5/12/24 23:53,TA Yongin,"Ch 5 코드만 봤을 때 저는 그런 의미로 작성된 주석이라고 보았습니다. 
다만 숙제 9 - 1번의 비슷한 주석 처리가 된 step 5에서는 문제에 명시되어 있듯 ‘score’ 의 mean 과 standard deviation 을 계산해야 하므로 착오가 없으시길 바라겠습니다. "
5/13/24 1:29,눈물바다에 빠진 라이언,아아 알겠습니다 감사합니다!
5/13/24 18:07,Luca [루카],"Is the normalizingFlow1d.py code on the website complete? Since it only has ""Step 2"" and because otherwise solving problem 2 seems to be changing just a few lines of the Chapter 5 notebook code and copying the rest wholesale unless I've misunderstood?"
5/13/24 19:40,TA Hyojun,"For problem 2, please refer to the code in Chapter 5 and modify it appropriately to solve the problem."
5/15/24 13:15,신난 어피치,삭제된 메시지입니다.
5/15/24 13:15,신난 어피치,1D change of variable formula에서 첫번째랑 두번째 식은 다르지만 2번째 3번째 식은 같은 거 맞나요?
5/15/24 13:15,신난 어피치,삭제된 메시지입니다.
5/15/24 13:15,신난 어피치,삭제된 메시지입니다.
5/15/24 13:21,신난 어피치,죄송합니다. 해결했습니다!
5/15/24 17:46,쑥스럽게 인사하는 프로도,안녕하세요 조금 뒷북이지만 chapter 2 code에서 LR linear에 bias=False로 돼있는 부분은 오타일까요?
5/16/24 13:29,TA Hyojun,제가 생각하기엔 오타가 맞는 것 같습니다. LR을 정의하는 Step 2 부분은 다 같은 코드를 사용해야 할 것 같은데 맨 처음만 bias = True로 적혀 있고 다른 부분은 bias = False로 적혀있네요.
5/16/24 17:05,으쓱으쓱 어피치,사진
5/16/24 17:05,으쓱으쓱 어피치,"이미지의 파란색 등호가 어떻게 가능한지 항상 궁금했었는데, hw9를 하며 찾아보니 LOTUS라는 법칙이 있어서 h(x)의 density를 모른 상태에서도 기댓값을 구할 수 있다는 것을 알게 되었습니다. 그런데, 이대로 전개하게 되면 E[logX]는 기대한 것과 다른 형태로 나오게 되는데, 어느 부분에서 잘못 생각하고 있는 지 알 수 있을까요??"
5/16/24 18:29,Luca [루카],"You've gotten confused with variable naming I think. You're technically defining a new random variable Z=f(X)/g(X) where X is the original random variable in D_KL. Then use LOTUS within the expectation *with respect to* the distribution of X, hence you use the PDF of X, f(x)"
5/16/24 18:44,으쓱으쓱 어피치,I completely got it. Thanks a lot :)
5/16/24 20:23,부끄러워하는 라이언,"안녕하세요, 과제 8 solution에 오타가 있는 것 같은데요, 여기 아래의 식에서 T(aX)=aT(X)인게 맞지 않나요?"
5/16/24 20:23,부끄러워하는 라이언,사진
5/16/24 20:25,부끄러워하는 라이언,"그리고, linear opeartor임을 보이려면 상수배뿐만 아니라 덧셈에 대해서도 성립함을 보여야 하지 않나요?"
5/16/24 21:37,TA Hyojun,네 해당 부분은 오타인 것 같습니다. 또한 말씀하신 대로 덧셈에 대해서도 성립함을 보이는 것이 원칙적으로는 맞을 것 같습니다.
5/16/24 21:51,부끄러워하는 라이언,감사합니다~
5/17/24 12:10,기타치는 튜브,"과제9 5번 문제(multivariate gaussian 사이의 KL-divergence)에서, multivariate gaussian의 pdf를 직접 대입해서 푸는 것 이외의 방법이 있을까요? 문제에서 explicit한 pdf가 주어지지 않아서 다른 방법이 있는지 궁금합니다."
5/17/24 12:13,손을 번쩍 든 무지,파일: general_notes.pdf
5/17/24 12:13,손을 번쩍 든 무지,삭제된 메시지입니다.
5/17/24 12:14,손을 번쩍 든 무지,저도 똑같은 문제 가지고 검색해 보다가
5/17/24 12:14,손을 번쩍 든 무지,위키피디아 링크 타고 이 파일 찾아서 참조해 보고 있습니다
5/17/24 12:17,엄지척 어피치,과제8에 pdf 유도하라는 문제가 있어서 그걸 참고하라는 게 아닐까 싶어요
5/17/24 12:19,기타치는 튜브,두 분 모두 감사합니다
5/17/24 22:54,손을 번쩍 든 무지,사진
5/17/24 22:58,손을 번쩍 든 무지,"노란색 부분에 대한 질문인데요, 이 집합의 의미가 

1) (θ, φ) ∈ argmax g for any φ ∈ Φ
2) (θ, φ) ∈ argmax g for some φ ∈ Φ

둘 중 어떤 것인지 알 수 있을까요?"
5/17/24 23:05,기타치는 튜브,2)로 이해하고 풀었습니다
5/19/24 20:14,deee,혹시 ch5 1-d Flow code에서 MyDataset class의 역할이 무엇인지 알 수 있을까요?
5/19/24 22:21,손을 번쩍 든 무지,사진
5/19/24 22:21,손을 번쩍 든 무지,Flow 1-d code 의 dz_by_dx 구할 때 log를 구하고 다시 exp를 취하는 이유가 무엇인가요?
5/19/24 22:21,손을 번쩍 든 무지,그냥 distribution 함수에 x를 대입하고 weights 곱하면 되지 않나 싶네요..
5/20/24 11:28,화난 라이언,제가 알아봤을 땐 torch distribution class에 probability를 반환하는 메소드가 없었던 것 같아요
5/20/24 11:29,화난 라이언,있다면 그렇게 하시면 될 것 같습니다